{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn, einsum\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import v2\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","\n","import einops\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","from pyspark.sql import SparkSession"]},{"cell_type":"markdown","metadata":{},"source":["### Creating a spark dataframe from the parquet files"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:01:27.770519Z","iopub.status.busy":"2024-03-14T15:01:27.769677Z","iopub.status.idle":"2024-03-14T15:01:39.780817Z","shell.execute_reply":"2024-03-14T15:01:39.779556Z","shell.execute_reply.started":"2024-03-14T15:01:27.770481Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/03/14 15:01:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","                                                                                \r"]}],"source":["# Creating a spark session\n","spark = SparkSession.builder \\\n","    .appName(\"DatasetCreator\") \\\n","    .master('local[*]') \\\n","    .config(\"spark.driver.memory\", \"15g\") \\\n","    .getOrCreate()\n","\n","# Loading the parquet files from the directory\n","parquet_files_path = \"/kaggle/input/regress\"\n","df = spark.read.parquet(parquet_files_path)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:04:31.396426Z","iopub.status.busy":"2024-03-14T15:04:31.395114Z","iopub.status.idle":"2024-03-14T15:04:31.433335Z","shell.execute_reply":"2024-03-14T15:04:31.432181Z","shell.execute_reply.started":"2024-03-14T15:04:31.396381Z"},"trusted":true},"outputs":[],"source":["# Limiting the number of rows to 10000\n","df = df.limit(10000)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:04:33.191915Z","iopub.status.busy":"2024-03-14T15:04:33.191493Z","iopub.status.idle":"2024-03-14T15:04:33.207424Z","shell.execute_reply":"2024-03-14T15:04:33.206209Z","shell.execute_reply.started":"2024-03-14T15:04:33.191883Z"},"trusted":true},"outputs":[],"source":["#Sampling random 1000 rows from the dataset\n","df = df.sample(withReplacement=False, fraction=0.1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:04:34.921632Z","iopub.status.busy":"2024-03-14T15:04:34.921211Z","iopub.status.idle":"2024-03-14T15:06:42.042055Z","shell.execute_reply":"2024-03-14T15:06:42.041195Z","shell.execute_reply.started":"2024-03-14T15:04:34.921600Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#Converting to pandas dataframe\n","df = df.toPandas()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:06:42.044296Z","iopub.status.busy":"2024-03-14T15:06:42.043777Z","iopub.status.idle":"2024-03-14T15:06:42.401358Z","shell.execute_reply":"2024-03-14T15:06:42.400510Z","shell.execute_reply.started":"2024-03-14T15:06:42.044268Z"},"trusted":true},"outputs":[],"source":["spark.stop()"]},{"cell_type":"markdown","metadata":{},"source":["### Processing the Data :-\n","#### 1. Dividing the data into train and test sets\n","#### 2. Creating a pytorch Dataset and a Dataloader"]},{"cell_type":"markdown","metadata":{},"source":["#### Finding mean and variance"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:07:37.854646Z","iopub.status.busy":"2024-03-14T15:07:37.854226Z","iopub.status.idle":"2024-03-14T15:08:20.420600Z","shell.execute_reply":"2024-03-14T15:08:20.419369Z","shell.execute_reply.started":"2024-03-14T15:07:37.854617Z"},"trusted":true},"outputs":[],"source":["images = torch.stack([torch.tensor(img) for img in df['X_jet']], dim=0)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:08:22.637235Z","iopub.status.busy":"2024-03-14T15:08:22.635880Z","iopub.status.idle":"2024-03-14T15:08:23.189123Z","shell.execute_reply":"2024-03-14T15:08:23.187877Z","shell.execute_reply.started":"2024-03-14T15:08:22.637202Z"},"trusted":true},"outputs":[],"source":["mean = images.mean(dim=(0, 2, 3))\n","std = images.std(dim=(0,2,3))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:08:26.149200Z","iopub.status.busy":"2024-03-14T15:08:26.148442Z","iopub.status.idle":"2024-03-14T15:08:26.184565Z","shell.execute_reply":"2024-03-14T15:08:26.183483Z","shell.execute_reply.started":"2024-03-14T15:08:26.149167Z"},"trusted":true},"outputs":[],"source":["del images"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:08:27.337739Z","iopub.status.busy":"2024-03-14T15:08:27.336863Z","iopub.status.idle":"2024-03-14T15:08:28.325194Z","shell.execute_reply":"2024-03-14T15:08:28.324119Z","shell.execute_reply.started":"2024-03-14T15:08:27.337707Z"},"trusted":true},"outputs":[],"source":["X = df['X_jet']\n","y = df['m']\n","\n","scaler = StandardScaler()\n","y = scaler.fit_transform(np.array(y).reshape(-1, 1))\n","\n","# Splitting the data into train and test sets with 80% for training and 20% for testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:08:29.162903Z","iopub.status.busy":"2024-03-14T15:08:29.162283Z","iopub.status.idle":"2024-03-14T15:08:32.165069Z","shell.execute_reply":"2024-03-14T15:08:32.164110Z","shell.execute_reply.started":"2024-03-14T15:08:29.162871Z"},"trusted":true},"outputs":[],"source":["#Creating a custom Pytorch Dataset\n","class RegressDataset(Dataset):\n","    def __init__(self, X, y, transform=False):\n","        self.X = X\n","        self.y = y\n","        self.transform = transform\n","    def __len__(self):\n","        return len(self.X)\n","    def __getitem__(self, idx):\n","        img = np.array(self.X.iloc[idx])\n","        img = torch.tensor(img, dtype=torch.float32)\n","        img = v2.Normalize(mean=mean, std=std)(img)\n","        y = torch.tensor(self.y[idx], dtype=torch.float32)\n","        return img, y\n","    \n","# Train and Test pytorch Datasets\n","train_dataset = RegressDataset(X_train, y_train)\n","test_dataset = RegressDataset(X_test, y_test)\n","\n","#Defining the batch size \n","BATCH_SIZE = 32\n","\n","# Train and test pytorch Dataloaders\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,  shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["#### Model Building"]},{"cell_type":"markdown","metadata":{},"source":["**This paper https://arxiv.org/abs/2103.11886 notes that ViT struggles to attend at greater depths (past 12 layers), and suggests mixing the attention of each head post-softmax as a solution, dubbed Re-attention. The below solution is the implementation of the above paper to \n","build DeepVit or DeepVisionTransoformer for better performance**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n","        super().__init__()\n","        inner_dim = dim_head *  heads\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.norm = nn.LayerNorm(dim)\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.reattn_weights = nn.Parameter(torch.randn(heads, heads))\n","\n","        self.reattn_norm = nn.Sequential(\n","            Rearrange('b h i j -> b i j h'),\n","            nn.LayerNorm(heads),\n","            Rearrange('b i j h -> b h i j')\n","        )\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        b, n, _, h = *x.shape, self.heads\n","        x = self.norm(x)\n","\n","        qkv = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n","\n","        # attention\n","\n","        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n","        attn = dots.softmax(dim=-1)\n","        attn = self.dropout(attn)\n","\n","        # re-attention\n","\n","        attn = einsum('b h i j, h g -> b g i j', attn, self.reattn_weights)\n","        attn = self.reattn_norm(attn)\n","\n","        # aggregate and out\n","\n","        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        out =  self.to_out(out)\n","        return out\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n","                FeedForward(dim, mlp_dim, dropout = dropout)\n","            ]))\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","class DeepViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n","        super().__init__()\n","        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n","        num_patches = (image_size // patch_size) ** 2\n","        patch_dim = channels * patch_size ** 2\n","        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n","            nn.LayerNorm(patch_dim),\n","            nn.Linear(patch_dim, dim),\n","            nn.LayerNorm(dim)\n","        )\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n","\n","        self.pool = pool\n","        self.to_latent = nn.Identity()\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        b, n, _ = x.shape\n","\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x += self.pos_embedding[:, :(n + 1)]\n","        x = self.dropout(x)\n","\n","        x = self.transformer(x)\n","\n","        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n","\n","        x = self.to_latent(x)\n","        return self.mlp_head(x)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:10:45.094548Z","iopub.status.busy":"2024-03-14T15:10:45.093761Z","iopub.status.idle":"2024-03-14T15:10:46.170100Z","shell.execute_reply":"2024-03-14T15:10:46.169213Z","shell.execute_reply.started":"2024-03-14T15:10:45.094514Z"},"trusted":true},"outputs":[],"source":["model = DeepViT(\n","    image_size = 125,\n","    patch_size = 25,\n","    num_classes = 1,\n","    dim = 1024,\n","    depth = 12,\n","    heads = 16,\n","    channels = 8,\n","    mlp_dim = 2048,\n","    dropout = 0.1,\n","    emb_dropout = 0.1\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:10:47.141899Z","iopub.status.busy":"2024-03-14T15:10:47.140905Z","iopub.status.idle":"2024-03-14T15:10:47.146366Z","shell.execute_reply":"2024-03-14T15:10:47.145204Z","shell.execute_reply.started":"2024-03-14T15:10:47.141865Z"},"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available else 'cpu'"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:10:48.762412Z","iopub.status.busy":"2024-03-14T15:10:48.761431Z","iopub.status.idle":"2024-03-14T15:10:48.902565Z","shell.execute_reply":"2024-03-14T15:10:48.901509Z","shell.execute_reply.started":"2024-03-14T15:10:48.762378Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DeepViT(\n","  (to_patch_embedding): Sequential(\n","    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=25, p2=25)\n","    (1): LayerNorm((5000,), eps=1e-05, elementwise_affine=True)\n","    (2): Linear(in_features=5000, out_features=1024, bias=True)\n","    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (transformer): Transformer(\n","    (layers): ModuleList(\n","      (0-11): 12 x ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (reattn_norm): Sequential(\n","            (0): Rearrange('b h i j -> b i j h')\n","            (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n","            (2): Rearrange('b i j h -> b h i j')\n","          )\n","          (to_out): Sequential(\n","            (0): Linear(in_features=1024, out_features=1024, bias=True)\n","            (1): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Dropout(p=0.1, inplace=False)\n","            (4): Linear(in_features=2048, out_features=1024, bias=True)\n","            (5): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (to_latent): Identity()\n","  (mlp_head): Sequential(\n","    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (1): Linear(in_features=1024, out_features=1, bias=True)\n","  )\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["#### Setting the loss function and the optimizer"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:10:54.497212Z","iopub.status.busy":"2024-03-14T15:10:54.496831Z","iopub.status.idle":"2024-03-14T15:10:54.504261Z","shell.execute_reply":"2024-03-14T15:10:54.502970Z","shell.execute_reply.started":"2024-03-14T15:10:54.497185Z"},"trusted":true},"outputs":[],"source":["#Using the Adamw loss function as it is widely used for transformer architectures\n","loss_fn = torch.nn.MSELoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)"]},{"cell_type":"markdown","metadata":{},"source":["#### Training and testing the model"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:18:15.566751Z","iopub.status.busy":"2024-03-14T15:18:15.565887Z","iopub.status.idle":"2024-03-14T15:24:21.042988Z","shell.execute_reply":"2024-03-14T15:24:21.041911Z","shell.execute_reply.started":"2024-03-14T15:18:15.566720Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"804d22235c88445882c70f4a690a199e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/30 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 || Train Loss: 0.397 || Test Loss: 0.046\n","Epoch: 1 || Train Loss: 1.684 || Test Loss: 0.093\n","Epoch: 2 || Train Loss: 1.415 || Test Loss: 0.030\n","Epoch: 3 || Train Loss: 0.805 || Test Loss: 0.253\n","Epoch: 4 || Train Loss: 1.089 || Test Loss: 0.063\n","Epoch: 5 || Train Loss: 0.584 || Test Loss: 0.328\n","Epoch: 6 || Train Loss: 1.038 || Test Loss: 0.044\n","Epoch: 7 || Train Loss: 1.415 || Test Loss: 0.132\n","Epoch: 8 || Train Loss: 0.842 || Test Loss: 0.318\n","Epoch: 9 || Train Loss: 1.017 || Test Loss: 0.292\n","Epoch: 10 || Train Loss: 0.744 || Test Loss: 0.098\n","Epoch: 11 || Train Loss: 0.949 || Test Loss: 0.212\n","Epoch: 12 || Train Loss: 0.837 || Test Loss: 0.090\n","Epoch: 13 || Train Loss: 0.811 || Test Loss: 0.728\n","Epoch: 14 || Train Loss: 0.754 || Test Loss: 0.111\n","Epoch: 15 || Train Loss: 1.634 || Test Loss: 0.208\n","Epoch: 16 || Train Loss: 0.924 || Test Loss: 0.050\n","Epoch: 17 || Train Loss: 0.499 || Test Loss: 0.456\n","Epoch: 18 || Train Loss: 0.586 || Test Loss: 0.080\n","Epoch: 19 || Train Loss: 0.570 || Test Loss: 0.103\n","Epoch: 20 || Train Loss: 1.095 || Test Loss: 0.069\n","Epoch: 21 || Train Loss: 0.735 || Test Loss: 0.131\n","Epoch: 22 || Train Loss: 0.476 || Test Loss: 0.041\n","Epoch: 23 || Train Loss: 0.929 || Test Loss: 0.259\n","Epoch: 24 || Train Loss: 1.467 || Test Loss: 0.026\n","Epoch: 25 || Train Loss: 1.718 || Test Loss: 0.057\n","Epoch: 26 || Train Loss: 1.068 || Test Loss: 0.283\n","Epoch: 27 || Train Loss: 1.260 || Test Loss: 0.020\n","Epoch: 28 || Train Loss: 0.784 || Test Loss: 0.001\n","Epoch: 29 || Train Loss: 1.608 || Test Loss: 0.001\n"]}],"source":["from tqdm.auto import tqdm\n","epochs = 30\n","for epoch in tqdm(range(epochs)):\n","    model.train()\n","    for images, weights in train_dataloader:\n","        images, weights = images.to(device), weights.to(device)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = loss_fn(outputs.squeeze(), weights.squeeze())\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","    model.eval()\n","    with torch.inference_mode():\n","        for images, weights in test_dataloader:\n","            images, weights = images.to(device), weights.to(device)\n","            outputs = model(images)\n","            test_loss = loss_fn(outputs.squeeze(), weights.squeeze())\n","            \n","    print(f'Epoch: {epoch} || Train Loss: {loss:.3f} || Test Loss: {test_loss:.3f}')"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:32:38.957362Z","iopub.status.busy":"2024-03-14T15:32:38.956572Z","iopub.status.idle":"2024-03-14T15:32:40.827483Z","shell.execute_reply":"2024-03-14T15:32:40.826451Z","shell.execute_reply.started":"2024-03-14T15:32:38.957327Z"},"trusted":true},"outputs":[],"source":["checkpoint = {\n","    'state_dict' : model.state_dict(),\n","    'optimizer' : optimizer.state_dict()\n","}\n","torch.save(checkpoint, 'checkpoint.pth')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4579224,"sourceId":7816511,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
