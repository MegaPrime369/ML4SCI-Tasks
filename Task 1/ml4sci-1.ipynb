{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7848427,"sourceType":"datasetVersion","datasetId":4602213}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nimport matplotlib.pyplot as plt\nfrom torch import nn\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:38:14.379967Z","iopub.execute_input":"2024-03-16T12:38:14.380333Z","iopub.status.idle":"2024-03-16T12:38:17.005004Z","shell.execute_reply.started":"2024-03-16T12:38:14.380300Z","shell.execute_reply":"2024-03-16T12:38:17.003991Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#importing the electron and photon datasets\nelectrons_file = h5py.File('/kaggle/input/electron-photon/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5', 'r')\nphotons_file = h5py.File('/kaggle/input/electron-photon/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5', 'r')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:38:19.051899Z","iopub.execute_input":"2024-03-16T12:38:19.052911Z","iopub.status.idle":"2024-03-16T12:38:19.103586Z","shell.execute_reply.started":"2024-03-16T12:38:19.052875Z","shell.execute_reply":"2024-03-16T12:38:19.102626Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Splitting the datasets into images and labels\nelectrons_X = electrons_file['X']\nelectrons_y = electrons_file['y']\n\nphotons_X = photons_file['X']\nphotons_y = photons_file['y']","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:38:20.569813Z","iopub.execute_input":"2024-03-16T12:38:20.570182Z","iopub.status.idle":"2024-03-16T12:38:20.577165Z","shell.execute_reply.started":"2024-03-16T12:38:20.570152Z","shell.execute_reply":"2024-03-16T12:38:20.576235Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Converthing them to torch tensors\nelectrons_X_tensor = torch.tensor(electrons_X[:])\nelectrons_y_tensor = torch.tensor(electrons_y[:])\n\nphotons_X_tensor = torch.tensor(photons_X[:])\nphotons_y_tensor = torch.tensor(photons_y[:])","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:38:22.224881Z","iopub.execute_input":"2024-03-16T12:38:22.225281Z","iopub.status.idle":"2024-03-16T12:38:38.222405Z","shell.execute_reply.started":"2024-03-16T12:38:22.225250Z","shell.execute_reply":"2024-03-16T12:38:38.221598Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Concatenating the images and labels into single tensors\nX = torch.cat((electrons_X_tensor, photons_X_tensor), dim=0)\ny = torch.cat((electrons_y_tensor, photons_y_tensor), dim=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:38:47.665675Z","iopub.execute_input":"2024-03-16T12:38:47.665999Z","iopub.status.idle":"2024-03-16T12:38:49.147276Z","shell.execute_reply.started":"2024-03-16T12:38:47.665975Z","shell.execute_reply":"2024-03-16T12:38:49.146420Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Shape of the image\nX.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:38:54.212275Z","iopub.execute_input":"2024-03-16T12:38:54.213121Z","iopub.status.idle":"2024-03-16T12:38:54.219426Z","shell.execute_reply.started":"2024-03-16T12:38:54.213088Z","shell.execute_reply":"2024-03-16T12:38:54.218425Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([498000, 32, 32, 2])"},"metadata":{}}]},{"cell_type":"code","source":"# Changing the shape as to make it compatible with pytorch requirements\nX = torch.transpose(X, 1, 3)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:38:56.842763Z","iopub.execute_input":"2024-03-16T12:38:56.843081Z","iopub.status.idle":"2024-03-16T12:38:56.849334Z","shell.execute_reply.started":"2024-03-16T12:38:56.843056Z","shell.execute_reply":"2024-03-16T12:38:56.848446Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Combining X and y into a single tensor dataset\ndataset = TensorDataset(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:38:58.551206Z","iopub.execute_input":"2024-03-16T12:38:58.551599Z","iopub.status.idle":"2024-03-16T12:38:58.557693Z","shell.execute_reply.started":"2024-03-16T12:38:58.551568Z","shell.execute_reply":"2024-03-16T12:38:58.556645Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Determining the train and test size\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:00.108334Z","iopub.execute_input":"2024-03-16T12:39:00.109204Z","iopub.status.idle":"2024-03-16T12:39:00.113284Z","shell.execute_reply.started":"2024-03-16T12:39:00.109170Z","shell.execute_reply":"2024-03-16T12:39:00.112437Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Dividing the dataset into train and test set\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:03.325700Z","iopub.execute_input":"2024-03-16T12:39:03.326094Z","iopub.status.idle":"2024-03-16T12:39:03.381513Z","shell.execute_reply.started":"2024-03-16T12:39:03.326053Z","shell.execute_reply":"2024-03-16T12:39:03.380508Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Making traing and test dataloaders\nbatch_size = 128\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:05.165046Z","iopub.execute_input":"2024-03-16T12:39:05.165410Z","iopub.status.idle":"2024-03-16T12:39:05.171348Z","shell.execute_reply.started":"2024-03-16T12:39:05.165383Z","shell.execute_reply":"2024-03-16T12:39:05.170453Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#ResNet15 implementation :-\nclass Block(nn.Module):\n  def __init__(self, input_channels, intermediate_channels, stride=1, identity_downsample=None):\n    super().__init__()\n    self.in_channels = input_channels\n    self.intermediate_channels = intermediate_channels\n    self.identity_downsample = identity_downsample\n    self.expansion = 1\n\n    self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n                           out_channels = self.intermediate_channels,\n                           kernel_size=3,\n                           stride=1,\n                           padding=1,\n                           bias=False)\n    self.bn1 = nn.BatchNorm2d(self.intermediate_channels)\n    self.relu = nn.ReLU()\n    self.conv2 = nn.Conv2d(in_channels = self.intermediate_channels,\n                           out_channels = self.intermediate_channels*self.expansion,\n                           kernel_size=3,\n                           padding=1,\n                           stride=stride,\n                           bias=False\n                           )\n    self.bn2 = nn.BatchNorm2d(self.intermediate_channels*self.expansion)\n\n  def forward(self, x):\n    identity = x.clone()\n\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n\n    if self.identity_downsample != None:\n      identity = self.identity_downsample(identity)\n\n    x += identity\n    x = self.relu(x)\n    return x\n\nclass ResNet(nn.Module):\n  def __init__(self, block, img_channels, num_classes, layers):\n    super().__init__()\n    self.in_channels = 64\n\n    self.conv1 = nn.Conv2d(in_channels=img_channels,\n                           out_channels = 64,\n                           kernel_size=7,\n                           stride=2,\n                           padding=3,\n                           bias = False)\n    self.bn1 = nn.BatchNorm2d(64)\n    self.relu = nn.ReLU()\n    self.conv2 = nn.Conv2d(\n        in_channels = 64,\n        out_channels = 64,\n        kernel_size = 1,\n        stride = 1,\n        padding = 0,\n        bias = False\n    )\n    self.bn2 = nn.BatchNorm2d(64)\n    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n\n    self.layer1 = self.make_layer(layers[0], Block, 64, 1)\n    self.layer2 = self.make_layer(layers[1], Block, 128, 2)\n    self.layer3 = self.make_layer(layers[2], Block, 256, 2)\n\n    self.avg = nn.AdaptiveAvgPool2d((1, 1))\n    self.fc = nn.Linear(256 * 1, num_classes)\n\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = self.relu(x)\n    x = self.maxpool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n\n    x = self.avg(x)\n    x = x.reshape(x.shape[0], -1)\n    x = self.fc(x)\n\n    return x\n\n  def make_layer(self, num_residual_blocks, block, intermediate_channels, stride):\n    blocks = []\n    identity_downsample = None\n\n    if stride != 1 or self.in_channels != intermediate_channels * 1:\n      identity_downsample = nn.Sequential(\n          nn.Conv2d(in_channels = self.in_channels,\n                    out_channels = intermediate_channels * 1,\n                    stride=stride,\n                    kernel_size=1,\n                    padding=0,\n                    bias = False),\n          nn.BatchNorm2d(intermediate_channels * 1)\n      )\n    blocks.append(block(self.in_channels, intermediate_channels, stride, identity_downsample))\n\n    self.in_channels = intermediate_channels * 1\n\n    for i in range(num_residual_blocks - 1):\n      blocks.append(block(self.in_channels, intermediate_channels))\n\n    return nn.Sequential(*blocks)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:07.503433Z","iopub.execute_input":"2024-03-16T12:39:07.504289Z","iopub.status.idle":"2024-03-16T12:39:07.528052Z","shell.execute_reply.started":"2024-03-16T12:39:07.504255Z","shell.execute_reply":"2024-03-16T12:39:07.527146Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#The model below is of the following architecture:\n#1. First a convultional layer with a kernel size of 7\n#2. Then another conv. layer with kernel size of 1\n#3. Then a maxpooling layer\n#4. Each block consists of 2 conv. layers and there are a total of 6 blocks.\n#5. Hence a total of 15(1 + 1 + 1 + 6 * 2) layers which includes conv. layers and maxpool layers\nmodel = ResNet(Block, 2, 1, [2, 2, 2])","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:12.055146Z","iopub.execute_input":"2024-03-16T12:39:12.055791Z","iopub.status.idle":"2024-03-16T12:39:12.106984Z","shell.execute_reply.started":"2024-03-16T12:39:12.055737Z","shell.execute_reply":"2024-03-16T12:39:12.106167Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Setting the device\ndevice = \"cuda\" if torch.cuda.is_available else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:13.685171Z","iopub.execute_input":"2024-03-16T12:39:13.686177Z","iopub.status.idle":"2024-03-16T12:39:13.690239Z","shell.execute_reply.started":"2024-03-16T12:39:13.686141Z","shell.execute_reply":"2024-03-16T12:39:13.689276Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Transferring the model to device\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:15.565726Z","iopub.execute_input":"2024-03-16T12:39:15.566069Z","iopub.status.idle":"2024-03-16T12:39:15.830946Z","shell.execute_reply.started":"2024-03-16T12:39:15.566042Z","shell.execute_reply":"2024-03-16T12:39:15.829872Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU()\n  (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Block(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): Block(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Block(\n      (identity_downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): Block(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Block(\n      (identity_downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): Block(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"#Setting the loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:19.750302Z","iopub.execute_input":"2024-03-16T12:39:19.750972Z","iopub.status.idle":"2024-03-16T12:39:22.264213Z","shell.execute_reply.started":"2024-03-16T12:39:19.750938Z","shell.execute_reply":"2024-03-16T12:39:22.263435Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\n# Defining training function\ndef train(model, criterion, optimizer, train_loader, device):\n    model.train()\n    train_loss = 0.0\n    for inputs, targets in tqdm(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets.view(-1, 1))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * inputs.size(0)\n    return train_loss / len(train_loader.dataset)\n\n# Defining the evaluation function\ndef evaluate(model, data_loader, device):\n    model.eval()\n    y_true = []\n    y_scores = []\n    with torch.inference_mode():\n        for inputs, targets in data_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            y_true.extend(targets.cpu().numpy())\n            y_scores.extend(outputs.cpu().numpy())\n    return roc_auc_score(y_true, y_scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:39:25.664318Z","iopub.execute_input":"2024-03-16T12:39:25.665407Z","iopub.status.idle":"2024-03-16T12:39:25.674669Z","shell.execute_reply.started":"2024-03-16T12:39:25.665349Z","shell.execute_reply":"2024-03-16T12:39:25.673620Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Logic for early stopping:\nThe code checks the validation performance (in this case, ROC-AUC score) after each epoch and tracks the performance over time. If the performance does not improve for a certain number of epochs (max_monotonic_increase), indicating a monotonic increase in the validation loss, or if it surpasses a tolerance threshold and does not improve for a certain number of epochs (max_gradual_increase), training is stopped early.\n","metadata":{}},{"cell_type":"code","source":"# Setting maximum patience for early stopping\nmax_monotonic_increase = 10\nmax_gradual_increase = 10\n\n# Initializing the variables for early stopping and plotting\ncurrent_mono = 0\ncurrent_tolerance = 0\ntolerance = float('inf')\n# Initializing the best-auc variable\nbest_auc = 0.0","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:44:20.423601Z","iopub.execute_input":"2024-03-16T12:44:20.423957Z","iopub.status.idle":"2024-03-16T12:44:20.428974Z","shell.execute_reply.started":"2024-03-16T12:44:20.423931Z","shell.execute_reply":"2024-03-16T12:44:20.427950Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"epochs = 20\n# Training loop\nfor epoch in range(1, epochs + 1):\n    print(\"Epoch {}/{}\".format(epoch, epochs))\n    train_loss = train(model, criterion, optimizer, train_dataloader, device)\n    test_auc = evaluate(model, test_dataloader, device)\n    \n    print(\"Train Loss: {:.4f}, Test ROC-AUC: {:.4f}\".format(train_loss, test_auc))\n    \n    # Update patience and tolerance\n    if test_auc <= tolerance:\n        current_tolerance += 1\n    else:\n        current_tolerance = 0\n        tolerance = test_auc\n    \n    if current_tolerance == max_gradual_increase:\n        print(\"Early stopping training due to overfitting...\")\n        break\n     # Saving checkpoint\n    if test_auc > best_auc:\n        best_auc = test_auc\n        torch.save(model.state_dict(), '/kaggle/working/best_model.pth')\n        print(\"Saving model checkpoint...\")\n    \n    # Updating patience for early stopping\n    if test_auc <= best_auc:\n        current_mono += 1\n    else:\n        current_mono = 0\n    \n    if current_mono == max_monotonic_increase:\n        print(\"Early stopping training due to overfitting...\")\n        break\n\nprint(\"Training completed!\")","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:44:23.504102Z","iopub.execute_input":"2024-03-16T12:44:23.504471Z","iopub.status.idle":"2024-03-16T12:53:03.112787Z","shell.execute_reply.started":"2024-03-16T12:44:23.504442Z","shell.execute_reply":"2024-03-16T12:53:03.111831Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ba60879ad9402d8a191b39d29d18bc"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5548, Test ROC-AUC: 0.7887\nSaving model checkpoint...\nEpoch 2/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9853fd7721084c44b49750ed886d7370"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5513, Test ROC-AUC: 0.7885\nEpoch 3/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a7bf0d692424dceb193138715389662"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5484, Test ROC-AUC: 0.7897\nSaving model checkpoint...\nEpoch 4/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a95ef5888bc040f1b3657fc8b36b2b1c"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5452, Test ROC-AUC: 0.7914\nSaving model checkpoint...\nEpoch 5/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a270beff58734c4094066d0249aad6e2"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5428, Test ROC-AUC: 0.7937\nSaving model checkpoint...\nEpoch 6/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d0f32a2afed487d9f72772cd8700900"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5397, Test ROC-AUC: 0.7926\nEpoch 7/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7f172237934af18dc452bdb56c2f7b"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5368, Test ROC-AUC: 0.7945\nSaving model checkpoint...\nEpoch 8/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d162569025044b6eae40bc3cdff1cca3"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5340, Test ROC-AUC: 0.7921\nEpoch 9/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3461f9ed253b4878b04aefa3021ff008"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5308, Test ROC-AUC: 0.7917\nEpoch 10/20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a56fd009b95481a8ff6f7435393a75b"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.5273, Test ROC-AUC: 0.7925\nEarly stopping training due to overfitting...\nTraining completed!\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# Evaluating the best model on test data\nbest_model = ResNet(Block, 2, 1, [2, 2, 2]).to(device)\nbest_model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\ny_true = []\ny_scores = []\nwith torch.no_grad():\n    for inputs, targets in test_dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = best_model(inputs)\n        y_true.extend(targets.cpu().numpy())\n        y_scores.extend(outputs.cpu().numpy())\n\n# Calculating ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Plotting ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:55:20.786678Z","iopub.execute_input":"2024-03-16T12:55:20.787014Z","iopub.status.idle":"2024-03-16T12:55:25.632408Z","shell.execute_reply.started":"2024-03-16T12:55:20.786988Z","shell.execute_reply":"2024-03-16T12:55:25.631400Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACSM0lEQVR4nOzdd1hT1xsH8G8IJOyhTBFZTtyioCjiQLFaFSeK4qhatY7W0bpX3XvVuqqiVutExYnVOqriAlFcUEUUBwjKlpmc3x/3RzACSjDhAnk/z8PDveeuNwkhb849Q8AYYyCEEEIIUUMafAdACCGEEMIXSoQIIYQQorYoESKEEEKI2qJEiBBCCCFqixIhQgghhKgtSoQIIYQQorYoESKEEEKI2qJEiBBCCCFqixIhQgghhKgtSoRIhWZnZ4chQ4bwHYbaadOmDdq0acN3GF80d+5cCAQCJCQk8B1KmSMQCDB37lylnCs6OhoCgQD+/v5KOR8A3Lx5EyKRCM+fP1faOZWtX79+6Nu3L99hkC+gRIiUmL+/PwQCgexHU1MT1tbWGDJkCF69esV3eGVaeno65s+fjwYNGkBXVxdGRkZwd3fHrl27UF5mvXn48CHmzp2L6OhovkMpQCKRYMeOHWjTpg0qVaoEsVgMOzs7DB06FLdv3+Y7PKXYu3cv1qxZw3cYckozphkzZqB///6wtbWVlbVp00buf5KOjg4aNGiANWvWQCqVFnqed+/e4eeff0atWrWgra2NSpUqwcvLCydOnCjy2ikpKZg3bx4aNmwIfX196OjooF69epgyZQpev34t22/KlCk4fPgw7t69q7wHTpSPEVJCO3bsYADYr7/+ynbv3s22bt3Khg0bxoRCIXN0dGQZGRl8h8gyMzNZdnY232HIiY2NZXXr1mUaGhrM19eXbd68ma1du5a1bt2aAWA+Pj4sNzeX7zC/6ODBgwwAu3DhQoFtWVlZLCsrq/SDYox9+PCBderUiQFgrVu3ZsuXL2fbtm1js2bNYrVq1WICgYDFxMQwxhibM2cOA8Di4+N5ifVrdOnShdna2qrs/BkZGSwnJ0ehY4qKSSqVsoyMDKX9Xd+5c4cBYNeuXZMr9/DwYFWrVmW7d+9mu3fvZqtXr2bNmjVjANj06dMLnOfx48fM2tqaiUQiNnLkSLZ161a2fPly1qhRIwaATZ48ucAxT58+Zfb29kwoFLJ+/fqx3377jW3ZsoWNHTuWVa5cmdWoUUNufxcXF+bn56eUx01UgxIhUmJ5idCtW7fkyqdMmcIAsP379/MUGb8yMjKYRCIpcruXlxfT0NBgx44dK7Bt8uTJDABbsmSJKkMsVFpamkL7fy4R4tOYMWMYALZ69eoC23Jzc9ny5ctLNRGSSqXsw4cPSj+vKhIhiUTyVV9gVJ2c5Rk/fjyrVq0ak0qlcuUeHh6sbt26cmUZGRnM1taWGRgYyCVi2dnZrF69ekxXV5ddv35d7pjc3Fzm4+PDALB9+/bJynNycljDhg2Zrq4u+/fffwvElZycXCDhWrFiBdPT02OpqaklfrxEtSgRIiVWVCJ04sQJBoAtWrRIrvzRo0esV69ezMTEhInFYubs7FxoMpCYmMh++uknZmtry0QiEbO2tmZ+fn5yH1aZmZls9uzZzNHRkYlEIla1alX2888/s8zMTLlz2drassGDBzPGGLt16xYDwPz9/Qtc88yZMwwAO378uKzs5cuXbOjQoczc3JyJRCLm5OTEtm3bJnfchQsXGAD2119/sRkzZrAqVaowgUDAEhMTC33OgoODGQD23XffFbo9JyeH1ahRg5mYmMg+PJ89e8YAsOXLl7NVq1axatWqMW1tbda6dWsWHh5e4BzFeZ7zXruLFy+y0aNHMzMzM2ZsbMwYYyw6OpqNHj2a1axZk2lra7NKlSqx3r17s2fPnhU4/tOfvKTIw8ODeXh4FHie9u/fzxYsWMCsra2ZWCxm7dq1Y//991+Bx/Dbb78xe3t7pq2tzZo1a8YuX75c4JyFiYmJYZqamqxDhw6f3S9PXiL033//scGDBzMjIyNmaGjIhgwZwtLT0+X23b59O2vbti0zMzNjIpGI1alTh/3+++8Fzmlra8u6dOnCzpw5w5ydnZlYLJYlZcU9B2OMnTp1irVu3Zrp6+szAwMD1rRpU7Znzx7GGPf8fvrcf5yAFPf9AYCNGTOG/fnnn8zJyYlpamqyI0eOyLbNmTNHtm9KSgr78ccfZe9LMzMz5unpyUJCQr4YU97f8I4dO+Su/+jRI9anTx9mamrKtLW1Wc2aNQutuflUtWrV2JAhQwqUF5YIMcZY7969GQD2+vVrWdlff/0lq9EuTFJSEjM2Nma1a9eWle3bt48BYAsXLvxijHnu3r3LALCAgIBiH0NKl6ZK7rcRtZbXZsTExERW9uDBA7Rs2RLW1taYOnUq9PT0cODAAXh7e+Pw4cPo0aMHACAtLQ3u7u549OgRvvvuOzRp0gQJCQkIDAzEy5cvYWpqCqlUim7duuHKlSv4/vvvUadOHYSHh2P16tWIjIzE0aNHC42radOmcHBwwIEDBzB48GC5bfv374eJiQm8vLwAAHFxcWjevDkEAgHGjh0LMzMznD59GsOGDUNKSgp++uknuePnz58PkUiEyZMnIysrCyKRqNAYjh8/DgAYNGhQods1NTXh6+uLefPm4erVq/D09JRt27VrF1JTUzFmzBhkZmZi7dq1aNeuHcLDw2FhYaHQ85znhx9+gJmZGWbPno309HQAwK1bt3Dt2jX069cPVatWRXR0NDZu3Ig2bdrg4cOH0NXVRevWrTF+/HisW7cO06dPR506dQBA9rsoS5YsgYaGBiZPnozk5GQsW7YMAwYMwI0bN2T7bNy4EWPHjoW7uzsmTJiA6OhoeHt7w8TEBFWrVv3s+U+fPo3c3Fz4+fl9dr9P9e3bF/b29li8eDFCQ0Pxxx9/wNzcHEuXLpWLq27duujWrRs0NTVx/Phx/PDDD5BKpRgzZozc+SIiItC/f3+MHDkSI0aMQK1atRQ6h7+/P7777jvUrVsX06ZNg7GxMe7cuYMzZ87A19cXM2bMQHJyMl6+fInVq1cDAPT19QFA4ffHP//8gwMHDmDs2LEwNTWFnZ1doc/RqFGjcOjQIYwdOxZOTk549+4drly5gkePHqFJkyafjakw9+7dg7u7O7S0tPD999/Dzs4OT58+xfHjx7Fw4cIij3v16hVevHiBJk2aFLnPp/IaaxsbG8vKvvReNDIyQvfu3bFz5048efIE1atXR2BgIAAo9Pfl5OQEHR0dXL16tcD7j5QRfGdipPzKqxU4d+4ci4+PZzExMezQoUPMzMyMicVi2e0Hxhhr3749q1+/vtw3UqlUytzc3OTuqc+ePbvIb0951eC7d+9mGhoaBaqmN23axACwq1evyso+rhFijLFp06YxLS0t9v79e1lZVlYWMzY2lqulGTZsGLOysmIJCQly1+jXrx8zMjKS1dbk1XQ4ODgU6/aHt7c3A1BkjRFjjAUEBDAAbN26dYyx/G/TOjo67OXLl7L9bty4wQCwCRMmyMqK+zznvXatWrUq0G6jsMeRV5O1a9cuWdnnbo0VVSNUp04dubZDa9euZQBkNVtZWVmscuXKrFmzZnLtU/z9/RmAL9YITZgwgQFgd+7c+ex+efJqhD6toevRowerXLmyXFlhz4uXlxdzcHCQK7O1tWUA2JkzZwrsX5xzJCUlMQMDA+bq6lrgNtXHt4KKug2lyPsDANPQ0GAPHjwocB58UiNkZGTExowZU2C/jxUVU2E1Qq1bt2YGBgbs+fPnRT7Gwpw7d65A7W0eDw8PVrt2bRYfH8/i4+PZ48eP2c8//8wAsC5dusjt26hRI2ZkZPTZa61atYoBYIGBgYwxxho3bvzFYwpTs2ZN9s033yh8HCkd1GuMfDVPT0+YmZnBxsYGvXv3hp6eHgIDA2Xf3t+/f49//vkHffv2RWpqKhISEpCQkIB3797By8sL//33n6yX2eHDh9GwYcNCvzkJBAIAwMGDB1GnTh3Url1bdq6EhAS0a9cOAHDhwoUiY/Xx8UFOTg4CAgJkZWfPnkVSUhJ8fHwAAIwxHD58GF27dgVjTO4aXl5eSE5ORmhoqNx5Bw8eDB0dnS8+V6mpqQAAAwODIvfJ25aSkiJX7u3tDWtra9m6i4sLXF1dcerUKQCKPc95RowYAaFQKFf28ePIycnBu3fvUL16dRgbGxd43IoaOnSoXG2Zu7s7ACAqKgoAcPv2bbx79w4jRoyApmZ+hfWAAQPkahiLkvecfe75LcyoUaPk1t3d3fHu3Tu51+Dj5yU5ORkJCQnw8PBAVFQUkpOT5Y63t7eX1S5+rDjn+Pvvv5GamoqpU6dCW1tb7vi898DnKPr+8PDwgJOT0xfPa2xsjBs3bsj1iiqp+Ph4XL58Gd999x2qVasmt+1Lj/Hdu3cAUOTfw+PHj2FmZgYzMzPUrl0by5cvR7du3Qp03U9NTf3i38mn78WUlBSF/7byYqUhGsouujVGvtqGDRtQs2ZNJCcnY/v27bh8+TLEYrFs+5MnT8AYw6xZszBr1qxCz/H27VtYW1vj6dOn6NWr12ev999//+HRo0cwMzMr8lxFadiwIWrXro39+/dj2LBhALjbYqamprIPivj4eCQlJWHLli3YsmVLsa5hb2//2Zjz5P0TTU1Nlaum/1hRyVKNGjUK7FuzZk0cOHAAgGLP8+fizsjIwOLFi7Fjxw68evVKrjv/px/4ivr0Qy/vwywxMREAZGPCVK9eXW4/TU3NIm/ZfMzQ0BBA/nOojLjyznn16lXMmTMHwcHB+PDhg9z+ycnJMDIykq0X9fdQnHM8ffoUAFCvXj2FHkMeRd8fxf3bXbZsGQYPHgwbGxs4Ozujc+fOGDRoEBwcHBSOMS/xLeljBFDkMBN2dnbYunUrpFIpnj59ioULFyI+Pr5AUmlgYPDF5OTT96KhoaEsdkVjLU4SS/hBiRD5ai4uLmjatCkArtaiVatW8PX1RUREBPT19WXjd0yePLnQb8lAwQ++z5FKpahfvz5WrVpV6HYbG5vPHu/j44OFCxciISEBBgYGCAwMRP/+/WU1EHnxDhw4sEBbojwNGjSQWy9ObRDAtaE5evQo7t27h9atWxe6z7179wCgWN/SP1aS57mwuMeNG4cdO3bgp59+QosWLWBkZASBQIB+/foVORZLcX1a+5SnqA81RdWuXRsAEB4ejkaNGhX7uC/F9fTpU7Rv3x61a9fGqlWrYGNjA5FIhFOnTmH16tUFnpfCnldFz1FSir4/ivu327dvX7i7u+PIkSM4e/Ysli9fjqVLlyIgIADffPPNV8ddXJUrVwaQnzx/Sk9PT65tXcuWLdGkSRNMnz4d69atk5XXqVMHYWFhePHiRYFEOM+n78XatWvjzp07iImJ+eL/mY8lJiYW+kWGlA2UCBGlEgqFWLx4Mdq2bYvffvsNU6dOlX1j1NLSkvsHVRhHR0fcv3//i/vcvXsX7du3L9G3LB8fH8ybNw+HDx+GhYUFUlJS0K9fP9l2MzMzGBgYQCKRfDFeRX377bdYvHgxdu3aVWgiJJFIsHfvXpiYmKBly5Zy2/77778C+0dGRspqShR5nj/n0KFDGDx4MFauXCkry8zMRFJSktx+qviGmzc43pMnT9C2bVtZeW5uLqKjowskoJ/65ptvIBQK8eeffyrcYPpzjh8/jqysLAQGBsp9aH7uNmxJz+Ho6AgAuH///me/IBT1/H/t++NzrKys8MMPP+CHH37A27dv0aRJEyxcuFCWCBX3enl/q196rxcmL9l99uxZsfZv0KABBg4ciM2bN2Py5Mmy5/7bb7/FX3/9hV27dmHmzJkFjktJScGxY8dQu3Zt2evQtWtX/PXXX/jzzz8xbdq0Yl0/NzcXMTEx6NatW7H2J6WP2ggRpWvTpg1cXFywZs0aZGZmwtzcHG3atMHmzZvx5s2bAvvHx8fLlnv16oW7d+/iyJEjBfbL+3bet29fvHr1Clu3bi2wT0ZGhqz3U1Hq1KmD+vXrY//+/di/fz+srKzkkhKhUIhevXrh8OHDhf6j/jheRbm5ucHT0xM7duwodOTaGTNmIDIyEr/88kuBb+pHjx6Va+Nz8+ZN3LhxQ/YhpMjz/DlCobBADc369eshkUjkyvT09ACgQIL0NZo2bYrKlStj69atyM3NlZXv2bOnyBqAj9nY2GDEiBE4e/Ys1q9fX2C7VCrFypUr8fLlS4Xiyqsx+vQ24Y4dO5R+jo4dO8LAwACLFy9GZmam3LaPj9XT0yv0VuXXvj8KI5FIClzL3NwcVapUQVZW1hdj+pSZmRlat26N7du348WLF3LbvlQ7aG1tDRsbG4VGCP/ll1+Qk5MjV0vWu3dvODk5YcmSJQXOJZVKMXr0aCQmJmLOnDlyx9SvXx8LFy5EcHBwgeukpqZixowZcmUPHz5EZmYm3Nzcih0vKV1UI0RU4ueff0afPn3g7++PUaNGYcOGDWjVqhXq16+PESNGwMHBAXFxcQgODsbLly9lQ9D//PPPOHToEPr06YPvvvsOzs7OeP/+PQIDA7Fp0yY0bNgQfn5+OHDgAEaNGoULFy6gZcuWkEgkePz4MQ4cOICgoCDZrbqi+Pj4YPbs2dDW1sawYcOgoSH/nWDJkiW4cOECXF1dMWLECDg5OeH9+/cIDQ3FuXPn8P79+xI/N7t27UL79u3RvXt3+Pr6wt3dHVlZWQgICMDFixfh4+ODn3/+ucBx1atXR6tWrTB69GhkZWVhzZo1qFy5Mn755RfZPsV9nj/n22+/xe7du2FkZAQnJycEBwfj3LlzslsSeRo1agShUIilS5ciOTkZYrEY7dq1g7m5eYmfG5FIhLlz52LcuHFo164d+vbti+joaPj7+8PR0bFYNQ4rV67E06dPMX78eAQEBODbb7+FiYkJXrx4gYMHD+Lx48dyNYDF0bFjR4hEInTt2hUjR45EWloatm7dCnNz80KTzq85h6GhIVavXo3hw4ejWbNm8PX1hYmJCe7evYsPHz5g586dAABnZ2fs378fEydORLNmzaCvr4+uXbsq5f3xqdTUVFStWhW9e/eWTStx7tw53Lp1S67msKiYCrNu3Tq0atUKTZo0wffffw97e3tER0fj5MmTCAsL+2w83bt3x5EjR4rd9sbJyQmdO3fGH3/8gVmzZqFy5coQiUQ4dOgQ2rdvj1atWmHo0KFo2rQpkpKSsHfvXoSGhmLSpElyfytaWloICAiAp6cnWrdujb59+6Jly5bQ0tLCgwcPZLW5H3f///vvv6Grq4sOHTp8MU7Ck9LvqEYqiqIGVGSMG6HW0dGROTo6yrpnP336lA0aNIhZWloyLS0tZm1tzb799lt26NAhuWPfvXvHxo4dKxv6vmrVqmzw4MFyXdmzs7PZ0qVLWd26dZlYLGYmJibM2dmZzZs3jyUnJ8v2+7T7fJ7//vtPNujblStXCn18cXFxbMyYMczGxoZpaWkxS0tL1r59e7ZlyxbZPnndwg8ePKjQc5eamsrmzp3L6taty3R0dJiBgQFr2bIl8/f3L9B9+OMBFVeuXMlsbGyYWCxm7u7u7O7duwXOXZzn+XOvXWJiIhs6dCgzNTVl+vr6zMvLiz1+/LjQ53Lr1q3MwcGBCYXCYg2o+OnzVNRAe+vWrWO2trZMLBYzFxcXdvXqVebs7Mw6depUjGeXGxn4jz/+YO7u7szIyIhpaWkxW1tbNnToULmu9UWNLJ33/Hw8iGRgYCBr0KAB09bWZnZ2dmzp0qVs+/btBfbLG1CxMMU9R96+bm5uTEdHhxkaGjIXFxf2119/ybanpaUxX19fZmxsXGBAxeK+P/D/ARULg4+6z2dlZbGff/6ZNWzYkBkYGDA9PT3WsGHDAoNBFhVTUa/z/fv3WY8ePZixsTHT1tZmtWrVYrNmzSo0no+FhoYyAAWGCChqQEXGGLt48WKBIQEYY+zt27ds4sSJrHr16kwsFjNjY2Pm6ekp6zJfmMTERDZ79mxWv359pqury7S1tVm9evXYtGnT2Js3b+T2dXV1ZQMHDvziYyL8ETBWTmZ4JERNRUdHw97eHsuXL8fkyZP5DocXUqkUZmZm6NmzZ6G3fIj6ad++PapUqYLdu3fzHUqRwsLC0KRJE4SGhirUeJ+ULmojRAgpUzIzMwu0E9m1axfev3+PNm3a8BMUKXMWLVqE/fv3y4ZcKIuWLFmC3r17UxJUxlEbIUJImXL9+nVMmDABffr0QeXKlREaGopt27ahXr166NOnD9/hkTLC1dUV2dnZfIfxWfv27eM7BFIMlAgRQsoUOzs72NjYYN26dXj//j0qVaqEQYMGYcmSJUXO4UYIISVFbYQIIYQQoraojRAhhBBC1BYlQoQQQghRW2rXRkgqleL169cwMDCgSfAIIYSQcoIxhtTUVFSpUqXAILhfQ+0SodevXys0WR4hhBBCyo6YmBhUrVpVaedTu0TIwMAAAPdEGhoa8hwNIYQQQoojJSUFNjY2ss9xZVG7RCjvdpihoSElQoQQQkg5o+xmLdRYmhBCCCFqixIhQgghhKgtSoQIIYQQorYoESKEEEKI2qJEiBBCCCFqixIhQgghhKgtSoQIIYQQorYoESKEEEKI2qJEiBBCCCFqixIhQgghhKgtXhOhy5cvo2vXrqhSpQoEAgGOHj36xWMuXryIJk2aQCwWo3r16vD391d5nIQQQgipmHhNhNLT09GwYUNs2LChWPs/e/YMXbp0Qdu2bREWFoaffvoJw4cPR1BQkIojJYQQQkhFxOukq9988w2++eabYu+/adMm2NvbY+XKlQCAOnXq4MqVK1i9ejW8vLxUFSYhhBBCKqhyNft8cHAwPD095cq8vLzw008/8RMQIYQQQgCpBMhOBbKSgNxMQJIFZCUCTMot52Zxv3PSAWk2IMkGMhOBD7GArgUgzQGkudxPbgYQFwKY1geYBGASSHNz8eCRVCWhl6tEKDY2FhYWFnJlFhYWSElJQUZGBnR0dAock5WVhaysLNl6SkqKyuMkhBBCyhVJDpCVDGS+B1KeA5JMLqlJigIy3wEaIu53eiz3OysFyEnj1plENTHF3gQAvEnRx9D93rj01FollylXiVBJLF68GPPmzeM7DEIIIUT1GOMSmvQ3XA1NRgKX3KS/4ZKWrCTgw9v/JzTvuX1z07lanDLo2P1aGH6wGxLS9QCoJsZylQhZWloiLi5OriwuLg6GhoaF1gYBwLRp0zBx4kTZekpKCmxsbFQaJyGEEKJU2WlcMpP2GkiM5BKYl5cAHTPgQxz3k/7/39IcFQcjAET6gJY+91uoDSSEA9bugK45oKUHCMXcT0o0d4tLZMCty7aJuB/GAA1NQGTI/dbQ+v9vIeITsjFgzj9IT88FAJib6eBtvPIfTblKhFq0aIFTp07Jlf39999o0aJFkceIxWKIxWJVh0YIIYQoRprLJTYf3gIZ8VwNTepL4G0Y8P4hoKn7/3Y0cVxNjrKJjbkERGzEJSjalbgyHVOubU+l2oC2CZewaFfikhxdC0CnMiBQfadzMwtgzRoxRow4Dm/v2li1ygMODnOUfh1eE6G0tDQ8efJEtv7s2TOEhYWhUqVKqFatGqZNm4ZXr15h165dAIBRo0bht99+wy+//ILvvvsO//zzDw4cOICTJ0/y9RAIIYSQgqQSLoFJjARSXgDvH3G3qlJeAKkxQHyYki8o4BIYPUsuYdExA3TNALEJoFOJS2D0LAHtyly5jilX81KGSCRS5OZKIRbnxzVsWGPY2BiiY0dHpKamquS6vD4Lt2/fRtu2bWXrebewBg8eDH9/f7x58wYvXryQbbe3t8fJkycxYcIErF27FlWrVsUff/xBXecJIYSUnpwPXDKTGgMkR3PtcD7EAU8DudqTjHdc7Y4yGhFragN6VQCDqoC+NZfAmNTikh3tStwtJyM7rrwUamlUJSYmGYMGHUW9emZYv76zrFwgEMDLq7pKry1gjDGVXqGMSUlJgZGREZKTk2FoaMh3OIQQQsqa3Cwg7RXXe+rlZa47d2oMkBjB3cpKf/N15xcIudoZDS3uFpSxA9e+RrsSoF+FS3wMbQF9qzJXa6MKBw48wMiRJ5CUxDWGPnnSF5071yiwn6o+vyv+M0wIIYR86kM8kHCfu2WV9AR4cwN4fU155zdvzNXgGDsCBtX+X5tTmWt3o2cFaAiVd61yKiUlC+PHn8bOnXdlZTY2hjAwEJVqHJQIEUIIqZgY4xohv48EYm8AMZe4HlUJ94G0lyU4oQDQs+ASGyN7wMCGWzaw5trk6FfhEh5NbaU/lIomODgGAwceQVRUoqzMx6cuNm7sAhOTwnuBqwolQoQQQsovqYQb4C/pKTc2zvtHwPsILtGJD+cSoeLS0OR6cgFAdW8u2TGp+f92OTW5H0pyvkpurhQLF17G/PmXIZFwLXMMDETYsKEzBg5sAIFAUOoxUSJECCGk7JPmconNq3+5hOfmYq7bd3ZayRolV2nJJTqGtkC1dtwtLH1rtWiTw5d37z6ga9e/EBycXxvn5maDP//sAXt7E97iolecEEJI2cCkQOorrlFycjTw4jy3LMkGkp8WHP04K/nz5xMZAmYNuAH9jB25W1mmDbhloZbKHgYpnLGxNjQ1uZ5tQqEAs2d7YPp0d1kZXygRIoQQUroY43pfJT8D3lwHXl3hGiwnR3E9tBTl0IUbJ4cxoGprwLwR13ZH27hcdymvaIRCDeze3QM9ex7Ahg2d0bx5Vb5DAkCJECGEEFXKTuV6ZL0N46ZhSIkG4u9+uTbnU9qVARsPwKo51zDZpg13W4uHNiWkeC5dioaOjhZcXPInS7W1Ncbt2yN4aQtUFEqECCGEfL3cLODtHS7JSX7G/Y4P49rzFIdQBBjaASY1gEp1uNtYhraAcXXuR5OmSiovsrMlmDPnApYuvQp7exOEhY2EgUH+61eWkiCAEiFCCCGKyE4FEh4ACffyE553j7jBB1HM8Xn1LAGzRlwvLOPqXE2PaT26jVUBREQkwNc3AKGh3KCTUVGJ2LjxNn75pSXPkRWNEiFCCCEF5TVcfnuHa8eTcI+7vZX2qvjnEBsDlWpxjZYtmgI1e3OJj5hG9a9oGGPYujUUP/10BhkZ3BAEWloaWLiwHSZNcuM5us+jRIgQQgjXM+vlZe4n9ibwOhjITinesSJDLuExrgFYOHPLpvW521tl7DYIUb74+HSMGHEcx45FyMpq1aqMvXt7oUkTKx4jKx5KhAghRN1kvOdqep6d5ObQSnrKjbYszfn8cSJD7haWSS3Aoglg5ABUdqJGy2osKOgJhgw5htjYNFnZqFHOWLnSC7q65WOIAkqECCGkomKMu5X1+ho32nLYBkCSWbweW2IjwKoFl/hYunBzZxk7UsJDZOLi0uDtvR+ZmdytMFNTXWzf3g1du9biOTLFUCJECCEVhTQXeHMTeH4WeHYKiL1V/GNNanHJjmNXwLolNw4PJT3kMyws9LFkSXv89FMQvLwc4e/vDUtLfb7DUhglQoQQUl5lJgFPjnI9t96GcolPcQYk1LMEHLoCVdy42dAr1+FqgAj5DKmUQSKRQktLKCsbN84VVasaokePOtDQKJ+JMyVChBBSXiQ8ACIPcaMwvw0F3j38/P5iY+53/eHcQISVnbjEh2p6iILevEnFkCHH0KiRBZYu7SAr19AQoFcvJx4j+3qUCBFCSFmVHgs8P8f15Io+wzVs/hxDO8DKFajqAdi05QYn1BB+/hhCvuDYsccYNiwQ795l4O+/n8LLqzratbPnOyyloUSIEELKiuw0bt6tl5eB6CCu1udzLJy5H2t3oKo713uLECVJT8/GpElnsXlziKzMwqL8tQH6EkqECCGELzkZwKvLwH1/4P1jbi4uJil8X6EYsG4F2HYALJtxPbq0dEo1XKI+QkJew9c3AJGR72Rl3bvXwh9/dIOpqS6PkSkfJUKEEFKakqO5Bs5RJ7lu7bkfit7XtB5g6wVUa8f15KIGzUTFJBIpVqy4hpkzLyA3VwoA0NXVwpo1Xhg+vEmZmydMGSgRIoQQVcp4B7z4h+vSHv7H5/cVigGnQYCtJ1f7o1+ldGIkBEBCwgf06XMQFy9Gy8qcna2wd28v1KxZmb/AVIwSIUIIUSZpLlfTE3WS6+GVHFX0vjpmgF1Hriu7XUdA26T04iTkE0ZGYqSlZQPgOhZOndoKc+e2gUhUsRvcUyJECCFfK/Ul8OwM17Prv8NF76ehyc3HVa0d16XdvFGphUjIl2hpCbFnT094e+/Dxo1d4OFhx3dIpYISIUIIUVR6LPBgJzeOT9QJIPN94fsJNADzJlyPLtuO3ACGNPM6KSOCg2Ogq6uFhg0tZWU1a1bG/fs/lNvBEUuCEiFCCPmStNdAzAWuW/uLf7gBDYsiNuZqfMybcLU+ehalFiYhxZGbK8XChZcxf/5l1KxZGbdvfy83Qao6JUEAJUKEEFJQTgYQ8w/w7DTw/G8gMfLz+1u6AGYNgDoDuUbONIghKaOiohIxcGAAgoNfAgAePUrA77/fwuTJbjxHxh9KhAghBOC6tT//G3hyhKv9yc0sfD8NLcCsIVCt/f97d7kDmuJSDZUQRTHGsHv3PYwdewqpqVyDaKFQgDlzPPDTT815jo5flAgRQtRXxjvgvyNAxD7gxfnC9xEIuXm6qrpzyU+VFoCWXunGSchXSEzMwKhRJ3HgwANZmaOjCf78syeaN6/KY2RlAyVChBD1wRg3Q/urf7kE6PXVwvfTswTsOgGO3bn2PtTAmZRTFy9Gw8/vCF6+TJGVDR3aCGvXdoKBAdVkApQIEULUwdu7wN3fgciDQGZi4fsY2gF1fLkxfaxcuB5fhJRjb96kwsvrT2Rnc9O2mJhoY/Pmb9GnT12eIytbKBEihFQ8TMp1bb+zHnh1FXj3oPD9jKsD9p2BWj7cLa8KOH0AUV9WVgaYM8cDM2b8g7Zt7bBrVw9UrUq1m5+iRIgQUjHkZgIxF7kBDf8LKHxsHy09oKoHl/xUawdUrlPaURKiMowxSKUMQmF+beaUKS1hY2OIAQMaqF23+OKiRIgQUn5JJcDrYOCBP/B4L5CbUcSOAsBjBVBvKE1jQSqk+Ph0jBhxHI0bW2LOnDaycqFQA35+DfkLrBygRIgQUr4wBsTe5JKfiINA5ruC+2jqcCM5W7fkan8qO9FtL1JhBQU9wZAhxxAbm4YTJyLRsaMjWrSw4TuscoMSIUJI+ZCTAYRvAULWACnRBbeLDIDq3lzi49CFWyekAsvMzMW0aeewZs0NWZmJiY5snCBSPJQIEULKrqxkrpv700BusMOcNPntmjqAbQegeg+gRk/q5k7URnh4HAYMCEB4+FtZmZeXI/z9vWFpqc9jZOUPJUKEkLKFMeDlJeDuJuDpscJHeLZ25xKfukMAbePSjpAQ3kilDOvX38CUKeeQlcV1ixeLhVi2rAPGjnWhBtElQIkQIaRsiL+Xn/ykvS64XbsSd8uryQTAonHpx0cIz969+4ABAwIQFPRUVla/vjn27u2FevXMeYysfKNEiBDCD8aAuBCut9fLy9zyp3RMgZq9gZp9uG7vNJkpUWN6eiK8epUqW58woTkWLWoPbW36KP8a9OwRQkpXxjvg/nbg4W4gIbzgdoEQsGkD1BsG1OwFCEWlHiIhZZG2tib27u2J7t33YdOmb9GxoyPfIVUIlAgRQlQvJwN4E8zd+npyFJDmFNzH2BFoPA6oMxDQqVzqIRJS1oSEvIaengi1a5vKyurXt0Bk5DhoatIUMMpCiRAhRHWSn3HJz63lAFjB7eaNuZofh86AkX2ph0dIWSSRSLFixTXMnHkB9eqZ4/r1YRCL8z+uKQlSLkqECCHKlfEeeLgLeLQHiLtdcLuOGeDkB9Qfxg10SAiRiYlJhp/fEVy69BwAEBYWi99/v4UJE1rwHFnFRYkQIUQ53t4FwjYAj3YX7PKuoQnYeQE1egO1+wOaYn5iJKQMO3DgAUaOPIGkJO79IxAAU6e2wpgxLjxHVrFRIkQIKTlJDlf783gv8OKfgtsr1eba/Dj5AYbVSj8+QsqBlJQsjB9/Gjt33pWV2dgYYvfuHvDwsOMvMDVBiRAhRHGZidyIz7dXAO8ffbJRwA102HgcjfdDyBcEB8dg4MAjiIpKlJX5+NTFxo1dYGKiw2Nk6oMSIUJI8SX+BwTPAyIPApJP5jMytAUajeWSIF3TQg8nhOR79SoFbdrsRHY2N0K0gYEIGzZ0xsCBDSCgSYJLDSVChJAve3IMuLcFiD4DMKn8NoumQOtl3Ng/9M+bkGKztjbE5MktsGjRFbi52eDPP3vA3t6E77DUDiVChJDCMQZEnQCuzQHe3pHfpqkL1PEF6g8HLF0oASKkGBjjhpD4uLZn7tw2qFbNCMOGNaFu8TyhRIgQIk+SAzw7DVybDcTfld+mZwnUHwE4TwC06ZsrIcWVmJiBUaNOolmzKpg82U1WrqUlxMiRTXmMjFAiRAjhMClwfwdwZQbwIU5+m2l9wGUK1/VdQN9aCVHExYvR8PM7gpcvU3DkyCO0b2+Pxo2t+A6L/B8lQoQQIOIAEPwr8O6BfLlpfcB1BlCrDyVAhCgoO1uC2bMvYNmyq/j/XTHo64sQG5vGb2BEDiVChKgrxoCnx4Hrvxac+d22I9BoDODYldr/EFICEREJ8PUNQGjoG1lZ27Z22LWrB6pWNeQxMvIpSoQIUTc56cCdDcC/U1Fg/q/KdYFWi4Dq3XgJjZDyjjGGLVtCMGFCEDIycgEAWloaWLiwHSZNcoOGBn2xKGsoESJEXST+B9xZD4RvLTgFRqU6QPOZ/28DRP+oCSmJ9+8zMHToMQQGRsjKatWqjL17e6FJE2oTVFZRIkRIRZf8DDg/Fnh2quA2s4ZArb6Ay1RqA0TIVxKLhXj8OEG2Pnp0U6xY0RG6ulo8RkW+hBIhQiqqrGQgdC1wcymQ+yG/XEMTqOUDNPkRsGzGX3yEVDB6eiLs2dMT3bvvw6ZNXdC1ay2+QyLFQIkQIRVN8jPuFtjdzfIJkLYJ4DwRqPcdoF+Fv/gIqSDCw+OgpyeCg0P+mFpNm1ZBVNR4iMX08Vpe0CtFSEXx7iFwexU3G7w0J79coAHUGwa4LwF0KvEXHyEVhFTKsH79DUyZcg6NG1vh33+Hyo0KTUlQ+UKvFiHlXeJ/wPUFwKM/5ecB09DipsFwmQZUoip6QpThzZtUDBlyDGfPPgUAXL/+Ehs33sK4ca48R0ZKivfWkRs2bICdnR20tbXh6uqKmzdvfnb/NWvWoFatWtDR0YGNjQ0mTJiAzMzMzx5DSIWU+goIGgZsr8nVAuUlQVr6QNOfge9jgE7+lAQRoiTHjj1G/fobZUkQAEyY0BwjRjjzGBX5WrzWCO3fvx8TJ07Epk2b4OrqijVr1sDLywsREREwNzcvsP/evXsxdepUbN++HW5uboiMjMSQIUMgEAiwatUqHh4BITxIiQGC5wIPdgJMkl8u0ADc5gFNfgJE+nxFR0iFk56ejUmTzmLz5vyBR62s9OHv742OHR15jIwog4DlTYfLA1dXVzRr1gy//fYbAEAqlcLGxgbjxo3D1KlTC+w/duxYPHr0COfPn5eVTZo0CTdu3MCVK1eKdc2UlBQYGRkhOTkZhoY0uicpR7JSgLDfgevz5RtBi42AOn6A63RAn8YqIUSZQkJew9c3AJGR72Rl3t61sXVrV5ia6vIYmfpR1ec3b7fGsrOzERISAk9Pz/xgNDTg6emJ4ODgQo9xc3NDSEiI7PZZVFQUTp06hc6dOxd5naysLKSkpMj9EFLuPPoL2FEbuDItPwnS1OZugQ2NANqvpySIECWLiUmGm9t2WRKkq6uFrVu7IiCgLyVBFQhvt8YSEhIgkUhgYWEhV25hYYHHjx8Xeoyvry8SEhLQqlUrMMaQm5uLUaNGYfr06UVeZ/HixZg3b55SYyek1CQ8AM4OB95cly+vPwJwXwzoVOYnLkLUgI2NEX74oSnWrLkBZ2cr7N3bCzVr0nuuouG9sbQiLl68iEWLFuH3339HaGgoAgICcPLkScyfP7/IY6ZNm4bk5GTZT0xMTClGTEgJfYgHznwH7GoonwTZtAUGhwMdt1ASRIgKfNpaZPFiT6xa1RHXrg2jJKiC4q1GyNTUFEKhEHFxcXLlcXFxsLS0LPSYWbNmwc/PD8OHDwcA1K9fH+np6fj+++8xY8YMaGgUzOvEYjHEYrHyHwAhqpCbCYSsAW4t4UaGzmPsCDSfDdQdxFtohFRkKSlZGD/+NFxcrPHDD/kjrmtra2LChBY8RkZUjbcaIZFIBGdnZ7mGz1KpFOfPn0eLFoX/0X348KFAsiMUCgEUzOIJKXdeXgb863LtgPKSIJEB4PYrMCickiBCVCQ4OAaNGm3Czp13MWnSWTx6FM93SKQU8dp9fuLEiRg8eDCaNm0KFxcXrFmzBunp6Rg6dCgAYNCgQbC2tsbixYsBAF27dsWqVavQuHFjuLq64smTJ5g1axa6du0qS4gIKXey04Bby4AbCz8aEFEA1OzNtQMypu65hKhCbq4UCxZcxoIFlyGRcF+mtbQ08PRpIurUMeM5OlJaeE2EfHx8EB8fj9mzZyM2NhaNGjXCmTNnZA2oX7x4IVcDNHPmTAgEAsycOROvXr2CmZkZunbtioULF/L1EAj5Os/PA+dHc6ND5zGtD3TaCVg05i8uQiq4qKhEDBwYgODgl7IyNzcb/PlnD9jbm3zmSFLR8DqOEB9oHCFSJrx7CFycCEQHfVQoAJr9DLRayM0QTwhROsYYdu26i7FjTyMtLRsAIBQKMHu2B6ZPd5ebM4yULar6/Kb/toSUpvRY4NJkIGI/IM3NLzepBXQ7BJjW4y82Qiq4pKRMjBx5AgcOPJCVOTiYYM+enmjevCqPkRE+USJESGmQ5AD3NgP/TgVy0vPLtStzNUANvgcEAv7iI0QNCATAjRv5t8KGDGmEdes6wcCAeharM0qECFG1Nze5QRETwvPLtPSARmOA5jO5nmGEEJUzMtLG7t090LPnAfz+e2f06VOX75BIGUCJECGqkvIcODsCeP63fHltX8BjBU2JQYiKRUQkQE9PhKpV89uTuLvbIjr6R+jpiXiMjJQl1CqMEGVjUuDBLm5MoI+TIJNagM+/QJc9lAQRokKMMWzefBuNG2/GoEFHIJXK9wmiJIh8jBIhQpQpJQY40A44Mzi/LZC2CdBmFeB3B6jait/4CKng4uPT4e29H6NGnURGRi4uXIjGli0hfIdFyjC6NUaIskSdBE4PBjLf5Zc5dgO8ttO8YISUgqCgJxgy5BhiY9NkZaNGOWPQoIY8RkXKOkqECPlakmzgxiIg+FcA/6+CFxsDnpuAWn2pNxghKpaZmYtp085hzZobsjJTU11s394NXbvW4jEyUh5QIkTI10h5DpzoJz9DvLU70PUAoFf45MGEEOUJD4/DgAEBCA9/Kyvz8nKEv783LC31eYyMlBeUCBFSUlEngVMDgawkbl2gwc0Q32IWt0wIUannz5PQrNlWZGVJAABisRDLlnXA2LEu0NCgmlhSPPTfmhBFMSlwZQZw5Nv8JEi/CtArCHCbQ0kQIaXE1tZY1v6nfn1z3L79PcaPd6UkiCiEaoQIUUTGO+DUAPk5wqp5Al3+AnRN+YuLEDW1erUXbG2NMGmSG7S16SONKI6+uhJSXGlvgH2tPkqCBECzX4DeZykJIkTF0tOzMWrUCfj7h8mV6+mJMGNGa0qCSInRXw4hxRF3BzjUIb9rvKYuVwtUvRu/cRGiBkJCXmPAgABERLzDnj3hcHevBkfHSnyHRSoIqhEi5Ete/gvsb52fBOlaAANvURJEiIpJJFIsXXoFzZtvQ0QE9/6TShnu33/7hSMJKT6qESKkKIwBdzcCF34EpLlcmWl9oPtRwNiB19AIqehiYpLh53cEly49l5U5O1th795eqFmTBiglykOJECGFyc0E/h4JPNyVX1atHdDtCCA2LPo4QshXO3DgAUaOPIGkpEwA3JikU6e2wty5bSASCXmOjlQ0lAgR8qkPCcDxXsDLy/lljcZw84UJabJGQlQlNTUL48adxs6dd2VlNjaG2L27Bzw87PgLjFRolAgR8rHUl0BAZyAhnFvX0AI6/gHUHcRvXISogawsCc6efSpb9/Gpi40bu8DERIfHqEhFR42lCcnz8E/A3yk/CRKKgd5/UxJESCkxNdXFzp3eMDQUY9cub/z1Vy9KgojKUY0QIQBweSpwa2n+ur410OMEYN6It5AIqeiiohKhp6cFC4v8OcE6dHDE8+c/wdhYm8fIiDqhGiGi3nIzgRP95ZOgmr0BvzuUBBGiIowx7NwZhoYNN+G77wLBGJPbTkkQKU2UCBH1lZ0KBHwDROzLL2s+G/j2AKBrxl9chFRgiYkZ6NfvMIYMOYa0tGycOvUfduwI4zssosbo1hhRT+mxXKPot3e4dQ1NoMMWoN5QfuMipAK7eDEafn5H8PJliqxsyJBG6NPHiceoiLqjRIionyeBwNnhQEY8ty4yALoeBuw68BsXIRVUdrYEs2dfwLJlV5F3F8zERBubN3+LPn3q8hscUXuUCBH1EjwfuDY7f13HFOh5CrBsxl9MhFRgjx8nYMCAAISGvpGVtW1rh127eqBqVRqclPCPEiGiHhgDri+QT4KquHHtgQys+YuLkAosKioRTZpsRkYGN0WNlpYGFi5sh0mT3KChIeA5OkI41FiaVHy5WcDx3vJJUOPxQL8rlAQRokIODibo2bMOAKBWrcq4fn04fv65JSVBpEyhGiFSsWWnASf7AVEn88tcpgLui/mLiRA1smFDZ9jaGmHGjNbQ1dXiOxxCCviqGqHMzExlxUGI8mWnAQfa5CdBAiHQeQ8lQYSoQGZmLiZMOIODBx/IlRsZaWPhwvaUBJEyS+FESCqVYv78+bC2toa+vj6ioqIAALNmzcK2bduUHiAhJZIeB/zZFIgL4daFYm6k6Dq+/MZFSAUUHh4HF5etWLPmBr7//gRiYpL5DomQYlM4EVqwYAH8/f2xbNkyiET5M3HXq1cPf/zxh1KDI6REXl0DdjUEEiO4dU1doPc5wL4Tv3ERUsFIpQxr115Hs2ZbER7+FgCQkZGD27df8xwZIcWncCK0a9cubNmyBQMGDIBQKJSVN2zYEI8fP1ZqcIQo7Pk54JAn8CGOW9evwjWKrtqK37gIqWDevElF58578NNPQcjKkgAA6tc3x+3b36NHjzo8R0dI8SncWPrVq1eoXr16gXKpVIqcnBylBEVIiUQcAE72B5iUWzdrAHifAAxt+I2LkArm2LHHGD78OBISPsjKJkxojkWL2kNbm/rgkPJF4b9YJycn/Pvvv7C1tZUrP3ToEBo3bqy0wAhRyPNzwEnf/CSoqgfQ8ySgpcdvXIRUIOnp2Zg06Sw2bw6RlVlZ6cPf3xsdOzryGBkhJadwIjR79mwMHjwYr169glQqRUBAACIiIrBr1y6cOHFCFTES8nnRZ4Fj3QHGVc+jRk+ud5gmzWBNiDKlpGTh8OFHsnVv79rYurUrTE11eYyKkK+jcBuh7t274/jx4zh37hz09PQwe/ZsPHr0CMePH0eHDjRXEyllMZeAwJ5A7v+HcnD4Fuiyj5IgQlTAysoAf/zRFbq6Wti6tSsCAvpSEkTKPQFjeVPgqYeUlBQYGRkhOTkZhoY0z025FnkION4XwP//hO28gO5HKQkiREliYpKhpydCpUo6cuVv36bD3JxuO5PSparPb4VrhBwcHPDu3bsC5UlJSXBwcFBKUIR80aO98klQNU+gWwAlQYQoyYEDD9CgwSaMHHkCn35fpiSIVCQKJ0LR0dGQSCQFyrOysvDq1SulBEXIZ0UeAk4PgiwJcujCzSCvRVX0hHytlJQsDBlyFD4+h5CUlIlDhx5i795wvsMiRGWK3Vg6MDBQthwUFAQjIyPZukQiwfnz52FnZ6fU4AgpIOokcLxP/rqTH9DxD0BIw/cT8rWCg2MwYEAAnj1LkpX5+NRF5841+AuKEBUrdiLk7e0NABAIBBg8eLDcNi0tLdjZ2WHlypVKDY4QOS8uyCdB1b3/nwSJijyEEPJlublSLFx4GfPnX4ZEwtW0GhiIsGFDZwwc2AACAc0WTyquYidCUik3Pou9vT1u3boFU1NTlQVFSAGvrwNHvgVyM7h12w5A14OABg3eRsjXiIpKxMCBAQgOfikrc3OzwZ9/9oC9vQmPkRFSOhT+FHn27Jkq4iCkaPH3gIBvgNz/j2JbrR3Q/RglQYR8pSdP3qNJk81ITc0GAAiFAsye7YHp092hqalwE1JCyqUSfZKkp6fj0qVLePHiBbKzs+W2jR8/XimBEQIASHsNBHQGspK4dQtnwPs4oKXz2cMIIV/m6GiC9u0dcPToYzg4mGDPnp5o3rwq32ERUqoUToTu3LmDzp0748OHD0hPT0elSpWQkJAAXV1dmJubUyJElCcrGTjsBaT9vzdiZSeg11nqHUaIkggEAmzd2hW2tkaYP78tDAzEfIdESKlTuO5zwoQJ6Nq1KxITE6Gjo4Pr16/j+fPncHZ2xooVK1QRI1FHGe+Bg55Awn1uXdeCS4J0KvEbFyHlVHa2BFOnnsPJk5Fy5aamulizphMlQURtKZwIhYWFYdKkSdDQ0IBQKERWVhZsbGywbNkyTJ8+XRUxEnWTEgMcbAfE3ebWRQbcOEEG1vzGRUg5FRGRgBYttmHp0qv47rtAxMWl8R0SIWWGwomQlpYWNDS4w8zNzfHixQsAgJGREWJiYpQbHVE/qa+Afa2A+LvcusgA6H0OsGjCb1yElEOMMWzefBuNG29GaOgbAEBiYgauXqX/1YTkUbiNUOPGjXHr1i3UqFEDHh4emD17NhISErB7927Uq1dPFTESdZHzATjeC0jlkmvoWwM9TgDmjXgNi5DyKD4+HcOHH0dgYISsrFatyti7txeaNLHiMTJCyhaFa4QWLVoEKyvuTbRw4UKYmJhg9OjRiI+Px+bNm5UeIFETUglwwgd4c4Nb17cGBtykJIiQEggKeoIGDTbJJUGjRzdFaOhISoII+QTNPk/4J80FAnsDT49x65ragM9lwLIZv3ERUs5kZuZi2rRzWLPmhqzM1FQX27d3Q9eutXiMjJCvV2Zmny9KaGgovv32W2WdjqgLxoBTfvlJEAB03kNJECEl8PZtOnbsCJOtd+pUHeHhoykJIuQzFEqEgoKCMHnyZEyfPh1RUVEAgMePH8Pb2xvNmjWTTcNBSLEwBpwdAUTs+3+BAOjkD9ToyWdUhJRb1aoZYePGLhCLhVi3rhNOnfKFpaU+32ERUqYVu7H0tm3bMGLECFSqVAmJiYn4448/sGrVKowbNw4+Pj64f/8+6tSpo8pYSUUTshq4vy1/vdMOoO7govcnhMh58yYVenoiGBrmjwHUv399tGpVDTY2RjxGRkj5UewaobVr12Lp0qVISEjAgQMHkJCQgN9//x3h4eHYtGkTJUFEMc9OA5cm56932EpJECEKOHbsMRo02ITx408X2EZJECHFV+zG0np6enjw4AHs7OzAGINYLMaFCxfQsmVLVceoVNRYugxIegrsbgxkp3LrjcYC7dfzGxMh5UR6ejYmTTqLzZtDZGWHDvVBr15OPEZFiOqp6vO72LfGMjIyoKvLzfEkEAggFotl3egJKbYPb4FDHfKTIKvmQNvV/MZESDkREvIavr4BiIx8Jyvz9q4NDw87/oIipJxTaEDFP/74A/r6XMO73Nxc+Pv7w9TUVG4fmnSVFIkxIGgYkPyMW69Um5tJXkPhcT0JUSsSiRQrVlzDzJkXkJvLdUrR1dXC2rWdMGxYYwgEAp4jJKT8KvatMTs7uy++2QQCgaw3WXFt2LABy5cvR2xsLBo2bIj169fDxcWlyP2TkpIwY8YMBAQE4P3797C1tcWaNWvQuXPnYl2Pbo3x6PoC4OosbllkCAy6CxjZ8RoSIWVdTEwy/PyO4NKl57IyZ2cr7N3bCzVrVuYxMkJKF++3xqKjo5V20Tz79+/HxIkTsWnTJri6umLNmjXw8vJCREQEzM3NC+yfnZ2NDh06wNzcHIcOHYK1tTWeP38OY2NjpcdGlOz5ufwkCADarKIkiJAviIx8B1fXP5CUlAkAEAiAqVNbYe7cNhCJhDxHR0jFwOvI0q6urmjWrBl+++03AIBUKoWNjQ3GjRuHqVOnFth/06ZNWL58OR4/fgwtLa0SXZNqhHiQmQTsapg/h1jj8UC7tbyGREh5IJUydO68B0FBT2FjY4jdu3tQeyCitsr8yNKKys7ORkhICDw9PfOD0dCAp6cngoODCz0mMDAQLVq0wJgxY2BhYYF69eph0aJFkEgkpRU2URRjwN/f5ydBVq5A66X8xkRIOaGhIcCOHd3x/fdNcPfuKEqCCFEB3lqpJiQkQCKRwMLCQq7cwsICjx8/LvSYqKgo/PPPPxgwYABOnTqFJ0+e4IcffkBOTg7mzJlT6DFZWVnIysqSraekpCjvQZAvu7kEiDzILWvqAp12cXOJEULk5OZKsXDhZbi726JdO3tZuZWVATZv7spjZIRUbOWqu45UKoW5uTm2bNkCoVAIZ2dnvHr1CsuXLy8yEVq8eDHmzZtXypESAEDkIeDK9Pz1jn8AlWryFw8hZVRUVCIGDgxAcPBLWFsb4N690ahUSYfvsAhRC7zdGjM1NYVQKERcXJxceVxcHCwtLQs9xsrKCjVr1oRQmN9IsE6dOoiNjUV2dnahx0ybNg3Jycmyn5iYGOU9CFK0+HvA6Y9Gim48DqjTn794CCmDGGPYtesuGjXahODglwCA2Ng0XLjwjOfICFEfJUqEnj59ipkzZ6J///54+/YtAOD06dN48OBBsc8hEong7OyM8+fPy8qkUinOnz+PFi1aFHpMy5Yt8eTJE7nJXSMjI2FlZQWRSFToMWKxGIaGhnI/RMWyU4Fj3kDuB269Zm+g7Ro+IyKkzElMzEC/focxePBRpKZyX+QcHExw5cp3NEo0IaVI4UTo0qVLqF+/Pm7cuIGAgACkpaUBAO7evVvk7amiTJw4EVu3bsXOnTvx6NEjjB49Gunp6Rg6dCgAYNCgQZg2bZps/9GjR+P9+/f48ccfERkZiZMnT2LRokUYM2aMog+DqErejPJ5gyaaNQC8dgAC3iofCSlzLl6MRoMGm3DgQP6XxyFDGiEsbCSaN6/KY2SEqB+F2whNnToVCxYswMSJE2FgYCArb9eunawbfHH5+PggPj4es2fPRmxsLBo1aoQzZ87IGlC/ePECGhr5H6A2NjYICgrChAkT0KBBA1hbW+PHH3/ElClTFH0YRFVurwQi9nPLmjpAl/2ASJ/fmAgpI7KzJZgz5wKWLr2KvIFLjI21sWXLt+jTpy6/wRGiphQeR0hfXx/h4eGwt7eHgYEB7t69CwcHB0RHR6N27drIzMxUVaxKQeMIqVD0WeCwV/565z1AHV/+4iGkjImKSkSDBhuRnp4DAGjTxg67dnnTbPGEFEOZGUfI2NgYb968KVB+584dWFtbKyUoUg4lPACOdc9frz+CkiBCPuHgYIK1aztBS0sDy5Z54vz5QZQEEcIzhW+N9evXD1OmTMHBgwchEAgglUpx9epVTJ48GYMGDVJFjKSsY1IgsCeQ+//aQGt3wHMjvzERUgYkJHyArq4WdHXzR8L/7rvG8PCwQ/XqlXiMjBCSR+EaoUWLFqF27dqwsbFBWloanJyc0Lp1a7i5uWHmzJmqiJGUdWEbgcRIbtmgGtDzJKBB8yAR9RYU9AT162/Ezz+flSsXCASUBBFShpR4rrEXL17g/v37SEtLQ+PGjVGjRg1lx6YS1EZIyd49Bv5snF8b1PscYNue35gI4VFmZi6mTTuHNWtuyMpOnOiPLl1oMFFCvgbvs8/nuXLlClq1aoVq1aqhWrVqSguElEM56cCxbvlJkG1HSoKIWgsPj8OAAQEID38rK+vUqTqcnavwGBUh5HMUvjXWrl072NvbY/r06Xj48KEqYiLlxbnRQOJ/3LK+NdDlL37jIYQnUinD2rXX0azZVlkSJBYLsW5dJ5w65QtLSxpCgpCySuFE6PXr15g0aRIuXbqEevXqoVGjRli+fDlevnypivhIWRW+HXi4m1vW0AS8jwE61O6BqJ83b1LRufMe/PRTELKyJACA+vXNcfv29xg3zhUCgYDnCAkhn1PiNkIA8OzZM+zduxd//fUXHj9+jNatW+Off/5RZnxKR22ElCDmEhDwDZCbwa13/AOoP4zfmAjhQUREAlq12oGEhA+ysgkTmmPRovbQ1i5Xc1oTUuaVmXGEPmZvb4+pU6diyZIlqF+/Pi5duqSsuEhZlR4HHPk2PwlyGkRJEFFb1atXgpOTGQDAykofQUEDsWqVFyVBhJQjJU6Erl69ih9++AFWVlbw9fVFvXr1cPLkSWXGRsqim0uAHG5+OZg3ATps5jceQngkFGpg9+4e8PNrgHv3RqNjR0e+QyKEKEjhW2PTpk3Dvn378Pr1a3To0AEDBgxA9+7doaurq6oYlYpujX2F9xHAroaAJItb/y4SMCkfwyYQ8rUkEilWrLgGd3dbuLnZ8B0OIWqnzHSfv3z5Mn7++Wf07dsXpqamSguElHFZKdzo0XlJUL1hlAQRtRETkww/vyO4dOk57O2NERY2CoaGYr7DIoQogcKJ0NWrV1URBynLGANO+QLv/j9cgoEN0GYVvzERUkoOHHiAkSNPICmJGy8rOjoJZ88+Re/eTjxHRghRhmIlQoGBgfjmm2+gpaWFwMDAz+7brVs3pQRGypCQVUDU/9t/aeoC3Y8AYrqtSCq2lJQsjB9/Gjt33pWV2dgYYvfuHvDwsOMvMEKIUhWrjZCGhgZiY2Nhbm4ODY2i21cLBAJIJBKlBqhs1EZIQSkvgO0182+JdTsM1OjJb0yEqFhwcAwGDjyCqKhEWZmPT11s3NgFJiY6PEZGiPritY2QVCotdJmogQs/5idBtftTEkQqtNxcKRYuvIz58y9DIuG+IxoYiLBhQ2cMHNiABkckpAJSuPv8rl27kJWVVaA8Ozsbu3btUkpQpIx4cQF4cpRb1tQG2qzmNRxCVO3p0/dYvPiKLAlyc7PB3buj4OfXkJIgQioohROhoUOHIjk5uUB5amoqhg4dqpSgSBnwIQE4PSh/3e1XQM+Cv3gIKQW1apli2bIOEAoFmDevDS5dGgJ7exO+wyKEqJDCvcYYY4V+M3r58iWMjIyUEhQpAy78CKT9f/44K1fAeQK/8RCiAomJGdDV1YJYnP+vcNw4F7RrZ4969cx5jIwQUlqKnQg1btwYAoEAAoEA7du3h6Zm/qESiQTPnj1Dp06dVBIkKWUvLwOP93LLmjrAt/u5iVUJqUAuXoyGn98R9OtXF8uXd5SVCwQCSoIIUSPF/nTz9vYGAISFhcHLywv6+vqybSKRCHZ2dujVq5fSAySlTCoBzo/JX3dfChja8hcPIUqWnS3BnDkXsHTpVTAGrFgRjE6dqqN9ewe+QyOE8KDYidCcOXMAAHZ2dvDx8YG2trbKgiI8ur8DSLjPLVeuCzQazW88hChRREQCfH0DEBr6RlbWtq0datWiUfIJUVcK3+8YPHiwKuIgZUF6HHB5cv56m1V0S4xUCIwxbNkSggkTgpCRkQsA0NLSwMKF7TBpkhs0NKhHGCHqqlifcpUqVUJkZCRMTU1hYmLy2W6k79+/V1pwpBTlZgIB3wBZ/+8R6NgNsOv4+WMIKQfi49MxfPhxBAZGyMpq1aqMvXt7oUkTKx4jI4SUBcVKhFavXg0DAwPZMo2nUQFd/gV4e4dbFhsB7dbzGw8hShARkYA2bXYiNjZNVjZ6dFOsWNERurpaPEZGCCkrijXFRkVCU2wUIvYWsMeFWxYIgX5XgCrN+Y2JECXIyZGgZcvtuHXrNUxNdbF9ezd07VqL77AIISWgqs9vhQdUDA0NRXh4uGz92LFj8Pb2xvTp05Gdna20wEgpyc0CzgzJX2/5KyVBpMLQ0hJiz56e6NmzDsLDR1MSRAgpQOFEaOTIkYiMjAQAREVFwcfHB7q6ujh48CB++eUXpQdIVOzGQuDdQ25ZxwxoQgMnkvJJKmVYt+4G7tx5I1deo0ZlHD7cF5aW+kUcSQhRZwonQpGRkWjUqBEA4ODBg/Dw8MDevXvh7++Pw4cPKzs+okpvw7hECAAEGkCPE4AWzaxNyp83b1LRufMe/PjjGfj6BuDDhxy+QyKElBMKJ0KMMdkM9OfOnUPnzp0BADY2NkhISFBudER1cj4Ap/0Axr2WcJ4EWLnwGxMhJXDs2GM0aLAJQUFPAQCPHyfg9On/eI6KEFJeKDxITNOmTbFgwQJ4enri0qVL2LhxIwDg2bNnsLCgSTnLjcu/5A+caGgLuM3lNRxCFJWeno1Jk85i8+YQWZmVlT78/b3RsaMjj5ERQsoThROhNWvWYMCAATh69ChmzJiB6tWrAwAOHToENzc3pQdIVODRX0DYBm5ZUxvofhTQ0uU1JEIUERLyGr6+AYiMfCcr8/auja1bu8LUlP6WCSHFp7Tu85mZmRAKhdDSKttjc6h99/n3EcCuhoAki1v3WAk0nchvTIQUk0QixfLl1zBr1gXk5nK3dXV1tbBmjReGD29CY5wRUoGp6vO7xPMnhISE4NGjRwAAJycnNGnSRGlBERWRZAOBvfKToNr9AWfqJUbKj8ePE+SSIGdnK+zd2ws1a1bmOTJCSHmlcCL09u1b+Pj44NKlSzA2NgYAJCUloW3btti3bx/MzMyUHSNRlrDfgXcPuGUjB6DDZoC+QZNypG5dc8yf3xbTp5/H1KmtMHduG4hEQr7DIoSUYwr3Ghs3bhzS0tLw4MEDvH//Hu/fv8f9+/eRkpKC8ePHqyJGogw5H4Bby/LXv9kNiAz4i4eQYkhNzZLV/uT5+Wc33Lw5AosWtackiBDy1RROhM6cOYPff/8dderUkZU5OTlhw4YNOH36tFKDI0p0aTKQ/v+B5mw7AtbUsJ2UbcHBMWjUaDMWLLgsVy4UaqBp0yo8RUUIqWgUToSkUmmhDaK1tLRk4wuRMubJMeAuN8wBNDQB98X8xkPIZ+TmSjFv3kW4u+9AVFQi5s+/jGvXYvgOixBSQSmcCLVr1w4//vgjXr9+LSt79eoVJkyYgPbt2ys1OKIEWcnAudH5627zAQtq2E7KpqioRLRuvQNz516CRMJ1aG3evCqsrGh6DEKIaiicCP32229ISUmBnZ0dHB0d4ejoCHt7e6SkpGD9+vWqiJF8jYsT82+JWbUAXKbwGw8hhWCMYdeuu2jUaBOCg18CAIRCAebNa4NLl4bA3t6E3wAJIRWWwr3GbGxsEBoaivPnz8u6z9epUweenp5KD458pYQHwAN/bllLH+iyh3qJkTInMTEDo0efxP79D2RlDg4m2LOnJ5o3r8pjZIQQdaBQIrR//34EBgYiOzsb7du3x7hx41QVF1GG4Ln5c4m5TAGM7HkNh5BPRUQkoEOH3YiJSZGVDRnSCOvWdYKBgZjHyAgh6qLYidDGjRsxZswY1KhRAzo6OggICMDTp0+xfPlyVcZHSup1MBB5iFsWGwNNfuQ1HEIKY2trDGNjbcTEpMDERBubN3+LPn3q8h0WIUSNFLuN0G+//YY5c+YgIiICYWFh2LlzJ37//XdVxkZKijGubVCeZr/QmEGkTNLW1sTevb3QuXMN3Ls3mpIgQkipK/ZcYzo6Onj06BHs7OwAcN3odXR0EB0dDSsrK1XGqFRqMddYdBBwuBO3bOQADH0MCMv2HHCk4mOMYevWULRqVQ1OTjQCPSFEMar6/C52jVBWVhb09PTyD9TQgEgkQkZGhtKCIUqQmynfXb75LEqCCO/i49Ph7b0fI0eegK/vYWRl5fIdEiGEAFCwsfSsWbOgq6srW8/OzsbChQthZGQkK1u1apXyoiOKu7UcSH7GLZs1AJwG8hsPUXtBQU8wZMgxxMamAQDu3o3DiROR6NXLiefICCFEgUSodevWiIiIkCtzc3NDVFSUbF1AXbP59e4hcGPB/1cEQIet3EjShPAgMzMXU6eew9q1N2Rlpqa62L69G7p2rcVjZIQQkq/Yn5IXL15UYRjkq0lzgZMDAEk2t95kPGDlwm9MRG2Fh8fB1zcA9++/lZV5eTnC398blpY0SjQhpOyg6oKK4uFuID6MW9azAtx+5TUcop6kUob1629gypRzyMqSAADEYiGWLeuAsWNdoKFBtcaEkLKFEqGK4EM8cHV2/nqHzYC4gvaII2VaeHgcJk48C6mU64xav7459u7thXr1zHmOjBBCCqfwXGOkDLo0GUjj5mdCVQ/A4Vt+4yFqq2FDS0yf3goAMGFCc9y8OYKSIEJImUY1QuVdXCjwcBe3rKkLdNpB84mRUvPhQw60tTXlbnnNnu2Bjh0d4e5uy2NkhBBSPFQjVN5d+GjqjKaTaD4xUmpCQl6jcePNWLnymly5lpaQkiBCSLlRokTo33//xcCBA9GiRQu8evUKALB7925cuXJFqcGRL/gvAHj1/+dcvwrgMo3feIhakEikWLr0Cpo334bIyHeYMeMfhIa+4TssQggpEYUTocOHD8PLyws6Ojq4c+cOsrKyAADJyclYtGiR0gMkRUiOBoK+y193+xXQ0uEtHKIeYmKS0b79Lkydeh65uVIAQIMGFtDXF/EcGSGElIzCidCCBQuwadMmbN26FVpa+VM3tGzZEqGhoUoNjnzGhR+BrGRuuVo7oN5QfuMhFd6BAw/QoMEmXLr0HADXFG3atFa4dm0YataszHN0hBBSMgo3lo6IiEDr1q0LlBsZGSEpKUkZMZEvSYoCngZyy2IjoMtfgICaexHVSEnJwvjxp7Fz511ZmY2NIXbv7gEPDzv+AiOEECVQOBGytLTEkydPZLPQ57ly5QocHByUFRcpCmPA2eH56/W/B3SpezJRjYiIBHTuvBdRUYmyMh+futi06VsYG2vzGBkhhCiHwtUII0aMwI8//ogbN25AIBDg9evX2LNnDyZPnozRo0d/+QTk67z6F4i5wC1rVwZcpvIbD6nQqlY1hKYm92/CwECEXbu88ddfvSgJIoRUGAonQlOnToWvry/at2+PtLQ0tG7dGsOHD8fIkSMxbty4EgWxYcMG2NnZQVtbG66urrh582axjtu3bx8EAgG8vb1LdN1yhzHg8i/56y1mAzqV+IuHVHh6eiLs3dsTbdrY4e7dUfDza0iTKxNCKhQBY4yV5MDs7Gw8efIEaWlpcHJygr5+ySZS3L9/PwYNGoRNmzbB1dUVa9aswcGDBxEREQFz86Jv+URHR6NVq1ZwcHBApUqVcPTo0WJdLyUlBUZGRkhOToahYTmbhuLxfuBkP25ZzwoYHgVo0jdzohyMMezefQ8tW9rA0bFSgW2UABFC+KSqz+8St7AViURwcnKCi4tLiZMgAFi1ahVGjBiBoUOHwsnJCZs2bYKuri62b99e5DESiQQDBgzAvHnz1KddkiQHuDojf91zIyVBRGkSEzPQr99hDB58FAMGBCAnRyK3nZIgQkhFpXBj6bZt2372n+I///xT7HNlZ2cjJCQE06blDwSooaEBT09PBAcHF3ncr7/+CnNzcwwbNgz//vvvZ6+RlZUlG+sI4DLKcunKDCDpKbdctTVQvTu/8ZAK4+LFaPj5HcHLl9x748aNVzhxIhI9etThOTJCCFE9hROhRo0aya3n5OQgLCwM9+/fx+DBgxU6V0JCAiQSCSwsLOTKLSws8Pjx40KPuXLlCrZt24awsLBiXWPx4sWYN2+eQnGVOXGhwO3l+est5/MXC6kwsrMlmD37ApYtu4q8G+QmJtrYsqUrJUGEELWhcCK0evXqQsvnzp2LtLS0rw7oc1JTU+Hn54etW7fC1NS0WMdMmzYNEydOlK2npKTAxsZGVSGqxr8f9QyrO5SrESLkK0REJMDXN0Buaoy2be2wa1cPVK1aztrOEULIV1Da7PMDBw6Ei4sLVqxYUexjTE1NIRQKERcXJ1ceFxcHS0vLAvs/ffoU0dHR6Nq1q6xMKuWG+dfU1ERERAQcHR3ljhGLxRCLxYo8lLIlOgh4/je3LNAA2q3jNx5SrjHGsGVLCCZMCEJGRi4AQEtLAwsXtsOkSW5ys8gTQog6UFoiFBwcDG1txRrvikQiODs74/z587Iu8FKpFOfPn8fYsWML7F+7dm2Eh4fLlc2cOROpqalYu3Zt+avp+RLGgH+n56+3XQuISt4wnZA7d2IxatRJ2XqtWpWxd28vNGlixWNUhBDCH4UToZ49e8qtM8bw5s0b3L59G7NmzVI4gIkTJ2Lw4MFo2rQpXFxcsGbNGqSnp2PoUG7urEGDBsHa2hqLFy+GtrY26tWrJ3e8sbExABQorxDCfgfe/n/+NpMaQEMasJJ8nSZNrDBxYnOsWnUdo0c3xYoVHaGrq/XlAwkhpIJSOBEyMjKSW9fQ0ECtWrXw66+/omPHjgoH4OPjg/j4eMyePRuxsbFo1KgRzpw5I2tA/eLFC2hoqOE8WlnJwLWPEkuPVYCGkL94SLmUlZULkUgo19Nz0aL26NSpOjp0cPzMkYQQoh4UGlBRIpHg6tWrqF+/PkxMTFQZl8qUmwEVby3PH0W6Zl+g635+4yHlTnh4HHx9AzB6dFP88EMzvsMhhJCvUiYGVBQKhejYsSPNMq9q0txPptJQ/JYjUV9SKcPatdfRrNlW3L//FpMmncXDh/F8h0UIIWWSwrfG6tWrh6ioKNjb26siHgIA97bmL1dpCZhWwPZPRCXevEnF0KHHEBT0VFZWowbNR0cIIUVRuPHNggULMHnyZJw4cQJv3rxBSkqK3A/5StJc4Obi/PVG1ECaFM+xY4/RoMEmuSRowoTmuHlzBJyczHiMjBBCyq5i1wj9+uuvmDRpEjp37gwA6Natm1wDzLxJGSUSSVGnIMVxZz2QGsMtV64L1PblNx5S5qWnZ2PSpLPYvDlEVmZlpQ9/f2907EgNogkh5HOK3VhaKBTizZs3ePTo0Wf38/DwUEpgqlKmG0szBmy1A1JfcOt9/gGqteU1JFK2RUa+Q9eufyEy8p2szNu7NrZu7QpTU10eIyOEEOVS1ed3sWuE8vKlsp7olGtPjuUnQVXcKAkiX2RhoYfsbK4WVldXC2vXdsKwYY1ptnhCCCkmhdoI0T9XFft4YtUmP/EWBik/jIy08eefPeDqao07d0Zi+PAm9D4lhBAFKNRrrGbNml/8J/v+/fuvCkhtvQ4GXl/jlg1tgRo9P78/UUsHDz5A8+ZVYWOTP7Bpy5bVEBw8jBIgQggpAYUSoXnz5hUYWZooSfCv+ctNJ9Mo0kROSkoWxo8/jZ0776JNGzucO+cHoTC/QpeSIEIIKRmFEqF+/frB3NxcVbGorxf/ANFnuGU9S6D+CH7jIWVKcHAMBg48gqioRADAxYvROHEiEt271+Y5MkIIKf+K3UaIvnGq0MUJ+cvNpgCaYv5iIWVGbq4U8+ZdhLv7DlkSZGAgwq5d3ujWrRbP0RFCSMWgcK8xomQvLgDx97hlk5pA43H8xkPKhKioRAwcGIDg4JeyMjc3G/z5Zw/Y25fPef4IIaQsKnYiJJVKVRmHemIMuP5R2yDnidQ2SM0xxrB79z2MHXsKqanZAAChUIDZsz0wfbo7NDUVHgyeEELIZyg81xhRoqfHgZiL3LKhLVB3MJ/RkDLg9u3XGDz4qGzdwcEEe/b0RPPmVfkLihBCKjD6eskXJgWuzsxfb7kA0NTmLx5SJjRrZo2RI50BAEOGNEJY2EhKggghRIWoRogv9/2BhHBu2bQ+ULs/r+EQfuTkSKCpqSHXGWHlyo7o3LkGNYgmhJBSQDVCfGBMfoZ5jxXUNkgNRUQkoHnzbdi5865cuZ6eiJIgQggpJZQI8eHVv0DSE27ZyhWw68hvPKRUMcawefNtNG68GaGhbzBu3Gk8eUIjshNCCB/o1hgfQtflL9cbzl8cpNTFx6dj+PDjCAyMkJVZWxsgIyOHx6gIIUR9USJU2uJCgP8Oc8u6FoCTH7/xkFITFPQEQ4YcQ2xsmqxs1ChnrFzpBV1dLR4jI4QQ9UWJUGm7Njd/2XkCjSKtBjIzczFt2jmsWXNDVmZqqovt27uha1dqC0QIIXyiRKg0vboKRJ3glvWsgEZj+I2HqNyTJ+/Rs+d+hIe/lZV16lQdO3Z0h6WlPo+REUIIASgRKl3B8/KXm88CRPRBWNGZmGjj3bsMAIBYLMTy5R0wdqwLzd1HCCFlBPUaKy3x4cDzv7llXQugPjWSVgeVK+vC3787Gja0wO3b32PcOFdKggghpAyhGqHScntF/rLLFEBIjWMrouPHI9CsmbXcba8OHRwREmIPoZC+dxBCSFlD/5lLw/PzwMNd3LKmDlBnAL/xEKVLT8/GqFEn0K3bPnz33TEwxuS2UxJECCFlE/13Lg2ha/OXW84HdM35i4UoXUjIazRpsgWbN4cAAE6ffoITJyJ5jooQQkhxUCKkaqmvgGenuGWRAdB4HL/xEKWRSKRYuvQKmjffhsjIdwAAXV0tbN3aFd9+W5Pn6AghhBQHtRFStZBVAJNwy43GAEIRv/EQpYiJSYaf3xFcuvRcVubsbIW9e3uhZs3KPEZGCCFEEZQIqZIkB3iwM3+daoMqhP3772PUqJNISsoEAAgEwNSprTB3bhuIRDR5LiGElCeUCKnSs1NAJnfLBNW9Af0qvIZDvt716y/Rr99h2bqNjSF27+4BDw87/oIihBBSYtRGSJVuLctfdhrEXxxEaZo3rwo/vwYAAB+furh7dxQlQYQQUo5RjZCqvLwCvL7GLRtXB6p35zceUiJSKYOGhvwAiL/91hldutRA3751aXBEQggp56hGSFXu/p6/3Hg8IKCnuryJikpEq1bbceDAA7lyQ0MxfHzqURJECCEVANUIqUJOev7kqlr6NJ1GOcMYw+7d9zB27Cmkpmbj0aMTaNGiKmxsjPgOjRBCiJJRNYUqPN4HZKdyy9W7A1o6/MZDii0xMQP9+h3G4MFHkZqaDQCoVElHNnEqIYSQioVqhJSNMSDso9ti9UfwFwtRyMWL0fDzO4KXL1NkZUOGNMK6dZ1gYCDmMTJCCCGqQomQsj05BrwN5ZbNGgBVW/MbD/mi7GwJZs++gGXLriJvijBjY21s2fIt+vSpy29whBBCVIoSIWULXZ2/3HwWN9oeKbOiohLRp89BhIa+kZW1aWOHXbu8qU0QIYSoAWojpEzJ0cDLy9yyflWgRk9ewyFfpqOjiRcvkgEAWloaWLbME+fPD6IkiBBC1AQlQsr0aE/+cv3h1GW+HLCyMsC2bd1Qu7Yprl8fjp9/bllg3CBCCCEVF90aU6b/8qdeQK2+/MVBinTuXBQaN7ZE5cq6srJu3Wrhm2+qQ0uL5gkjhBB1Q1UWyhJ/D3h7h1uuXBeoXIffeIiczMxcTJhwBh067MbIkSfA8lpF/x8lQYQQop4oEVKW82Pyl+sN5S8OUkB4eBxcXLZizZobAIDDhx/hzJknPEdFCCGkLKBESBmyUoDYm9yypjZQdwiv4RCOVMqwdu11NGu2FeHhbwEAYrEQ69Z1QqdO1XmOjhBCSFlAbYSUIeo4IOFGIUaVloBOZX7jIXjzJhVDhx5DUNBTWVn9+ubYu7cX6tUz5zEyQgghZQklQspwcWL+sss0/uIgAIDAwAgMGxaIhIQPsrIJE5pj0aL20NamP3lCCCH56FPha0X/DXzgbrtAvwpg48FvPGru6tUX6N59n2zd0lIfO3d6o2NHRx6jIoQQUlZRG6Gv9eJc/nKNXoAG5ZZ8cnOzQY8etQEA3bvXQnj4aEqCCCGEFIk+tb8GY8Cz0/nrTX7kLxY1xRiD4KNpTAQCAbZu7Ypu3Wph8OCGctsIIYSQT1GN0Nd4HQwkhHPLVq6AMdU8lKaYmGS0a7cLJ05EypVXrqyLIUMaURJECCHki6hG6Gs8O5W/7DSYvzjU0IEDDzBy5AkkJWXiwYO3uHdvNCwt9fkOixBCSDlDNUJfI+pE/nL17vzFoUZSUrIwZMhR+PgcQlJSJgBAW1sTr1+n8hwZIYSQ8ohqhEoq9SUQf5dbNm/C9RgjKhUcHIMBAwLw7FmSrMzHpy42buwCExMd/gIjhBBSblEiVFK3V+Yv23XkLw41kJsrxYIFl7FgwWVIJNwcYQYGImzY0BkDBzagtkCEEEJKjBKhkoq7nb/s8C1/cVRw0dFJ8PU9jODgl7IyNzcb/PlnD9jbm/AYGSGEkIqA2giVxIe3wKsr+etV3PiLpYLT0BDg4cN4AIBQKMC8eW1w6dIQSoIIIYQoBSVCJfFfQP5y4/EA3ZpRmWrVjLBp07dwcDDBlSvfYfZsD2hq0p8tIYQQ5aBPlJL4OBGq3Z+/OCqgf/99jpSULLmyfv3q4cGDH9C8eVWeoiKEEFJRlYlEaMOGDbCzs4O2tjZcXV1x8+bNIvfdunUr3N3dYWJiAhMTE3h6en52f6V7Hwk8/5tb1rUArFxK79oVWHa2BFOnnoOHhz/GjTtdYDtNlkoIIUQVeE+E9u/fj4kTJ2LOnDkIDQ1Fw4YN4eXlhbdv3xa6/8WLF9G/f39cuHABwcHBsLGxQceOHfHq1avSCfje5vzlRj8AAt6fwnIvIiIBLVpsw9KlV8EYsGvXXZw9+5TvsAghhKgBAWOM8RmAq6srmjVrht9++w0AIJVKYWNjg3HjxmHq1KlfPF4ikcDExAS//fYbBg0a9MX9U1JSYGRkhOTkZBgaGioe8MH2wIt/uOUBtwDLpoqfgwDg5gnbsiUEEyYEISMjFwCgpaWBhQvbYdIkN2hoUNsrQgghnK/+/C4Cr/cbsrOzERISgmnTpsnKNDQ04OnpieDg4GKd48OHD8jJyUGlSpUK3Z6VlYWsrPw2JykpKSUPOO11fhKkZwlYOJf8XGouPj4dw4cfR2BghKysVq3K2Lu3F5o0seIxMkIIIeqE1/s6CQkJkEgksLCwkCu3sLBAbGxssc4xZcoUVKlSBZ6enoVuX7x4MYyMjGQ/NjY2JQ/440bSToOpt1gJBQU9QYMGm+SSoNGjmyI0dCQlQYQQQkpVuW7gsmTJEuzbtw9HjhyBtrZ2oftMmzYNycnJsp+YmJiSXzCvkTQAVG1d8vOosX//fY5OnfYgNjYNAGBqqovAwH74/fcu0NXV4jk6Qggh6obXW2OmpqYQCoWIi4uTK4+Li4OlpeVnj12xYgWWLFmCc+fOoUGDBkXuJxaLIRaLvz7Y7DTg+bn8ddvCa6DI57VqVQ2dOlXHmTNP0KlTdezY0Z1mjSeEEMIbXmuERCIRnJ2dcf78eVmZVCrF+fPn0aJFiyKPW7ZsGebPn48zZ86gadNSaqwcdRLI/cAt1+gJCEWlc90KRiAQYMeO7vj99844dcqXkiBCCCG84v3W2MSJE7F161bs3LkTjx49wujRo5Geno6hQ4cCAAYNGiTXmHrp0qWYNWsWtm/fDjs7O8TGxiI2NhZpaWmqDTT8j/xlGkSxWGJj09Cly16cPx8lV25pqY/Ro5vRZKmEEEJ4x/sodT4+PoiPj8fs2bMRGxuLRo0a4cyZM7IG1C9evICGRn6+tnHjRmRnZ6N3795y55kzZw7mzp2rukBfXuJ+a+nTJKvFEBgYgWHDApGQ8AF378bi7t1RqFxZl++wCCGEEDm8jyNU2ko0DkFSFLDNkVs2sgeGR31+fzWWnp6NSZPOYvPmEFmZlZU+jh/vD2fnKjxGRgghpDyrkOMIlRuv/s1frjuEtzDKupCQ1xgwIAAREe9kZd7etbF1a1eYmlJtECGEkLKHEqHiuLU8f5m6zRcgkUixYsU1zJx5Abm5UgCArq4W1q7thGHDGlNbIEIIIWUWJUJfwhjw7kH+uqUrf7GUQS9fpsDP7wguXoyWlTk7W2Hv3l6oWbMyf4ERQgghxcB7r7EyL29KDQDQrgRo6fAXSxmUkZGDW7e4CW8FAmDatFa4dm0YJUGEEELKBUqEvuTiT/nLzabwFkZZVaNGZaxb9w1sbAxx4cJgLFrUHiKRkO+wCCGEkGKhROhzGAMS7uev1xvKXyxlxM2br/DhQ45c2dChjfDw4Rh4eNjxExQhhBBSQpQIfU5ciPy6rhk/cZQBublSzJt3EW5u2zB58lm5bQKBAPr6NNI2IYSQ8ocSoc+5vz1/ueUC/uLgWVRUIlq33oG5cy9BImHYuPE2Llx4xndYhBBCyFejXmOfE3WC+62pAzQey28sPGCMYffuexg79hRSU7MBAEKhALNne8Dd3Zbn6AghhJCvR4lQUd5HAqkx3LJFU0BsxG88pSwxMQOjR5/E/v35Qwc4OJhgz56eaN68Ko+REUIIIcpDiVBR/juUv2zfib84eHDpUjT8/I4gJiZFVjZkSCOsW9cJBgZiHiMjhBBClIsSoaI8/DN/uXpP/uIoZZcuRaNt253Im4HOxEQbmzd/iz596vIbGCGEEKIC1Fi6MMnRwPtH3LJlM6BybV7DKU2tWlVD69Zc+5+2be1w795oSoIIIYRUWFQjVJiXl/OX7bz4i4MHQqEGdu/ugYMHH+Knn5pDQ4PmCSOEEFJxUY1QYZ4cyV+2rriTrMbHp6NXrwO4evWFXLmNjREmTmxBSRAhhJAKj2qEPiWVANH/HzBQ2wSo1pbfeFQkKOgJhgw5htjYNISGvsHdu6NgaEgNoQkhhKgXqhH6VPw9IPcDt1y1DaBRsXLFzMxc/PTTGXTqtAexsWkAgLS0bERGvuM5MkIIIaT0VaxPeWWIPp2/bFOxaoPCw+Pg6xuA+/ffyso6daqOHTu6w9JSn8fICCGEEH5QIvSp6KD8ZYcu/MWhRFIpw/r1NzBlyjlkZUkAAGKxEMuXd8DYsS4QCKgtECGEEPVEidDHMpPye4wZOQBG9ryGowxv3qRi6NBjCAp6KiurX98ce/f2Qr165jxGRgghhPCP2gh97MW5/GVbT6AC1JS8f5+BixejZesTJjTHzZsjKAkihBBCQImQvIT7+ctVPfiLQ4nq1jXH8uUdYGmpj6CggVi1ygva2lQRSAghhACUCMl7cz1/2bIZf3F8hbt3Y5GVlStXNnasCx4+/AEdOzryFBUhhBBSNlEilIdJgTc3uGXtyoBxdX7jUZBEIsXSpVfQtOlWzJjxj9w2gUAAExMdniIjhBBCyi5KhPKkvACykrhlC+dy1T4oJiYZ7dvvwtSp55GbK8XKlcG4cuXFlw8khBBC1Bw1FskTfzd/2bwxf3Eo6MCBBxg58gSSkjIBcPnb1Kmt4OJizXNkhBBCSNlHiVCej9sHWbnwF0cxpaRkYfz409i5Mz+Bs7ExxO7dPeDhYcdfYIQQQkg5QolQnoQH+ctmDfmLoxiCg2MwcOARREUlysp8fOpi48Yu1BaIEEIIUQAlQnmijnO/NXXK9ECKFy9Gw9NzFyQSBgAwMBBhw4bOGDiwAY0QTQghhCiIGksDQHZq/rJJLUBQdp+Wli1t4OxcBQDg5maDu3dHwc+vISVBhBBCSAlQjRAAxN7OX2YS/uIoBi0tIfbs6Yn9++9jypRW0NQsu0kbIYQQUtZRIgQAr67kLzf7mb84PpGYmIGxY09j4sTmslogAKhevRJmzGjNY2SEqCfGGHJzcyGRlO0vTISUV1paWhAKhaV6TUqEAPlEyNKVvzg+cvFiNPz8juDlyxSEhLxGaOhI6Opq8R0WIWorOzsbb968wYcPH/gOhZAKSyAQoGrVqtDX1y+1a1IixBgQd4tb1jUHTGrwGk52tgSzZ1/AsmVXwbj20Hj7Nh0PHrxFs2Y0NhAhfJBKpXj27BmEQiGqVKkCkUhE7fIIUTLGGOLj4/Hy5UvUqFGj1GqGKBFKegpk/r8bullDXkeUjohIgK9vAEJD38jK2ra1w65dPVC1qiFvcRGi7rKzsyGVSmFjYwNdXV2+wyGkwjIzM0N0dDRycnIoESo1b0Pzl6u05CUExhi2bAnBhAlByMjgJkzV0tLAwoXtMGmSGzQ06JsnIWWBhgZ1TiBElfioaaVEKPZW/rJFk1K/fHx8OoYPP47AwAhZWa1albF3by80aWJV6vEQQggh6oQSoZeX85ctmpb65WNiUnDq1H+y9dGjm2LFio7UMJoQQggpBepdz5ubmX9rzNgR0C/9GpgmTaywYEFbmJrqIjCwH37/vQslQYQQUgZERETA0tISqampX96ZFEvz5s1x+PBhvsOQo96JUOxNQMq1yYG1e6lc8vHjBOTkyI9BMnmyGx48+AFdu9YqlRgIIepjyJAhEAgEEAgE0NLSgr29PX755RdkZmYW2PfEiRPw8PCAgYEBdHV10axZM/j7+xd63sOHD6NNmzYwMjKCvr4+GjRogF9//RXv379X8SMqPdOmTcO4ceNgYGBQYFvt2rUhFosRGxtbYJudnR3WrFlToHzu3Llo1KiRXFlsbCzGjRsHBwcHiMVi2NjYoGvXrjh//ryyHkahDh48iNq1a0NbWxv169fHqVOnPrv/x39HH//UrVtXtk9qaip++ukn2NraQkdHB25ubrh165bceWbOnImpU6dCKpWq5HGVhHonQpEfZaXmjVV6KamUYe3a62jUaBMWLLgst00o1IC5uZ5Kr08IUV+dOnXCmzdvEBUVhdWrV2Pz5s2YM2eO3D7r169H9+7d0bJlS9y4cQP37t1Dv379MGrUKEyePFlu3xkzZsDHxwfNmjXD6dOncf/+faxcuRJ3797F7t27S+1xZWdnq+zcL168wIkTJzBkyJAC265cuYKMjAz07t0bO3fuLPE1oqOj4ezsjH/++QfLly9HeHg4zpw5g7Zt22LMmDFfEf3nXbt2Df3798ewYcNw584deHt7w9vbG/fv3y/ymLVr1+LNmzeyn5iYGFSqVAl9+vSR7TN8+HD8/fff2L17N8LDw9GxY0d4enri1atXsn2++eYbpKam4vTp0yp7fApjaiY5OZkBYMnJyYztdmZsBbifl1dUds3Xr1OYl9duBsxlwFymoTGP3bjxUmXXI4QoV0ZGBnv48CHLyMjgOxSFDR48mHXv3l2urGfPnqxx48ay9RcvXjAtLS02ceLEAsevW7eOAWDXr19njDF248YNBoCtWbOm0OslJiYWGUtMTAzr168fMzExYbq6uszZ2Vl23sLi/PHHH5mHh4ds3cPDg40ZM4b9+OOPrHLlyqxNmzasf//+rG/fvnLHZWdns8qVK7OdO3cyxhiTSCRs0aJFzM7Ojmlra7MGDRqwgwcPFhknY4wtX76cNW3atNBtQ4YMYVOnTmWnT59mNWvWLLDd1taWrV69ukD5nDlzWMOGDWXr33zzDbO2tmZpaWkF9v3c8/i1+vbty7p06SJX5urqykaOHFnscxw5coQJBAIWHR3NGGPsw4cPTCgUshMnTsjt16RJEzZjxgy5sqFDh7KBAwcWet7PvdfkPr+VSL0bS6flZ6mqaih97NhjDB9+HAkJ+aPRjh/vggYNLFRyPUJIKfqzKZBe8NaISulZAgNvf3m/Ity/fx/Xrl2Dra2trOzQoUPIyckpUPMDACNHjsT06dPx119/wdXVFXv27IG+vj5++OGHQs9vbGxcaHlaWho8PDxgbW2NwMBAWFpaIjQ0VOFbJDt37sTo0aNx9epVAMCTJ0/Qp08fpKWlyUYjDgoKwocPH9CjRw8AwOLFi/Hnn39i06ZNqFGjBi5fvoyBAwfCzMwMHh4ehV7n33//RdOmBT8XUlNTcfDgQdy4cQO1a9dGcnIy/v33X7i7K9a84v379zhz5gwWLlwIPb2CdwSKeh4BYM+ePRg5cuRnz3/69OkiYwoODsbEiRPlyry8vHD06NEvxp1n27Zt8PT0lP0d5U09o62tLbefjo4Orly5Ilfm4uKCJUuWFPtaqqa+iVB2qvw/ME2xUk+fnp6NSZPOYvPmEFmZpaU+du70RseOjkq9FiGEJ+mx8l+oyqgTJ05AX18fubm5yMrKgoaGBn777TfZ9sjISBgZGcHKqmCHEZFIBAcHB0RGRgIA/vvvPzg4OEBLS7FOHXv37kV8fDxu3bqFSpUqAQCqV6+u8GOpUaMGli1bJlt3dHSEnp4ejhw5Aj8/P9m1unXrBgMDA2RlZWHRokU4d+4cWrRoAQBwcHDAlStXsHnz5iIToefPnxeaCO3btw81atSQtY3p168ftm3bpnAi9OTJEzDGULt2bYWOA4Bu3brB1fXz00FZWxc9E0FsbCwsLOS/jFtYWBTa3qkwr1+/xunTp7F3715ZmYGBAVq0aIH58+ejTp06sLCwwF9//YXg4OACr3OVKlUQExMDqVRaJsbmUt9E6M2N/OXq3ko9dUjIa/j6BiAy8p2srHv3Wvjjj24wNaVRaQmpMPQsy8U127Zti40bNyI9PR2rV6+GpqYmevXqVaLLs7y5fxQUFhaGxo0by5KgknJ2dpZb19TURN++fbFnzx74+fkhPT0dx44dw759+wBwCceHDx/QoUMHueOys7PRuHHRbUMzMjIK1G4AwPbt2zFw4EDZ+sCBA+Hh4YH169cX2qi6KCV9HgEu6VDkWsq2c+dOGBsbw9vbW6589+7d+O6772BtbQ2hUIgmTZqgf//+CAkJkdtPR0cHUqkUWVlZ0NHRKcXIC6e+iVDkgfxlJfYY++efZ/Dy+hO5uVx1r66uFtas8cLw4U1obiJCKpqvuEVVmvT09GTfyrdv346GDRti27ZtGDZsGACgZs2aSE5OxuvXr1GlShW5Y7Ozs/H06VO0bdtWtu+VK1eQk5OjUK3Qlz7wNDQ0CiQHOTk5hT6WTw0YMAAeHh54+/Yt/v77b+jo6KBTp04AuFtyAHDy5MkCtSRicdF3AkxNTZGYmChX9vDhQ1y/fh03b97ElClTZOUSiQT79u3DiBEjAACGhoZITk4ucM6kpCQYGRkB4Gq2BAIBHj9+XGQMRfnaW2OWlpaIi4uTK4uLi4Ol5ZeTbMYYtm/fDj8/P4hEIrltjo6OuHTpEtLT05GSkgIrKyv4+PjAwcFBbr/3799DT0+vTCRBgDr3GstOy1+u1l5pp23Z0gZOTmYAAGdnK9y5MxIjRjhTEkQIKRM0NDQwffp0zJw5ExkZGQCAXr16QUtLCytXriyw/6ZNm5Ceno7+/fsDAHx9fZGWlobff/+90PMnJSUVWt6gQQOEhYUV2b3ezMwMb968kSsLCwsr1mNyc3ODjY0N9u/fjz179qBPnz6yJM3JyQlisRgvXrxA9erV5X5sbGyKPGfjxo3x8OFDubJt27ahdevWuHv3LsLCwmQ/EydOxLZt22T71apVq0AtCACEhoaiZs2aAIBKlSrBy8sLGzZsQHp6eoF9i3oeAe7W2MfXL+ynsNt6eVq0aFGge/7ff/8tu3X4OZcuXcKTJ09kSXRh9PT0YGVlhcTERAQFBaF79+5y2+/fv//Z2rhSp9Sm1+WArNX5Otv8HmOSXKVe4/79ODZjxnmWlaXc8xJC+FHReo3l5OQwa2trtnz5clnZ6tWrmYaGBps+fTp79OgRe/LkCVu5ciUTi8Vs0qRJcsf/8ssvTCgUsp9//pldu3aNRUdHs3PnzrHevXsX2ZssKyuL1axZk7m7u7MrV66wp0+fskOHDrFr164xxhg7c+YMEwgEbOfOnSwyMpLNnj2bGRoaFug19uOPPxZ6/hkzZjAnJyemqanJ/v333wLbKleuzPz9/dmTJ09YSEgIW7duHfP39y/yeQsMDGTm5uYsN5f7P56dnc3MzMzYxo0bC+z78OFDBoDdv3+fMcbY1atXmYaGBluwYAF7+PAhCw8PZ9OnT2eamposPDxcdtzTp0+ZpaUlc3JyYocOHWKRkZHs4cOHbO3atax27dpFxva1rl69yjQ1NdmKFSvYo0eP2Jw5c5iWlpZcbFOnTmV+fn4Fjh04cCBzdXUt9Lxnzpxhp0+fZlFRUezs2bOsYcOGzNXVlWVnZ8vt5+HhwX799ddCz8FHrzH1TYQWCbkkaEfdrzhXJhs+/Bi7fz9OiRESQsqaipYIMcbY4sWLmZmZmVzX7WPHjjF3d3emp6fHtLW1mbOzM9u+fXuh592/fz9r3bo1MzAwYHp6eqxBgwbs119//Wy37+joaNarVy9maGjIdHV1WdOmTdmNGzdk22fPns0sLCyYkZERmzBhAhs7dmyxE6G8ZMTW1pZJpVK5bVKplK1Zs4bVqlWLaWlpMTMzM+bl5cUuXbpUZKw5OTmsSpUq7MyZM4wxxg4dOsQ0NDRYbGxsofvXqVOHTZgwQbYeFBTEWrZsyUxMTGRd/Qu73uvXr9mYMWOYra0tE4lEzNramnXr1o1duHChyNiU4cCBA6xmzZpMJBKxunXrspMnT8ptHzx4sNxzzxhjSUlJTEdHh23ZsqXQc+7fv585ODgwkUjELC0t2ZgxY1hSUpLcPi9fvmRaWlosJiam0HPwkQgJGPuKFlvlUEpKCoyMjJC8ADDUBuDQFegRqPB5goNjMHDgEURFJaJBAwvcvDkcYrH6NrkipCLLzMzEs2fPYG9vX2gDWlIxbdiwAYGBgQgKCuI7lApjypQpSExMxJYtWwrd/rn3muzzOzkZhoaGSotJfdsI5TGt++V9PpKbK8W8eRfh7r4DUVFcQ7pnzxJx717cF44khBBSnowcORKtW7emucaUyNzcHPPnz+c7DDlUhVGp+GM4REUlYuDAAAQHv5SVubnZ4M8/e8De3kQV0RFCCOGJpqYmZsyYwXcYFcqkSZP4DqEASoSMa3xxF8YYdu++h7FjTyE1lZvbRigUYPZsD0yf7g5NTapYI4QQQsojSoS+cGssMTEDo0efxP79D2RlDg4m2LOnJ5o3r6rq6AghhBCiQuqdCBnYAGKjz+7y6FECDh7MH0tiyJBGWLeuEwwMlDslByGk7FOzviWElDo+3mPqfU/H/MsDOrm52WDGDHcYG2vjwIHe2LGjOyVBhKiZvMH5Pnz48IU9CSFfIzs7r/mJsNSuqd41QoKCeeCzZ4moVs0IQmH+tlmzWmPkSGdYWyuvux4hpPwQCoUwNjbG27dvAQC6uro0WjwhSiaVShEfHw9dXV1oapZeeqLeiVClOrJFxhi2bAnBhAlBmDPHA1OmtJJt09ISUhJEiJrLm4cpLxkihCifhoYGqlWrVqpfNNQ7EbJuCQCIj0/H8OHHERgYAQCYOfMCOnZ0ROPGVnxGRwgpQwQCAaysrGBubl7oZKCEkK8nEomgoVG6rXbKRCK0YcMGLF++HLGxsWjYsCHWr18PFxeXIvc/ePAgZs2ahejoaNSoUQNLly5F586dFb9wpdoICnqCIUOOITY2fxLW4cMbo1Yt05I8FEJIBScUCku1/QIhRLV4byy9f/9+TJw4EXPmzEFoaCgaNmwILy+vIqufr127hv79+2PYsGG4c+cOvL294e3tjfv37yt03cwcIX6aE4lOnfbIkiBTU10EBvbDxo3fQldX66sfGyGEEELKNt7nGnN1dUWzZs3w22+/AeAaS9nY2GDcuHGYOnVqgf19fHyQnp6OEydOyMqaN2+ORo0aYdOmTV+8Xt5cJXUsR+JRbP6tr06dqmPHju6wtNRXwqMihBBCiDJVyLnGsrOzERISAk9PT1mZhoYGPD09ERwcXOgxwcHBcvsDgJeXV5H7F+VRLDclhlgsxLp1nXDqlC8lQYQQQoia4bWNUEJCAiQSCSwsLOTKLSws8Pjx40KPiY2NLXT/2NjYQvfPyspCVlaWbD05OTlvC5yczLBtW3c4OZnRpHqEEEJIGZaSkgJA+YMulonG0qq0ePFizJs3r5Atq/HwIdCiRdmbAI4QQgghhXv37h2MjD4/K4QieE2ETE1NIRQKERcXJ1ceFxcnG7PjU5aWlgrtP23aNEycOFG2npSUBFtbW7x48UKpTyRRXEpKCmxsbBATE6PU+72kZOj1KDvotSg76LUoO5KTk1GtWjVUqlRJqeflNRESiURwdnbG+fPn4e3tDYBrLH3+/HmMHTu20GNatGiB8+fP46effpKV/f3332jRokWh+4vFYojFBafEMDIyoj/qMsLQ0JBeizKEXo+yg16LsoNei7JD2eMM8X5rbOLEiRg8eDCaNm0KFxcXrFmzBunp6Rg6dCgAYNCgQbC2tsbixYsBAD/++CM8PDywcuVKdOnSBfv27cPt27exZcsWPh8GIYQQQsoh3hMhHx8fxMfHY/bs2YiNjUWjRo1w5swZWYPoFy9eyGV/bm5u2Lt3L2bOnInp06ejRo0aOHr0KOrVq8fXQyCEEEJIOcV7IgQAY8eOLfJW2MWLFwuU9enTB3369CnRtcRiMebMmVPo7TJSuui1KFvo9Sg76LUoO+i1KDtU9VrwPqAiIYQQQghfeJ9igxBCCCGEL5QIEUIIIURtUSJECCGEELVFiRAhhBBC1FaFTIQ2bNgAOzs7aGtrw9XVFTdv3vzs/gcPHkTt2rWhra2N+vXr49SpU6UUacWnyGuxdetWuLu7w8TEBCYmJvD09Pzia0cUo+h7I8++ffsgEAhkA5+Sr6foa5GUlIQxY8bAysoKYrEYNWvWpP9VSqLoa7FmzRrUqlULOjo6sLGxwYQJE5CZmVlK0VZcly9fRteuXVGlShUIBAIcPXr0i8dcvHgRTZo0gVgsRvXq1eHv76/4hVkFs2/fPiYSidj27dvZgwcP2IgRI5ixsTGLi4srdP+rV68yoVDIli1bxh4+fMhmzpzJtLS0WHh4eClHXvEo+lr4+vqyDRs2sDt37rBHjx6xIUOGMCMjI/by5ctSjrxiUvT1yPPs2TNmbW3N3N3dWffu3Usn2ApO0dciKyuLNW3alHXu3JlduXKFPXv2jF28eJGFhYWVcuQVj6KvxZ49e5hYLGZ79uxhz549Y0FBQczKyopNmDChlCOveE6dOsVmzJjBAgICGAB25MiRz+4fFRXFdHV12cSJE9nDhw/Z+vXrmVAoZGfOnFHouhUuEXJxcWFjxoyRrUskElalShW2ePHiQvfv27cv69Kli1yZq6srGzlypErjVAeKvhafys3NZQYGBmznzp2qClGtlOT1yM3NZW5ubuyPP/5ggwcPpkRISRR9LTZu3MgcHBxYdnZ2aYWoNhR9LcaMGcPatWsnVzZx4kTWsmVLlcapboqTCP3yyy+sbt26cmU+Pj7My8tLoWtVqFtj2dnZCAkJgaenp6xMQ0MDnp6eCA4OLvSY4OBguf0BwMvLq8j9SfGU5LX41IcPH5CTk6P0CfbUUUlfj19//RXm5uYYNmxYaYSpFkryWgQGBqJFixYYM2YMLCwsUK9ePSxatAgSiaS0wq6QSvJauLm5ISQkRHb7LCoqCqdOnULnzp1LJWaST1mf32ViZGllSUhIgEQikU3PkcfCwgKPHz8u9JjY2NhC94+NjVVZnOqgJK/Fp6ZMmYIqVaoU+EMniivJ63HlyhVs27YNYWFhpRCh+ijJaxEVFYV//vkHAwYMwKlTp/DkyRP88MMPyMnJwZw5c0oj7AqpJK+Fr68vEhIS0KpVKzDGkJubi1GjRmH69OmlETL5SFGf3ykpKcjIyICOjk6xzlOhaoRIxbFkyRLs27cPR44cgba2Nt/hqJ3U1FT4+flh69atMDU15TsctSeVSmFubo4tW7bA2dkZPj4+mDFjBjZt2sR3aGrn4sWLWLRoEX7//XeEhoYiICAAJ0+exPz58/kOjZRQhaoRMjU1hVAoRFxcnFx5XFwcLC0tCz3G0tJSof1J8ZTktcizYsUKLFmyBOfOnUODBg1UGabaUPT1ePr0KaKjo9G1a1dZmVQqBQBoamoiIiICjo6Oqg26girJe8PKygpaWloQCoWysjp16iA2NhbZ2dkQiUQqjbmiKslrMWvWLPj5+WH48OEAgPr16yM9PR3ff/89ZsyYITdJOFGtoj6/DQ0Ni10bBFSwGiGRSARnZ2ecP39eViaVSnH+/Hm0aNGi0GNatGghtz8A/P3330XuT4qnJK8FACxbtgzz58/HmTNn0LRp09IIVS0o+nrUrl0b4eHhCAsLk/1069YNbdu2RVhYGGxsbEoz/AqlJO+Nli1b4smTJ7JkFAAiIyNhZWVFSdBXKMlr8eHDhwLJTl6CymjqzlKltM9vxdpxl3379u1jYrGY+fv7s4cPH7Lvv/+eGRsbs9jYWMYYY35+fmzq1Kmy/a9evco0NTXZihUr2KNHj9icOXOo+7ySKPpaLFmyhIlEInbo0CH25s0b2U9qaipfD6FCUfT1+BT1GlMeRV+LFy9eMAMDAzZ27FgWERHBTpw4wczNzdmCBQv4eggVhqKvxZw5c5iBgQH766+/WFRUFDt79ixzdHRkffv25eshVBipqanszp077M6dOwwAW7VqFbtz5w57/vw5Y4yxqVOnMj8/P9n+ed3nf/75Z/bo0SO2YcMG6j6fZ/369axatWpMJBIxFxcXdv36ddk2Dw8PNnjwYLn9Dxw4wGrWrMlEIhGrW7cuO3nyZClHXHEp8lrY2toyAAV+5syZU/qBV1CKvjc+RomQcin6Wly7do25uroysVjMHBwc2MKFC1lubm4pR10xKfJa5OTksLlz5zJHR0emra3NbGxs2A8//MASExNLP/AK5sKFC4V+BuQ9/4MHD2YeHh4FjmnUqBETiUTMwcGB7dixQ+HrChijujxCCCGEqKcK1UaIEEIIIUQRlAgRQgghRG1RIkQIIYQQtUWJECGEEELUFiVChBBCCFFblAgRQgghRG1RIkQIIYQQtUWJECFEjr+/P4yNjfkOo8QEAgGOHj362X2GDBkCb2/vUomHEFK2USJESAU0ZMgQCASCAj9PnjzhOzT4+/vL4tHQ0EDVqlUxdOhQvH37Vinnf/PmDb755hsAQHR0NAQCAcLCwuT2Wbt2Lfz9/ZVyvaLMnTtX9jiFQiFsbGzw/fff4/379wqdh5I2QlSrQs0+TwjJ16lTJ+zYsUOuzMzMjKdo5BkaGiIiIgJSqRR3797F0KFD8fr1awQFBX31uYuaNfxjRkZGX32d4qhbty7OnTsHiUSCR48e4bvvvkNycjL2799fKtcnhHwZ1QgRUkGJxWJYWlrK/QiFQqxatQr169eHnp4ebGxs8MMPPyAtLa3I89y9exdt27aFgYEBDA0N4ezsjNu3b8u2X7lyBe7u7tDR0YGNjQ3Gjx+P/7V3ryFRbl0cwP9OqTONY2IRzqRdzaEvatMFtKAyzYmsQUuzBjSyC9poFF0kKrVQT5SGRRctykuSZhQGopKkME6QdjEh85pmkRRlKJaTOrPOh/ChyUun97zQ+zbrB37Y+9l7z9rbDy72s3A+f/48bmw2NjZwcXGBQqHAmjVrEBcXh4qKCvT398NsNuP48eNwdXWFvb09vL29UVZWJswdGBiATqeDXC6HWCzGzJkzkZqaarH28Kux2bNnAwAWLFgAGxsbrFixAoDlLUtWVhYUCoXFN7sDgEajwbZt24R2cXExVCoVxGIx5syZg6SkJAwNDY27z4kTJ8LFxQXTp0+Hv78/QkNDce/ePeG5yWRCVFQUZs+eDYlEAqVSiYyMDOF5YmIicnJyUFxcLNwuVVVVAQBev36NsLAwODk5wdnZGRqNBh0dHePGwxgbiRMhxqyMSCTC2bNn8fz5c+Tk5OD+/fs4ePDgmOO1Wi1cXV1RW1uLx48fIz4+Hra2tgCAtrY2qNVqbNiwAfX19SgsLER1dTV0Ot0vxSSRSGA2mzE0NISMjAykpaXh9OnTqK+vR2BgINavX4+WlhYAwNmzZ3H37l3cvHkTTU1NyM/Px6xZs0Zdt6amBgBQUVGBrq4u3L59e8SY0NBQfPz4EZWVlUJfd3c3ysrKoNVqAQB6vR4RERHYs2cPGhoakJmZiezsbCQnJ//jPXZ0dKC8vBx2dnZCn9lshqurK4qKitDQ0IBjx47h8OHDuHnzJgBg//79CAsLg1qtRldXF7q6uuDr64vBwUEEBgZCJpNBr9fDYDDAwcEBarUaAwMD/zgmxhjwR377PGPWLjIykiZMmEBSqVT42bhx46hji4qKaMqUKUL72rVrNHnyZKEtk8koOzt71LlRUVG0c+dOiz69Xk8ikYj6+/tHnfPj+s3NzeTh4UGLFi0iIiKFQkHJyckWcxYvXkwxMTFERBQbG0t+fn5kNptHXR8A3blzh4iI2tvbCQA9ffrUYkxkZCRpNBqhrdFoaNu2bUI7MzOTFAoFmUwmIiJatWoVpaSkWKyRl5dHcrl81BiIiBISEkgkEpFUKiWxWCx8k3Z6evqYc4iIdu/eTRs2bBgz1uHPViqVFmfw9etXkkgkVF5ePu76jDFLXCPE2B9q5cqVuHjxotCWSqUAvt2OpKamorGxEb29vRgaGoLRaMSXL18wadKkEevs27cP27dvR15envB6Z+7cuQC+vTarr69Hfn6+MJ6IYDab0d7ejvnz548aW09PDxwcHGA2m2E0GrFs2TJcuXIFvb29ePv2LZYuXWoxfunSpXj27BmAb6+1AgICoFQqoVarERQUhNWrV/+rs9JqtdixYwcuXLgAe3t75OfnIzw8HCKRSNinwWCwuAEymUzjnhsAKJVK3L17F0ajEdevX0ddXR1iY2Mtxpw/fx5Xr15FZ2cn+vv7MTAwAG9v73HjffbsGVpbWyGTySz6jUYj2tra/oMTYMx6cSLE2B9KKpXC3d3doq+jowNBQUGIjo5GcnIynJ2dUV1djaioKAwMDIz6Bz0xMRFbtmxBSUkJSktLkZCQgIKCAgQHB6Ovrw+7du1CXFzciHkzZswYMzaZTIYnT55AJBJBLpdDIpEAAHp7e3+6L5VKhfb2dpSWlqKiogJhYWHw9/fHrVu3fjp3LOvWrQMRoaSkBIsXL4Zer8eZM2eE5319fUhKSkJISMiIuWKxeMx17ezshN/BX3/9hbVr1yIpKQknTpwAABQUFGD//v1IS0uDj48PZDIZTp06hYcPH44bb19fHxYuXGiRgA77XymIZ+z/BSdCjFmRx48fw2w2Iy0tTbjtGK5HGY+Hhwc8PDywd+9ebN68GdeuXUNwcDBUKhUaGhpGJFw/IxKJRp3j6OgIhUIBg8GA5cuXC/0GgwFLliyxGLdp0yZs2rQJGzduhFqtRnd3N5ydnS3WG67HMZlM48YjFosREhKC/Px8tLa2QqlUQqVSCc9VKhWampp+eZ8/OnLkCPz8/BAdHS3s09fXFzExMcKYH2907OzsRsSvUqlQWFiIadOmwdHR8V/FxJi142JpxqyIu7s7BgcHce7cObx8+RJ5eXm4dOnSmOP7+/uh0+lQVVWFV69ewWAwoLa2VnjldejQITx48AA6nQ51dXVoaWlBcXHxLxdLf+/AgQM4efIkCgsL0dTUhPj4eNTV1WHPnj0AgPT0dNy4cQONjY1obm5GUVERXFxcRv0nkNOmTYNEIkFZWRnevXuHnp6eMT9Xq9WipKQEV69eFYqkhx07dgy5ublISkrC8+fP8eLFCxQUFODIkSO/tDcfHx94enoiJSUFADBv3jw8evQI5eXlaG5uxtGjR1FbW2sxZ9asWaivr0dTUxM+fPiAwcFBaLVaTJ06FRqNBnq9Hu3t7aiqqkJcXBzevHnzSzExZvV+d5ESY+y/b7QC22Hp6ekkl8tJIpFQYGAg5ebmEgD69OkTEVkWM3/9+pXCw8PJzc2N7OzsSKFQkE6nsyiErqmpoYCAAHJwcCCpVEqenp4jip2/92Ox9I9MJhMlJibS9OnTydbWlry8vKi0tFR4npWVRd7e3iSVSsnR0ZFWrVpFT548EZ7ju2JpIqLLly+Tm5sbiUQiWr58+ZjnYzKZSC6XEwBqa2sbEVdZWRn5+vqSRCIhR0dHWrJkCWVlZY25j4SEBPLy8hrRf+PGDbK3t6fOzk4yGo20detWmjx5Mjk5OVF0dDTFx8dbzHv//r1wvgCosrKSiIi6urooIiKCpk6dSvb29jRnzhzasWMH9fT0jBkTY2wkGyKi35uKMcYYY4z9HvxqjDHGGGNWixMhxhhjjFktToQYY4wxZrU4EWKMMcaY1eJEiDHGGGNWixMhxhhjjFktToQYY4wxZrU4EWKMMcaY1eJEiDHGGGNWixMhxhhjjFktToQYY4wxZrU4EWKMMcaY1fobADwuu+X0QeoAAAAASUVORK5CYII="},"metadata":{}}]},{"cell_type":"code","source":"#Model weights\nmodel.state_dict()","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:55:33.687725Z","iopub.execute_input":"2024-03-16T12:55:33.688427Z","iopub.status.idle":"2024-03-16T12:55:34.460576Z","shell.execute_reply.started":"2024-03-16T12:55:33.688397Z","shell.execute_reply":"2024-03-16T12:55:34.459444Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('conv1.weight',\n              tensor([[[[ 0.7041,  0.2576,  0.0749,  ..., -0.6531, -0.1880, -0.5367],\n                        [ 0.6931, -0.0556,  0.3573,  ...,  0.0131, -0.1520,  0.0090],\n                        [ 0.1453,  0.2114, -0.0401,  ...,  0.1847,  0.1799, -0.4846],\n                        ...,\n                        [-0.1531, -0.4277,  0.2416,  ..., -0.7797, -0.0217,  0.7188],\n                        [ 1.0999,  1.0475,  0.9523,  ..., -0.3269, -0.5014, -0.5402],\n                        [ 0.0831,  0.0215,  0.7343,  ..., -0.4612, -0.3391, -1.1380]],\n              \n                       [[-0.0246,  0.0965, -0.0391,  ...,  0.1091, -0.2653, -0.2248],\n                        [-0.3413,  0.0225,  0.0254,  ..., -0.0019, -0.0630,  0.0857],\n                        [-0.1245, -0.3763,  0.1498,  ...,  0.0483,  0.0060,  0.1022],\n                        ...,\n                        [-0.0192,  0.2847, -0.1571,  ..., -0.1285, -0.3833,  0.2029],\n                        [ 0.0492,  0.0970, -0.2278,  ..., -0.0669,  0.2730, -0.1347],\n                        [ 0.1150,  0.2131,  0.0899,  ..., -0.1566,  0.0821, -0.0319]]],\n              \n              \n                      [[[ 0.2477,  0.2555,  0.1103,  ...,  0.2932, -0.6200, -0.6015],\n                        [ 0.5250,  0.1165,  0.5281,  ...,  0.3224,  0.2380, -0.1433],\n                        [ 0.8242,  0.5034,  0.6940,  ...,  0.3502,  0.6549,  0.1431],\n                        ...,\n                        [-0.3888, -0.5764, -0.4789,  ...,  0.1625,  0.0417,  0.0357],\n                        [ 0.0684,  0.5105,  0.2146,  ..., -0.2500, -0.7879, -0.4381],\n                        [-0.0406,  0.1443, -0.8026,  ..., -0.2544, -0.4846, -0.1733]],\n              \n                       [[-0.1195, -0.0259, -0.1781,  ...,  0.2085,  0.1089, -0.0266],\n                        [-0.0498,  0.0833, -0.0415,  ..., -0.1806, -0.2134, -0.0765],\n                        [-0.1799,  0.3133, -0.2484,  ...,  0.1060, -0.0692,  0.0360],\n                        ...,\n                        [-0.2719, -0.2492, -0.0876,  ...,  0.3067, -0.1867,  0.0363],\n                        [-0.1141, -0.0060,  0.3466,  ..., -0.2762, -0.3460,  0.1789],\n                        [-0.0914,  0.0131,  0.3428,  ..., -0.2280, -0.0476,  0.0768]]],\n              \n              \n                      [[[-0.3359, -0.3536, -0.3183,  ..., -0.5775, -0.2895, -0.3699],\n                        [-0.3626, -0.2390, -0.8268,  ..., -0.9720, -1.0284, -0.2911],\n                        [ 0.2822,  0.7759, -0.1458,  ..., -0.1148,  0.1760,  0.2022],\n                        ...,\n                        [-0.5162, -0.5713, -0.4279,  ..., -0.4239,  0.0142, -0.0576],\n                        [-0.6757, -1.0062, -0.5406,  ..., -0.4544, -0.3527,  0.0199],\n                        [-1.0394, -0.3057, -0.2049,  ..., -0.5202, -0.4036, -0.2635]],\n              \n                       [[ 0.0281, -0.0161,  0.0306,  ..., -0.0325,  0.0987,  0.2765],\n                        [-0.1987,  0.2026, -0.1069,  ..., -0.1132, -0.1071, -0.1787],\n                        [ 0.0475,  0.2424,  0.2104,  ...,  0.2469,  0.0421,  0.3803],\n                        ...,\n                        [-0.3027, -0.2330, -0.0534,  ...,  0.2137, -0.0887,  0.2308],\n                        [-0.0312, -0.0115, -0.0973,  ...,  0.2150, -0.1370,  0.1262],\n                        [ 0.0636,  0.1185,  0.1597,  ..., -0.0308, -0.2302,  0.0942]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0193,  0.1483, -0.1390,  ...,  0.7782,  0.2638,  0.1092],\n                        [-0.2392, -1.4796, -1.3079,  ...,  0.1145, -0.2378,  0.1233],\n                        [ 0.2194,  0.6400, -0.3976,  ..., -0.1256, -0.0432, -0.1009],\n                        ...,\n                        [-1.2425, -0.9002, -0.8090,  ..., -0.6169, -1.0243, -1.4196],\n                        [-1.5935, -1.4335, -0.1418,  ..., -0.5578, -1.1908, -1.2594],\n                        [ 0.3830,  0.2088,  0.3196,  ..., -1.2414, -1.0319, -0.1737]],\n              \n                       [[-0.1391,  0.0217, -0.0879,  ...,  0.1810,  0.0260, -0.0888],\n                        [ 0.3592, -0.0481,  0.0430,  ..., -0.0068, -0.1395,  0.0399],\n                        [-0.1448,  0.0804,  0.0572,  ..., -0.2953,  0.0288,  0.0254],\n                        ...,\n                        [-0.0406,  0.0517,  0.0949,  ...,  0.0027, -0.1343,  0.0265],\n                        [-0.0494, -0.1480, -0.0834,  ...,  0.1375, -0.0302, -0.1081],\n                        [-0.1596, -0.0455, -0.0269,  ..., -0.0118, -0.0430,  0.0609]]],\n              \n              \n                      [[[ 0.1335, -0.3259,  0.0993,  ..., -0.2395, -0.3900,  0.3825],\n                        [ 0.1096,  0.2934,  0.5191,  ..., -0.0087, -0.0729, -0.4147],\n                        [ 0.0385,  0.2138, -0.3275,  ..., -0.4883, -0.7820, -0.3546],\n                        ...,\n                        [-0.3983, -0.1500,  0.0952,  ..., -0.9207, -0.2150,  0.0323],\n                        [-0.4407, -0.3332,  0.1862,  ...,  0.1702,  0.0031,  0.3607],\n                        [-0.1865, -0.5602,  0.2315,  ..., -0.1417, -0.0747,  0.5400]],\n              \n                       [[ 0.2749,  0.0551,  0.0405,  ...,  0.3920,  0.2231,  0.0637],\n                        [ 0.3101,  0.1755,  0.1198,  ...,  0.2535,  0.1821, -0.3976],\n                        [-0.1344,  0.1237,  0.0184,  ..., -0.1159,  0.0563,  0.2260],\n                        ...,\n                        [ 0.2685, -0.0365,  0.0629,  ..., -0.3021, -0.1966, -0.0173],\n                        [-0.2785,  0.0166, -0.2423,  ..., -0.3890, -0.1326, -0.1717],\n                        [-0.0843,  0.1816,  0.4133,  ..., -0.2096, -0.0421, -0.2660]]],\n              \n              \n                      [[[ 0.8619,  0.3340,  0.0456,  ..., -0.7054, -0.5519, -0.6325],\n                        [ 0.4681,  0.1723,  0.5400,  ..., -0.2399,  0.3892, -0.1709],\n                        [ 0.0267,  0.5459, -0.7843,  ...,  0.2542, -0.2043, -0.3593],\n                        ...,\n                        [ 0.1955,  0.3378,  0.6755,  ...,  0.4423,  0.4737,  0.0727],\n                        [ 0.8412,  0.9863,  0.7487,  ..., -0.2181, -0.1971,  0.0506],\n                        [-0.8290,  0.2351,  0.0704,  ...,  0.1519, -0.2358, -0.4578]],\n              \n                       [[-0.1082,  0.0703, -0.0221,  ...,  0.0817,  0.3004, -0.1779],\n                        [-0.1610, -0.0627,  0.1710,  ..., -0.0841,  0.0199, -0.0711],\n                        [ 0.0247,  0.1478, -0.3679,  ...,  0.1631,  0.0721,  0.1425],\n                        ...,\n                        [ 0.0467, -0.1613, -0.1011,  ...,  0.1602,  0.1024,  0.0659],\n                        [ 0.0425, -0.1217,  0.1162,  ...,  0.0694,  0.3389, -0.0329],\n                        [ 0.1881,  0.0487, -0.0614,  ..., -0.1921,  0.0655,  0.3230]]]],\n                     device='cuda:0')),\n             ('bn1.weight',\n              tensor([0.5434, 0.3249, 0.7152, 0.8633, 1.5576, 0.6476, 0.5669, 1.0552, 0.7611,\n                      0.5996, 0.9085, 0.6967, 0.6950, 3.0277, 0.9609, 0.5138, 1.2310, 1.9553,\n                      0.7149, 0.7359, 1.8137, 0.7656, 0.5697, 0.5021, 0.5370, 0.6100, 0.5766,\n                      1.2667, 0.6727, 0.4936, 0.6445, 0.5512, 0.4091, 0.4590, 0.5406, 0.9735,\n                      0.5642, 0.6602, 0.8908, 0.6048, 0.5286, 0.5410, 0.7690, 1.0622, 1.2581,\n                      0.5926, 1.0551, 0.6810, 0.5390, 0.9370, 0.6255, 0.6902, 0.5803, 0.7050,\n                      0.9310, 0.8339, 0.5284, 0.5255, 1.4180, 0.7760, 1.0396, 1.0217, 0.4340,\n                      0.5498], device='cuda:0')),\n             ('bn1.bias',\n              tensor([-0.1749, -0.5668,  0.0500,  0.1970,  0.4586,  0.3370,  0.0028,  0.0013,\n                      -0.0134, -0.1679,  0.0331,  0.2994, -0.0953,  0.6532, -0.0491, -0.3835,\n                       0.9552,  0.7809,  0.1205, -0.1201,  0.6531,  0.4701,  0.4872,  0.1594,\n                       0.1114, -0.0888,  0.0248,  0.6029,  0.1140,  0.0073,  0.0231, -0.4785,\n                       0.0952,  0.0358,  0.0664, -0.0452,  0.2504,  0.0097,  0.2336, -0.4745,\n                       0.1323,  0.2220, -0.0113,  0.1988,  0.1770,  0.0803,  0.6929,  0.0105,\n                      -0.2076, -0.0551,  0.2070, -0.0572,  0.1421,  0.0673,  0.0023, -0.0075,\n                      -0.1776, -0.1042,  0.1235,  0.0757, -0.0387,  0.0161, -0.4338, -0.2139],\n                     device='cuda:0')),\n             ('bn1.running_mean',\n              tensor([ 0.0005,  0.0028, -0.0186, -0.0237, -0.0190, -0.0115, -0.0126, -0.0258,\n                      -0.0186, -0.0164, -0.0122, -0.0130, -0.0160, -0.0409, -0.0249,  0.0011,\n                      -0.0282, -0.0236, -0.0208,  0.0141, -0.0249,  0.0090,  0.0097, -0.0079,\n                      -0.0060, -0.0082, -0.0121, -0.0245, -0.0151, -0.0104, -0.0150, -0.0040,\n                      -0.0020, -0.0057, -0.0046, -0.0248,  0.0003, -0.0134, -0.0223, -0.0167,\n                      -0.0082,  0.0141, -0.0199, -0.0234, -0.0310, -0.0043, -0.0254, -0.0094,\n                       0.0141, -0.0219,  0.0112, -0.0211,  0.0075, -0.0092, -0.0263, -0.0188,\n                      -0.0084, -0.0083, -0.0284, -0.0140, -0.0258, -0.0217, -0.0032,  0.0054],\n                     device='cuda:0')),\n             ('bn1.running_var',\n              tensor([0.0107, 0.0102, 0.0161, 0.0289, 0.0128, 0.0121, 0.0143, 0.0250, 0.0187,\n                      0.0171, 0.0139, 0.0182, 0.0184, 0.0815, 0.0205, 0.0116, 0.0315, 0.0187,\n                      0.0171, 0.0116, 0.0237, 0.0079, 0.0095, 0.0150, 0.0103, 0.0130, 0.0114,\n                      0.0300, 0.0126, 0.0125, 0.0137, 0.0107, 0.0113, 0.0118, 0.0133, 0.0217,\n                      0.0094, 0.0125, 0.0276, 0.0149, 0.0112, 0.0110, 0.0221, 0.0284, 0.0325,\n                      0.0118, 0.0262, 0.0150, 0.0123, 0.0193, 0.0108, 0.0218, 0.0121, 0.0126,\n                      0.0228, 0.0188, 0.0145, 0.0113, 0.0308, 0.0165, 0.0265, 0.0214, 0.0112,\n                      0.0099], device='cuda:0')),\n             ('bn1.num_batches_tracked', tensor(46695, device='cuda:0')),\n             ('conv2.weight',\n              tensor([[[[-0.0868]],\n              \n                       [[-0.0110]],\n              \n                       [[ 0.0601]],\n              \n                       ...,\n              \n                       [[-0.0662]],\n              \n                       [[ 0.0133]],\n              \n                       [[-0.1149]]],\n              \n              \n                      [[[-0.2960]],\n              \n                       [[-0.0792]],\n              \n                       [[-0.0492]],\n              \n                       ...,\n              \n                       [[-0.1209]],\n              \n                       [[-0.2609]],\n              \n                       [[-0.2028]]],\n              \n              \n                      [[[-0.3227]],\n              \n                       [[-0.1615]],\n              \n                       [[-0.0018]],\n              \n                       ...,\n              \n                       [[-0.2536]],\n              \n                       [[-0.7090]],\n              \n                       [[-0.2945]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0767]],\n              \n                       [[ 0.3280]],\n              \n                       [[-0.0994]],\n              \n                       ...,\n              \n                       [[ 0.1626]],\n              \n                       [[ 0.4197]],\n              \n                       [[-0.3681]]],\n              \n              \n                      [[[-0.3147]],\n              \n                       [[-0.2507]],\n              \n                       [[ 0.0367]],\n              \n                       ...,\n              \n                       [[-0.1366]],\n              \n                       [[-0.2786]],\n              \n                       [[-0.1454]]],\n              \n              \n                      [[[ 0.1072]],\n              \n                       [[-0.1666]],\n              \n                       [[-0.1936]],\n              \n                       ...,\n              \n                       [[-0.2256]],\n              \n                       [[ 0.1847]],\n              \n                       [[-0.1033]]]], device='cuda:0')),\n             ('bn2.weight',\n              tensor([1.6398, 1.5701, 1.5969, 1.0424, 1.1034, 0.9892, 1.2643, 1.2454, 1.1046,\n                      1.3118, 0.9956, 1.2166, 1.0088, 1.0179, 0.8829, 1.1930, 1.3025, 1.1867,\n                      1.2702, 1.4428, 1.0559, 1.2121, 0.7412, 1.3744, 1.0935, 1.3065, 1.1751,\n                      0.7758, 1.9546, 1.3034, 0.8785, 0.9896, 0.9801, 1.2396, 0.7778, 1.0241,\n                      0.9632, 1.4947, 1.1832, 1.2417, 0.7626, 1.6241, 0.9547, 1.2701, 1.1374,\n                      0.9732, 1.0596, 0.8627, 1.3027, 1.1498, 1.1538, 1.1938, 1.4863, 1.0851,\n                      1.1730, 0.6605, 1.1891, 1.0350, 1.1408, 1.2946, 1.1901, 1.0207, 0.7862,\n                      0.9504], device='cuda:0')),\n             ('bn2.bias',\n              tensor([-0.3462, -0.3326,  0.1146,  0.2312, -0.1229,  0.3921, -0.2189,  0.2222,\n                      -0.1221, -0.2302, -0.1095, -0.0338, -0.1102, -0.3792,  0.0482, -0.0889,\n                      -0.2895, -0.1038, -0.2366,  0.0081, -0.2290,  0.0032, -0.3148, -0.0797,\n                      -0.6963,  0.0240,  0.0826, -0.3903, -0.2213, -0.2389, -0.3388, -0.2306,\n                      -0.3163, -0.0468, -0.1665, -0.5515, -0.2819, -0.2594, -0.2259, -0.1852,\n                      -0.1845, -0.3601, -0.0240,  0.2487,  0.1322, -0.1091, -0.3077, -0.3570,\n                       0.1229, -0.0897,  0.0128, -0.2614, -0.1752, -0.3273, -0.1048,  0.0070,\n                      -0.3139, -0.3078,  0.1115, -0.1146, -0.2837, -0.1174,  0.0306, -0.0329],\n                     device='cuda:0')),\n             ('bn2.running_mean',\n              tensor([-6.1397, -2.9340, -1.2493,  0.5937, -2.4570,  1.7433, -1.6945,  2.1969,\n                      -0.1548, -2.2106, -2.1927,  2.4000, -1.9386, -1.7713, -1.0325, -0.2152,\n                      -1.0532,  1.5495, -1.8943, -2.2757, -2.0445,  1.5969, -2.5164, -1.2502,\n                      -1.6818, -1.7631, -0.7370, -2.1370, -1.7920, -1.5573, -1.3378, -2.5147,\n                      -3.4889, -3.2035, -0.0408, -2.1950, -2.9229, -1.2472, -1.9094, -2.8504,\n                      -1.9121, -1.5292, -0.0329,  0.8893,  0.5477, -0.0889, -2.7342, -2.7902,\n                       2.2551, -2.9148,  2.3548, -1.1793, -1.8602, -1.6632, -0.3879,  2.4793,\n                      -3.0134, -3.4997,  2.8578, -1.9699,  0.0794,  1.5519,  0.3363,  1.2255],\n                     device='cuda:0')),\n             ('bn2.running_var',\n              tensor([1.2679, 0.6128, 0.8383, 0.4260, 0.4197, 0.8629, 0.5341, 0.5268, 0.7863,\n                      0.5036, 0.4894, 1.0218, 0.9847, 0.4538, 0.6754, 0.7507, 0.4743, 0.6928,\n                      0.7346, 1.1227, 1.0207, 0.5556, 0.7819, 0.6406, 0.4313, 0.3589, 0.6158,\n                      1.1107, 0.8989, 0.5373, 0.9470, 0.3987, 0.4102, 1.4613, 0.4021, 1.2176,\n                      0.9254, 0.7246, 0.8751, 0.9645, 0.6310, 0.7267, 0.7072, 0.8466, 0.4628,\n                      0.7260, 0.6543, 0.6105, 0.7328, 0.4843, 0.5513, 0.3812, 0.6543, 0.9307,\n                      0.6734, 0.4416, 0.5300, 0.5782, 0.7772, 0.5174, 0.6011, 0.6112, 0.6743,\n                      0.9025], device='cuda:0')),\n             ('bn2.num_batches_tracked', tensor(46695, device='cuda:0')),\n             ('layer1.0.conv1.weight',\n              tensor([[[[ 3.7156e-03,  3.0836e-02, -3.6169e-03],\n                        [ 2.1877e-01, -3.9361e-01,  6.3776e-03],\n                        [-1.0547e-01, -1.8404e-02, -4.5756e-02]],\n              \n                       [[-1.3551e-02,  9.2204e-02, -1.5628e-01],\n                        [ 1.2971e-01,  5.4445e-02, -1.8986e-01],\n                        [-3.3326e-01, -1.2669e-01, -3.7669e-01]],\n              \n                       [[ 3.5341e-01,  2.1718e-01,  3.6460e-01],\n                        [-9.7122e-02, -1.0289e-01, -1.0895e-01],\n                        [ 7.4010e-02,  2.7617e-02,  3.7821e-02]],\n              \n                       ...,\n              \n                       [[-1.4005e-01, -7.2084e-02,  1.5508e-02],\n                        [ 1.5324e-02,  2.5682e-01,  3.3279e-01],\n                        [ 1.6940e-01,  2.5956e-01,  2.7992e-01]],\n              \n                       [[ 2.4184e-01,  3.8319e-02,  1.0921e-01],\n                        [ 1.2523e-01,  2.6440e-01,  3.7144e-01],\n                        [ 4.3459e-01,  5.3288e-01,  6.1048e-01]],\n              \n                       [[ 5.8786e-03, -1.0278e-01, -1.6447e-01],\n                        [ 6.9389e-02,  3.2459e-02, -1.0748e-01],\n                        [ 2.6255e-01, -8.3145e-04, -1.3786e-01]]],\n              \n              \n                      [[[ 1.4408e-01,  2.0090e-01,  1.6409e-01],\n                        [-2.4665e-02, -4.0998e-01, -2.2656e-01],\n                        [ 1.6321e-01,  3.7363e-01,  2.2112e-01]],\n              \n                       [[-4.3827e-01, -1.0636e-01, -1.4206e-01],\n                        [ 5.7500e-02,  7.9311e-02, -1.7305e-01],\n                        [-1.6129e-01,  9.9666e-02, -1.3794e-01]],\n              \n                       [[ 1.2874e-01, -1.5983e-01,  8.3077e-02],\n                        [-1.0088e-01, -1.3507e-01,  3.5620e-02],\n                        [-2.6644e-02, -1.7007e-01,  3.1551e-01]],\n              \n                       ...,\n              \n                       [[-3.3853e-02,  7.4916e-02,  1.0050e-01],\n                        [ 1.3049e-01,  2.4065e-01,  3.5397e-01],\n                        [-7.2790e-02,  1.7171e-01,  3.0656e-01]],\n              \n                       [[ 4.1500e-02,  6.1138e-02,  8.9968e-03],\n                        [-7.2356e-02,  2.9200e-01,  3.1380e-01],\n                        [ 1.3007e-01,  5.2878e-01,  6.7873e-01]],\n              \n                       [[ 2.9758e-02, -1.0072e-02, -8.2031e-02],\n                        [ 9.1092e-03,  6.8670e-02,  1.4831e-03],\n                        [ 1.7025e-01,  4.9126e-02, -6.0943e-02]]],\n              \n              \n                      [[[ 2.0675e-01,  9.2677e-03,  8.3456e-02],\n                        [ 1.4919e-01, -1.3936e-01,  3.4071e-01],\n                        [ 6.2614e-02,  4.2343e-01, -1.0019e-01]],\n              \n                       [[-1.6480e-02,  1.1920e-01, -4.7658e-02],\n                        [ 6.4831e-02,  2.7663e-01,  6.6067e-02],\n                        [-4.2815e-01,  6.2167e-02, -3.9179e-01]],\n              \n                       [[ 1.2234e-01,  3.7662e-01,  3.0227e-01],\n                        [ 3.7766e-02,  3.8898e-01,  3.2164e-01],\n                        [-2.7988e-01,  1.0911e-02,  3.4941e-01]],\n              \n                       ...,\n              \n                       [[-1.4264e-01, -1.5822e-01, -1.7349e-01],\n                        [-1.2129e-01, -6.6211e-02,  9.2543e-03],\n                        [-4.5527e-02,  9.4616e-02,  1.1824e-01]],\n              \n                       [[-2.1379e-01, -2.3185e-01, -4.0663e-01],\n                        [ 1.7028e-01,  3.5553e-01,  2.3447e-01],\n                        [ 4.0996e-01,  2.0951e-01,  3.2035e-01]],\n              \n                       [[-2.5885e-01, -3.3685e-02, -3.0176e-02],\n                        [-6.1902e-02, -1.1083e-01, -2.0968e-01],\n                        [ 1.9472e-01, -7.3505e-02, -1.3444e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-4.6939e-01, -1.1744e+00,  1.7273e-01],\n                        [-3.6212e-01, -6.7309e-01, -1.7682e-01],\n                        [ 3.5090e-01, -1.0585e-01,  6.9595e-02]],\n              \n                       [[ 1.0320e-02, -5.9247e-03,  3.4131e-01],\n                        [-3.3103e-01, -6.0859e-01, -4.1260e-01],\n                        [ 4.1737e-01,  7.4136e-02, -5.7156e-02]],\n              \n                       [[ 2.7801e-01,  2.6613e-01, -1.8052e-01],\n                        [-1.3277e-01, -1.5081e-01,  5.0336e-02],\n                        [ 2.2628e-01,  3.1273e-01, -2.2476e-02]],\n              \n                       ...,\n              \n                       [[ 1.7867e-02, -1.1360e-01, -3.5113e-01],\n                        [-2.2036e-01,  1.4643e-02, -2.6998e-01],\n                        [ 5.7481e-02, -3.2058e-02, -3.3425e-03]],\n              \n                       [[-6.2429e-02, -9.7575e-02,  6.2601e-02],\n                        [-3.2319e-02,  2.3880e-01,  3.6534e-02],\n                        [ 1.0886e-01,  1.9120e-01,  1.0429e-01]],\n              \n                       [[ 2.6161e-02,  1.4886e-01, -2.4444e-02],\n                        [-4.0947e-01,  1.4130e-02, -1.9139e-01],\n                        [ 8.3501e-02,  5.1735e-03,  2.2989e-02]]],\n              \n              \n                      [[[ 9.1527e-02,  1.1599e-01, -3.5489e-02],\n                        [ 5.2439e-02, -4.0900e-01, -1.3979e-01],\n                        [-2.3069e-01, -8.2178e-02,  2.5510e-02]],\n              \n                       [[-3.6236e-01, -1.5834e-01, -3.3293e-01],\n                        [ 1.4998e-01,  7.0897e-02, -3.7094e-01],\n                        [-5.0923e-01, -4.1742e-01, -4.2945e-01]],\n              \n                       [[-2.5952e-01,  8.3691e-02,  3.1623e-01],\n                        [-2.3946e-01,  3.3453e-01,  1.1314e-01],\n                        [-1.9969e-01, -1.9009e-01,  5.5500e-03]],\n              \n                       ...,\n              \n                       [[-1.5499e-01,  1.8175e-01,  2.7291e-01],\n                        [ 1.7740e-02,  3.1195e-01, -4.8649e-03],\n                        [-4.8220e-02, -8.5301e-02,  1.0706e-02]],\n              \n                       [[ 1.9547e-01,  4.8053e-02, -6.7341e-02],\n                        [ 9.4464e-02,  2.8675e-01,  2.6343e-01],\n                        [ 3.7481e-01,  4.0848e-01,  8.8863e-02]],\n              \n                       [[-1.3219e-01,  7.9530e-02,  3.5578e-01],\n                        [-5.9929e-02, -2.7227e-02, -7.2503e-02],\n                        [ 3.1532e-01, -2.1134e-01, -2.2060e-01]]],\n              \n              \n                      [[[ 6.1582e-02,  8.4322e-02,  2.0279e-01],\n                        [ 1.4758e-01,  2.8100e-01,  2.3046e-01],\n                        [-2.4470e-01,  1.1957e-02, -2.8811e-01]],\n              \n                       [[-3.5749e-01, -1.8324e-01, -2.8790e-01],\n                        [ 6.0658e-02,  1.5018e-01, -9.6311e-02],\n                        [-7.1679e-01, -5.1651e-01, -4.3588e-01]],\n              \n                       [[ 2.3573e-01,  3.1665e-01,  2.5306e-01],\n                        [-1.2280e-01,  9.2996e-02,  2.1154e-01],\n                        [-3.0518e-01, -1.6771e-01,  2.5712e-01]],\n              \n                       ...,\n              \n                       [[ 9.2884e-02,  2.2232e-01,  2.2667e-01],\n                        [-2.8451e-01, -8.2994e-02,  9.2457e-02],\n                        [ 7.4470e-03, -2.4014e-02,  6.5899e-02]],\n              \n                       [[-2.9387e-01,  8.5371e-03,  9.6828e-02],\n                        [-2.3227e-04,  4.6190e-01,  3.8736e-01],\n                        [ 8.3461e-02, -2.1692e-04,  1.1537e-01]],\n              \n                       [[-3.9465e-02,  1.1216e-01,  1.9266e-01],\n                        [-4.2610e-02, -3.1237e-02, -4.6375e-02],\n                        [ 2.8903e-01, -1.5242e-01, -1.2669e-01]]]], device='cuda:0')),\n             ('layer1.0.bn1.weight',\n              tensor([0.7069, 0.6200, 0.9119, 1.2070, 0.8823, 1.1857, 0.9817, 0.7719, 0.9570,\n                      1.5951, 1.0338, 0.9426, 0.8200, 0.9179, 1.4578, 0.9360, 0.5983, 1.0540,\n                      0.7527, 1.0300, 0.8873, 1.3422, 0.6930, 1.0386, 0.9964, 0.6108, 1.1867,\n                      0.6663, 0.9195, 1.1541, 1.1058, 1.0326, 0.7430, 0.8842, 0.8758, 0.7450,\n                      0.9029, 0.7381, 0.8545, 1.2036, 1.3279, 1.2110, 0.8198, 0.8697, 0.8673,\n                      0.7120, 0.7696, 0.8400, 1.8226, 0.7639, 0.7607, 0.6836, 1.0336, 1.2527,\n                      0.7490, 1.2530, 0.8821, 0.9840, 0.7763, 1.1104, 0.8474, 1.1338, 1.0017,\n                      0.6395], device='cuda:0')),\n             ('layer1.0.bn1.bias',\n              tensor([-0.2042, -0.1189, -0.2382, -0.1185, -0.2459, -0.3039, -0.2298, -0.3230,\n                      -0.2453, -0.2964, -0.4041, -0.3328, -0.1941, -0.1032, -0.2434, -0.0689,\n                      -0.1014, -0.0215, -0.1803, -0.3663, -0.3898, -0.1473, -0.3068,  0.1798,\n                      -0.4463, -0.1506, -0.3801, -0.2675, -0.2151, -0.2894, -0.3543, -0.1530,\n                      -0.1533, -0.1723, -0.2470, -0.2602, -0.1364, -0.2289, -0.3816, -0.2572,\n                      -0.1578, -0.2214, -0.1340, -0.4956, -0.3008, -0.3501, -0.3968, -0.1921,\n                      -0.0716, -0.3303, -0.0737, -0.2285, -0.2667, -0.1471, -0.4790, -0.3469,\n                      -0.2578, -0.2864, -0.4162, -0.0972, -0.1209, -0.3317, -0.2394, -0.3100],\n                     device='cuda:0')),\n             ('layer1.0.bn1.running_mean',\n              tensor([ 31.6959,  27.5910,  27.9019, -12.3022, -14.7072,  -9.8382, -11.2902,\n                      -13.6658, -20.2315,  -7.7962,   5.9586,  19.8962, -23.4890,  -7.1714,\n                      -10.6110, -12.1398,  -2.4199,  -8.5783,  32.6404,   3.2162, -14.2142,\n                       -4.4225,  -5.1342,  -9.3082,   6.2464,  17.6321, -18.7583,  20.3991,\n                      -18.0612,  -7.9232, -17.7573, -11.4232, -15.7229, -11.7525, -15.5368,\n                       -9.4461, -15.5832, -13.7779,  30.7609, -13.0866,  -9.7321, -13.4851,\n                       -6.5799,  -2.5975,  36.8376,  22.4248,  -4.6428, -17.3781,  -4.5692,\n                      -25.5687,  -7.5286, -12.0422,  -5.1637, -23.0023,   3.0730, -12.5610,\n                      -17.6317, -10.8029,  20.0719,  -7.1512, -18.3795, -18.4993,  12.0236,\n                       21.3402], device='cuda:0')),\n             ('layer1.0.bn1.running_var',\n              tensor([1229.8728, 1031.7291, 1584.0995,  378.0204,  843.8130,  578.5148,\n                       428.9109,  649.8223, 1367.4695,  182.1621,  302.7630, 1053.8804,\n                       900.4869,  266.4589,  479.3494,  461.9674,  549.3087,  335.9142,\n                      1614.5912,  348.2226,  215.5383,  290.3977,  487.0108,  358.3612,\n                       152.2713,  519.5498,  864.2732,  687.6476,  984.8420,  382.7066,\n                       843.1645,  176.9067,  804.0790,  341.3807, 1144.7832,  513.2647,\n                      1074.0093,  440.4856, 1894.3534,  302.8437,  460.0207,  437.6725,\n                       793.6705,  118.2051, 2059.1472, 1057.9941,  445.6733, 1341.3854,\n                       425.4422, 1043.8174,  677.2587,  271.1425,   97.2133, 1156.8789,\n                       449.0593,  332.2270, 1135.0555,  249.9370,  933.7308,  595.1058,\n                       929.1558,  810.7458,  375.9513, 1020.4598], device='cuda:0')),\n             ('layer1.0.bn1.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer1.0.conv2.weight',\n              tensor([[[[ 6.4272e-03, -1.0210e-01, -1.9746e-01],\n                        [-2.5375e-01, -9.0234e-01, -2.5539e-01],\n                        [-2.0575e-01, -1.5558e-01,  2.5898e-02]],\n              \n                       [[-1.7423e-01, -1.7762e-01, -6.0758e-02],\n                        [-2.3789e-01, -4.0079e-01, -1.0246e-01],\n                        [-1.2433e-01, -1.4471e-01, -9.6175e-02]],\n              \n                       [[-2.6180e-02, -1.1115e-01, -1.2505e-01],\n                        [-3.0765e-01, -1.1253e+00, -3.1996e-01],\n                        [-2.6812e-01, -2.4295e-01,  6.7575e-02]],\n              \n                       ...,\n              \n                       [[ 1.6556e-01,  1.6310e-01,  5.8596e-02],\n                        [-6.6132e-02, -3.6368e-01, -7.7039e-02],\n                        [ 5.1526e-02, -1.0378e-01,  4.3874e-01]],\n              \n                       [[ 4.4129e-02,  1.8293e-02, -2.4056e-01],\n                        [ 3.7040e-02, -4.9768e-01, -2.3279e-01],\n                        [-1.5843e-01, -1.6362e-01, -1.2228e-02]],\n              \n                       [[ 5.3919e-02,  7.7623e-03,  1.8574e-02],\n                        [ 4.7322e-02, -1.9252e-01, -1.5742e-01],\n                        [-3.1626e-01, -1.6393e-01,  1.7405e-01]]],\n              \n              \n                      [[[ 1.0288e-02,  3.5649e-02, -5.7014e-02],\n                        [-1.5021e-01, -2.5089e-01, -1.3400e-01],\n                        [-3.5441e-01, -4.5780e-01, -2.6682e-01]],\n              \n                       [[-8.2165e-02,  1.9674e-02, -1.5155e-01],\n                        [-3.8824e-01, -4.6104e-01, -1.1546e-01],\n                        [-3.2539e-02, -4.8544e-02,  3.6124e-02]],\n              \n                       [[-5.3733e-02, -1.5652e-02,  7.4596e-02],\n                        [-2.0453e-01, -4.9707e-01, -2.3415e-01],\n                        [-3.3005e-01, -4.1016e-01, -1.4992e-01]],\n              \n                       ...,\n              \n                       [[ 2.3443e-02,  4.2977e-02,  1.2156e-01],\n                        [ 6.8152e-02, -4.0089e-01, -2.5154e-01],\n                        [ 2.3300e-02,  9.3304e-02,  2.0849e-01]],\n              \n                       [[-7.7029e-02,  1.1717e-02,  7.2302e-02],\n                        [-2.7385e-01, -6.8131e-02,  4.9951e-03],\n                        [-3.7070e-01, -2.5855e-01, -1.3587e-01]],\n              \n                       [[-1.6084e-01, -1.6555e-02,  1.6015e-01],\n                        [-1.5157e-01, -2.0190e-01, -9.2670e-02],\n                        [-4.6093e-01, -4.1333e-01, -1.3585e-01]]],\n              \n              \n                      [[[ 1.0076e-01,  3.3631e-01,  2.8478e-01],\n                        [ 3.0370e-01,  4.9583e-02, -2.4214e-02],\n                        [-1.0710e-01, -1.1453e-01,  9.6693e-03]],\n              \n                       [[ 1.4294e-01,  1.6724e-01,  1.1251e-01],\n                        [ 5.5626e-02,  5.1092e-02, -7.4928e-02],\n                        [-2.9481e-01, -1.2954e-01, -1.2039e-02]],\n              \n                       [[ 1.1911e-01, -6.1076e-02,  7.7507e-02],\n                        [ 1.5773e-01,  5.6594e-02, -1.3436e-02],\n                        [ 7.1702e-02,  1.1681e-04,  2.3401e-02]],\n              \n                       ...,\n              \n                       [[-2.0304e-02, -6.4957e-02, -2.5329e-02],\n                        [ 9.3059e-02, -5.2115e-01, -4.2184e-01],\n                        [ 5.8297e-02,  5.0124e-02,  4.4884e-01]],\n              \n                       [[ 1.5020e-02,  1.1153e-01,  2.6720e-01],\n                        [ 2.3731e-01,  1.0327e-01,  1.9868e-01],\n                        [ 1.8229e-02, -1.8698e-01, -1.2200e-01]],\n              \n                       [[ 4.1075e-02,  1.1386e-01,  1.2549e-01],\n                        [ 1.2640e-01,  1.1346e-02, -1.8159e-02],\n                        [ 2.2080e-01,  1.6208e-01,  4.0019e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-5.6095e-01, -2.1616e-01, -1.8826e-01],\n                        [-3.4910e-01, -1.3089e-01, -2.3077e-01],\n                        [-6.1145e-02, -9.1523e-02, -1.4523e-01]],\n              \n                       [[-5.4625e-01, -2.2739e-01, -1.3833e-01],\n                        [-1.3051e-01, -9.7833e-02, -2.2829e-01],\n                        [-5.7803e-02, -1.5682e-01, -1.4187e-01]],\n              \n                       [[-3.0476e-01, -1.9839e-01, -2.5119e-01],\n                        [-3.0127e-02,  2.0971e-04, -1.4990e-01],\n                        [ 7.2627e-02,  7.5987e-03, -8.4735e-02]],\n              \n                       ...,\n              \n                       [[-7.1351e-02, -3.4166e-02,  7.8030e-02],\n                        [-1.3262e-02, -2.0876e-01, -3.4721e-01],\n                        [-1.4011e-01, -1.0105e-01,  1.2102e-02]],\n              \n                       [[-1.7649e-01, -2.6753e-01, -2.5756e-01],\n                        [-1.5196e-01, -3.0303e-01, -1.9158e-01],\n                        [ 3.3547e-02, -9.3687e-02, -6.5518e-02]],\n              \n                       [[-3.8631e-01, -1.8788e-01, -1.9238e-01],\n                        [-2.4696e-01, -1.5529e-01, -2.5801e-01],\n                        [-7.7340e-02, -3.2482e-02, -1.4400e-01]]],\n              \n              \n                      [[[-1.8683e-02, -1.8283e-01, -7.0045e-02],\n                        [-3.0979e-01, -3.4678e-01, -2.4151e-01],\n                        [-2.2333e-02, -1.7482e-01, -8.9305e-02]],\n              \n                       [[-2.2419e-01, -2.4220e-01, -8.4318e-02],\n                        [-4.0643e-01, -6.5063e-01, -2.2372e-01],\n                        [-5.2407e-02,  8.0504e-02, -2.4406e-02]],\n              \n                       [[-3.5375e-02, -1.6599e-01, -9.0622e-02],\n                        [-1.2824e-01, -1.5105e-01, -1.0292e-01],\n                        [-1.7061e-01, -3.3461e-01,  6.1866e-02]],\n              \n                       ...,\n              \n                       [[ 1.4552e-01,  2.2437e-01,  1.7469e-01],\n                        [ 9.8187e-02, -2.2255e-01, -5.3048e-02],\n                        [ 9.5311e-02, -1.1289e-02,  1.5553e-01]],\n              \n                       [[ 1.8015e-01, -2.3077e-01, -8.2353e-02],\n                        [-2.9553e-01, -5.0959e-01, -1.4594e-01],\n                        [-1.4435e-01, -5.9340e-02,  1.7243e-01]],\n              \n                       [[ 1.2008e-01, -6.0868e-02, -9.3577e-02],\n                        [-8.6672e-02,  3.1988e-02,  8.7910e-02],\n                        [-4.0707e-01, -2.5961e-01, -1.1120e-01]]],\n              \n              \n                      [[[-1.7046e-01, -3.5927e-02, -1.7267e-01],\n                        [-2.5048e-01, -3.7712e-01, -2.7332e-01],\n                        [-3.5405e-01, -3.5150e-01, -1.1014e-01]],\n              \n                       [[-2.0759e-01, -8.2426e-02, -1.0594e-01],\n                        [-1.5689e-01, -1.0390e-01, -7.8200e-02],\n                        [-2.4130e-01, -2.6079e-01, -8.6608e-02]],\n              \n                       [[-2.4181e-01, -2.9510e-01, -2.1262e-01],\n                        [-3.0541e-01, -3.6343e-01, -2.2357e-01],\n                        [-1.6596e-01, -1.3623e-01, -1.8005e-01]],\n              \n                       ...,\n              \n                       [[-4.1659e-01, -6.8048e-04,  1.2079e-01],\n                        [ 3.3967e-02, -2.7090e-01, -2.3174e-01],\n                        [-5.4312e-02, -1.0588e-01,  8.3201e-02]],\n              \n                       [[-2.8211e-01, -1.5543e-01, -9.9184e-02],\n                        [ 8.7966e-02, -1.2272e-01, -9.1680e-02],\n                        [-2.7547e-01, -4.1356e-01, -3.9059e-01]],\n              \n                       [[-2.6465e-01, -8.5752e-02, -2.7102e-01],\n                        [-1.1848e-01, -1.6518e-01, -1.4201e-01],\n                        [-3.6186e-01, -2.7441e-01, -9.5854e-02]]]], device='cuda:0')),\n             ('layer1.0.bn2.weight',\n              tensor([1.1505, 0.7488, 0.7815, 0.7579, 0.9937, 0.5553, 0.9279, 0.4952, 0.9314,\n                      0.8191, 0.8037, 0.8884, 1.1747, 0.9097, 0.8275, 0.9351, 0.9777, 0.5686,\n                      0.7116, 0.6396, 0.6693, 0.6120, 0.9806, 0.9325, 1.4374, 0.8304, 0.5268,\n                      0.5590, 0.9665, 1.2604, 0.6980, 0.9645, 1.0494, 0.8909, 0.7634, 0.6234,\n                      0.5983, 0.7922, 1.0342, 1.0449, 0.5735, 0.7802, 0.4403, 0.6748, 0.5103,\n                      0.4964, 0.5101, 1.0377, 0.8227, 0.9585, 0.7161, 0.6164, 0.7425, 0.9107,\n                      0.7391, 1.0809, 1.2582, 0.9224, 0.7609, 0.5426, 0.3530, 0.7204, 1.0613,\n                      0.8936], device='cuda:0')),\n             ('layer1.0.bn2.bias',\n              tensor([ 0.0144, -0.3348, -0.3628,  0.1217, -0.3556, -0.0783, -0.1970, -0.1027,\n                      -0.2042, -0.4110,  0.0081, -0.0831, -0.2591, -0.0774, -0.1734, -0.1686,\n                      -0.2915, -0.2247, -0.3306, -0.3890, -0.6152, -0.1877, -0.2480, -0.1882,\n                      -0.3486, -0.1854, -0.1190, -0.5078, -0.1112, -0.3922, -0.1432, -0.0556,\n                      -0.0421, -0.0288, -0.2311, -0.2757, -0.5480, -0.2940, -0.1421, -0.2554,\n                      -0.3953, -0.3795, -0.2378, -0.1241, -0.1654, -0.1084, -0.1819, -0.2065,\n                       0.0197, -0.0922, -0.0640, -0.3753, -0.4395, -0.5905, -0.2511, -0.2811,\n                      -0.3344, -0.3078,  0.0485, -0.1039, -0.2679, -0.1703, -0.0428, -0.0701],\n                     device='cuda:0')),\n             ('layer1.0.bn2.running_mean',\n              tensor([ -7.7355,  -5.4955,   5.2574, -13.5740,  -4.2888,  -3.5819,  -7.0627,\n                       -9.3871, -10.7982,  -1.0756,  -3.4254,  -5.0122,  -6.0308,  -5.2878,\n                       -3.5150,  -3.3336, -10.5149,   1.0773,  -7.0594,  -7.9873,   9.0056,\n                       -7.5611,  -5.9428,  -3.6455,   6.8164,  -6.6070,  -1.4915,   6.5308,\n                       -4.7533, -10.2145,  -0.4565,  -2.6162,  -5.3401,  -4.0476,   4.3461,\n                        0.8737,   4.2686,  -2.5480,   1.1058,   3.4523,  11.8358,  -6.7410,\n                       -3.5448,  -6.1359,  -5.7807,  -1.2200,  -3.8130,  -4.9736,  -9.5044,\n                       -4.9330,  -8.3426,  -4.9785,  14.6107,  12.1705,  -7.5779,  -7.2720,\n                       -2.5048,  -2.8529,  -8.8717, -11.6024, -13.1701, -10.5926,  -3.3184,\n                      -12.9912], device='cuda:0')),\n             ('layer1.0.bn2.running_var',\n              tensor([136.6629, 102.8600,  32.2899,  58.2100, 134.6055, 110.3909,  55.0093,\n                      123.5556,  21.9407,  73.9090,  66.5734,  62.0179,  71.1350, 105.6895,\n                      126.7872, 124.9856,  13.0340,  66.4116,  51.3417,  80.2805, 102.2633,\n                      120.5149,  96.2495,  83.2299,  37.4559, 104.5998,  77.0238,  94.7706,\n                      139.0828,  75.8067, 119.1239, 109.5572,  55.1779, 137.6427, 101.2192,\n                       58.1438,  49.8741, 104.0054, 110.7324, 123.2839,  30.1462,  80.7844,\n                      184.7386, 131.7411,  78.8720,  69.4117,  43.7282,  53.6002,  53.8797,\n                       96.7014,  84.3235, 128.2377,  92.2469,  51.3222, 118.1079,  75.7085,\n                       77.2657, 103.5272,  39.8014,  81.4700,  61.2060,  57.7049,  42.0041,\n                      103.3486], device='cuda:0')),\n             ('layer1.0.bn2.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer1.1.conv1.weight',\n              tensor([[[[ 2.5805e-02,  1.2489e-01,  2.8566e-01],\n                        [-4.4842e-02,  2.0691e-02, -2.8154e-01],\n                        [-7.2015e-03, -3.1249e-01, -9.3373e-02]],\n              \n                       [[ 1.3155e-01,  1.8533e-01,  6.1496e-01],\n                        [-1.9674e-02, -1.0040e-01, -7.1477e-02],\n                        [-5.9810e-01, -3.4610e-01, -2.9434e-01]],\n              \n                       [[ 3.4106e-01,  7.7430e-02,  2.9069e-01],\n                        [ 8.5973e-02,  2.5822e-02,  2.7802e-01],\n                        [-6.4729e-02, -1.4942e-01,  2.6955e-03]],\n              \n                       ...,\n              \n                       [[-7.7796e-02, -2.8200e-01,  1.6230e-01],\n                        [-5.1128e-01, -1.8057e-01,  4.0983e-02],\n                        [-1.5414e-01, -1.1622e-01,  1.6250e-02]],\n              \n                       [[ 1.5412e-01,  1.3503e-01,  9.4342e-02],\n                        [-2.7036e-03,  8.2227e-02, -3.1736e-02],\n                        [-1.2630e-01,  6.0155e-02,  6.0402e-02]],\n              \n                       [[-5.1681e-02, -1.0643e-01,  1.0745e-01],\n                        [ 4.3410e-02,  6.5167e-02, -3.4639e-02],\n                        [-3.6335e-02, -1.7235e-01, -4.6900e-01]]],\n              \n              \n                      [[[ 2.7583e-01,  1.4896e-01, -7.9158e-02],\n                        [ 3.0734e-01, -1.1531e-02,  1.8252e-01],\n                        [ 1.7407e-01,  7.9819e-02, -5.0522e-01]],\n              \n                       [[ 2.1568e-01,  1.2493e-01, -6.0799e-02],\n                        [-1.3983e-01, -1.6832e-01,  2.2447e-01],\n                        [-3.0231e-01, -1.5152e-01, -2.6697e-01]],\n              \n                       [[ 4.2542e-02,  7.5258e-02, -5.7831e-02],\n                        [ 1.0433e-01,  4.4707e-03, -1.5121e-01],\n                        [-2.9792e-01, -3.3548e-01,  1.1400e-01]],\n              \n                       ...,\n              \n                       [[-2.3751e-02, -1.8540e-01, -4.4880e-01],\n                        [ 9.7401e-03, -1.7073e-01, -1.4716e-01],\n                        [-3.0621e-01, -4.1858e-01,  1.2201e-01]],\n              \n                       [[ 3.7309e-02,  2.0166e-01, -3.7511e-02],\n                        [ 1.6377e-01,  9.3785e-02, -4.8493e-01],\n                        [-7.7710e-02, -1.1211e-01, -1.4366e-01]],\n              \n                       [[ 3.9180e-01,  2.6257e-01,  2.2105e-01],\n                        [ 2.9275e-01,  9.7102e-02,  5.8166e-04],\n                        [-2.5107e-01, -3.3707e-01, -1.1001e-01]]],\n              \n              \n                      [[[ 1.5759e-01,  1.8568e-01, -5.9068e-02],\n                        [ 5.8221e-02, -1.5504e-01,  1.2697e-03],\n                        [-2.8713e-01,  9.2738e-02,  4.4031e-02]],\n              \n                       [[ 7.6570e-02,  5.1436e-02, -1.8840e-01],\n                        [-6.3844e-02, -2.6642e-01, -1.3002e-01],\n                        [-4.9312e-01, -3.5707e-01, -4.0492e-01]],\n              \n                       [[ 1.2191e-01,  2.5875e-01,  3.6363e-01],\n                        [ 1.7338e-01,  3.4746e-01,  1.5727e-01],\n                        [ 6.9672e-02, -4.9363e-02, -4.1253e-02]],\n              \n                       ...,\n              \n                       [[-2.5743e-01, -2.0178e-01, -7.7796e-02],\n                        [-2.0109e-01, -1.5053e-01,  2.1274e-03],\n                        [-3.1242e-01, -2.0067e-01,  5.5160e-02]],\n              \n                       [[-1.9700e-02, -1.5827e-02, -1.0030e-01],\n                        [-5.3397e-02,  6.9333e-02, -1.6336e-02],\n                        [ 3.0760e-01,  7.5868e-02,  1.1006e-01]],\n              \n                       [[ 1.4773e-01,  5.9089e-02, -1.6229e-01],\n                        [ 2.3711e-01, -1.4725e-01, -4.1125e-01],\n                        [ 2.8294e-02, -1.4901e-01, -9.3879e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.0041e-01, -3.9138e-01,  6.4286e-02],\n                        [-2.3408e-01, -1.1430e-01,  2.4077e-01],\n                        [-6.8663e-01, -2.7038e-01,  2.7655e-01]],\n              \n                       [[ 2.2568e-01,  7.1773e-02, -2.4303e-01],\n                        [-2.9772e-01,  7.0581e-02, -2.5290e-01],\n                        [-2.2658e-01, -2.0803e-01, -1.0971e-01]],\n              \n                       [[ 5.1410e-02,  1.3623e-01, -1.1635e-02],\n                        [-1.9271e-01,  6.8714e-02,  1.1936e-01],\n                        [ 2.8353e-01,  1.3417e-01,  3.7792e-02]],\n              \n                       ...,\n              \n                       [[-5.5302e-02, -2.4613e-01, -1.8632e-02],\n                        [-4.2595e-01, -1.1461e-01,  1.7756e-01],\n                        [-3.0186e-01,  9.7515e-02,  2.7564e-02]],\n              \n                       [[-2.6552e-01, -3.1760e-01, -4.3563e-02],\n                        [ 2.8820e-01,  1.7783e-01,  4.8202e-01],\n                        [-4.8335e-02, -2.9401e-01, -1.2970e-01]],\n              \n                       [[ 9.7006e-02,  1.6689e-01,  1.9735e-02],\n                        [-2.0917e-01, -9.3360e-02,  7.5798e-02],\n                        [ 1.8383e-01,  8.6527e-02,  5.1595e-02]]],\n              \n              \n                      [[[ 2.1037e-01,  1.0900e-01,  6.1276e-02],\n                        [-1.2254e-01, -7.9325e-02, -1.6430e-01],\n                        [-3.7850e-01, -2.7611e-01, -1.7256e-01]],\n              \n                       [[ 2.5185e-01,  2.2951e-01,  1.7578e-01],\n                        [-4.6275e-01, -2.2736e-01, -3.1292e-01],\n                        [ 1.0393e-01,  1.1565e-01,  1.5881e-02]],\n              \n                       [[-2.0856e-01, -2.7427e-01, -2.0580e-01],\n                        [-1.5628e-01, -1.8495e-01, -4.4785e-02],\n                        [ 1.0679e-02,  1.0317e-01,  2.6171e-02]],\n              \n                       ...,\n              \n                       [[ 2.8377e-02, -2.4660e-01, -8.3169e-02],\n                        [ 2.6702e-01,  1.8087e-01,  1.9854e-01],\n                        [ 2.9780e-01,  2.4733e-02,  2.8692e-02]],\n              \n                       [[ 9.7742e-02, -4.3289e-01, -5.2613e-01],\n                        [-6.4621e-02, -7.6737e-02,  2.7677e-01],\n                        [ 2.2740e-01,  3.0731e-01,  1.0617e-01]],\n              \n                       [[-7.4666e-02, -1.6578e-01, -2.2115e-01],\n                        [ 1.5883e-01, -2.7481e-02, -2.7594e-01],\n                        [ 2.5673e-01,  1.1481e-01,  1.4873e-01]]],\n              \n              \n                      [[[-3.7733e-01, -7.3997e-01,  5.0706e-01],\n                        [-4.1297e-01, -5.1074e-01, -3.2091e-01],\n                        [-7.8629e-01, -3.8915e-01,  3.9683e-01]],\n              \n                       [[-1.5381e-01,  1.3238e-01, -1.7421e-01],\n                        [-1.4005e-01, -1.0434e-01, -4.6606e-01],\n                        [-3.0276e-01,  9.2635e-02,  8.4878e-02]],\n              \n                       [[ 1.0530e-01, -7.2623e-02, -2.2473e-01],\n                        [-8.3837e-02, -1.0282e-01,  2.5655e-01],\n                        [ 2.3308e-02,  1.9266e-01,  6.6402e-02]],\n              \n                       ...,\n              \n                       [[-9.6564e-02,  1.8307e-01, -1.1908e-01],\n                        [-5.5925e-01,  1.8234e-01, -3.6510e-02],\n                        [ 3.3265e-02, -2.1473e-02,  1.6725e-02]],\n              \n                       [[ 5.9607e-02, -1.8693e-01,  5.9000e-02],\n                        [-7.8219e-02, -1.9753e-01,  3.5530e-01],\n                        [ 1.0544e-01, -3.0528e-01, -1.1919e-01]],\n              \n                       [[-1.0338e-03,  2.3140e-01,  1.6358e-01],\n                        [-2.2908e-01,  9.0720e-02,  6.0785e-02],\n                        [ 1.6730e-01,  9.5150e-02,  2.8162e-02]]]], device='cuda:0')),\n             ('layer1.1.bn1.weight',\n              tensor([0.8926, 1.0107, 0.8182, 0.8677, 0.8995, 1.0418, 0.8566, 0.7921, 0.8899,\n                      1.1260, 1.0979, 0.9127, 0.9479, 1.0689, 0.6971, 0.8345, 1.1727, 1.0042,\n                      0.9030, 0.8092, 0.7272, 1.0796, 0.9784, 0.7605, 0.9330, 0.8394, 0.9728,\n                      1.3170, 1.0320, 1.0495, 1.1740, 0.8390, 1.0389, 0.9966, 0.9822, 1.0280,\n                      1.1082, 1.0624, 0.7691, 0.8194, 0.7601, 0.7657, 0.7625, 1.1272, 0.9712,\n                      0.9322, 0.8353, 1.0208, 0.8261, 0.7407, 0.9431, 0.8399, 1.1039, 0.9176,\n                      0.5806, 1.0700, 0.7497, 0.9449, 0.9747, 0.9006, 0.8040, 0.8353, 1.4224,\n                      1.0005], device='cuda:0')),\n             ('layer1.1.bn1.bias',\n              tensor([-0.3184, -0.3577, -0.3835, -0.3477, -0.2624, -0.1822, -0.5361, -0.2606,\n                      -0.3715, -0.2579, -0.0989, -0.2827, -0.3851, -0.2503, -0.3005, -0.1502,\n                      -0.0781, -0.2003, -0.1814, -0.3409, -0.4357, -0.2886, -0.1914, -0.2772,\n                      -0.3722, -0.2700, -0.2713, -0.3271, -0.1281, -0.1980, -0.3290, -0.2133,\n                      -0.2582, -0.2202, -0.3478, -0.2595, -0.3040, -0.1189, -0.3364, -0.4762,\n                      -0.4264, -0.2811, -0.3322, -0.0876, -0.2976, -0.2851, -0.2237, -0.3465,\n                      -0.2088, -0.3222, -0.3299, -0.2604, -0.4306, -0.2467, -0.3893, -0.3194,\n                      -0.3151, -0.1996, -0.2198, -0.2620, -0.3984, -0.3251, -0.1952, -0.2918],\n                     device='cuda:0')),\n             ('layer1.1.bn1.running_mean',\n              tensor([  1.6723,   5.1143,  17.5552, -18.9569,  -9.4814,  -0.7797,  11.5072,\n                      -12.8922,   2.6996, -12.4412,  -8.4246, -16.6371,  10.0058,  -8.9809,\n                      -15.4361,  -9.8959,  -4.0734,  -6.6530,  -7.6228, -12.6932, -13.2642,\n                       -1.5836, -12.6997,  -4.7249,  -3.8809, -16.9464, -11.8627,  -6.4804,\n                      -12.0143, -15.1146, -15.2108,  15.6974,   6.5592, -16.0004,  -9.4988,\n                       -5.7523, -13.1418, -15.6687,  -4.9558,  -0.8408, -16.0003, -11.9332,\n                      -11.6820, -11.0408, -15.6035, -16.0469, -19.7067,   8.0024, -12.3617,\n                      -18.1115, -15.2324,  -3.4628,   8.4472, -17.8981,  13.4412,  -9.5244,\n                      -11.9520, -11.8112, -18.0876, -15.7798, -11.7136, -16.7102,  -0.7456,\n                      -15.8698], device='cuda:0')),\n             ('layer1.1.bn1.running_var',\n              tensor([152.9200, 174.3897, 603.4169, 425.6005, 181.6059, 195.1366, 471.6331,\n                      205.0724, 308.6921, 185.3142, 109.5600, 163.7714, 218.4442, 347.2506,\n                      382.9156, 330.7028, 204.9877, 391.6717, 372.7727, 221.0058, 293.0631,\n                      354.6078, 231.8331, 121.1666, 207.0454, 206.7263, 176.5173, 164.2965,\n                      333.0988, 254.6068, 255.8685, 339.7684, 397.7401, 423.3407, 120.0335,\n                       81.9897, 212.1059, 380.5538, 166.3923,  86.7288, 420.3835, 320.0256,\n                      215.4823, 243.9821, 226.0438, 427.2993, 914.7674, 242.0278, 357.8031,\n                      696.2366, 214.9176, 136.6471, 437.8170, 536.9063, 401.6281, 242.0232,\n                      246.9379, 223.4337, 320.9153, 556.9800, 166.3403, 270.7749, 159.4381,\n                      201.8642], device='cuda:0')),\n             ('layer1.1.bn1.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer1.1.conv2.weight',\n              tensor([[[[-0.1331, -0.1066,  0.2710],\n                        [ 0.0195,  0.1752, -0.0034],\n                        [-0.2760,  0.0251, -0.1615]],\n              \n                       [[-0.1756, -0.2709, -0.0458],\n                        [ 0.0880, -0.1176, -0.3382],\n                        [-0.0163, -0.5228, -0.4157]],\n              \n                       [[-0.3307, -0.1476, -0.0505],\n                        [-0.1630, -0.3475, -0.2024],\n                        [-0.1773, -0.1441, -0.0276]],\n              \n                       ...,\n              \n                       [[ 0.1421,  0.1815,  0.1706],\n                        [ 0.1696,  0.1618,  0.1990],\n                        [ 0.0353, -0.0647,  0.2953]],\n              \n                       [[ 0.0212, -0.0229, -0.0319],\n                        [-0.2768, -0.1920,  0.0763],\n                        [-0.3788, -0.4349, -0.3379]],\n              \n                       [[ 0.4632,  0.2621, -0.0508],\n                        [ 0.1633, -0.0784,  0.1914],\n                        [ 0.4442,  0.0898,  0.5489]]],\n              \n              \n                      [[[-0.0520,  0.1084, -0.1611],\n                        [ 0.1507, -0.3336, -0.0895],\n                        [-0.3716, -0.7284, -0.2340]],\n              \n                       [[ 0.0291, -0.1754,  0.0539],\n                        [ 0.1909, -0.1032, -0.4680],\n                        [ 0.1991, -0.3564, -0.2759]],\n              \n                       [[ 0.0289,  0.1446, -0.0151],\n                        [-0.1763, -0.5529, -0.4081],\n                        [-0.2390, -0.2884, -0.2339]],\n              \n                       ...,\n              \n                       [[ 0.0744, -0.0533,  0.0283],\n                        [ 0.0996,  0.0340,  0.1125],\n                        [ 0.1700, -0.0582, -0.2359]],\n              \n                       [[ 0.0376,  0.1181, -0.0155],\n                        [-0.1478, -0.2343, -0.1786],\n                        [ 0.0273, -0.1053, -0.0410]],\n              \n                       [[ 0.0906, -0.0576, -0.0955],\n                        [ 0.2292, -0.1181,  0.2438],\n                        [ 0.3977, -0.0140,  0.1620]]],\n              \n              \n                      [[[-0.2121, -0.4035, -0.0465],\n                        [ 0.0078, -0.2726, -0.4334],\n                        [-0.5031,  0.0100,  0.1124]],\n              \n                       [[-0.0148, -0.5799, -0.4778],\n                        [-0.2980, -0.0303, -0.4381],\n                        [ 0.3059, -0.6473, -0.4625]],\n              \n                       [[-0.3796, -0.2274, -0.1532],\n                        [-0.1535, -0.2617, -0.1883],\n                        [-0.1782, -0.2157,  0.0358]],\n              \n                       ...,\n              \n                       [[-0.1409,  0.2044, -0.1709],\n                        [ 0.2012,  0.0646,  0.0449],\n                        [ 0.1938, -0.2130, -0.1676]],\n              \n                       [[ 0.1048,  0.1285, -0.1239],\n                        [-0.1291, -0.1798, -0.1847],\n                        [-0.0894, -0.0111,  0.1691]],\n              \n                       [[ 0.1399,  0.1161, -0.0053],\n                        [ 0.1668, -0.1752,  0.0496],\n                        [ 0.2030, -0.2066,  0.1912]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.5627, -0.1521,  0.2601],\n                        [-0.3277, -0.1116, -0.1661],\n                        [-0.0783,  0.1127,  0.0166]],\n              \n                       [[-0.0374, -0.2219,  0.1852],\n                        [-0.0024,  0.0138,  0.0211],\n                        [-0.1253,  0.1013,  0.0165]],\n              \n                       [[-0.5819, -0.1671, -0.0543],\n                        [-0.3008, -0.1945, -0.1959],\n                        [ 0.0836, -0.0530, -0.0453]],\n              \n                       ...,\n              \n                       [[-0.3273,  0.1567, -0.0477],\n                        [-0.0015,  0.0626, -0.0189],\n                        [-0.2692, -0.0183,  0.0603]],\n              \n                       [[-0.0509, -0.0754, -0.1375],\n                        [ 0.2752, -0.0473,  0.0067],\n                        [-0.0776, -0.0620,  0.2041]],\n              \n                       [[-0.1367,  0.2074,  0.1512],\n                        [ 0.0703,  0.0426,  0.1729],\n                        [-0.0300, -0.0595,  0.1569]]],\n              \n              \n                      [[[-0.1190, -0.5917, -0.2762],\n                        [ 0.1474,  0.1143, -0.4043],\n                        [-0.1665, -0.3348, -0.0976]],\n              \n                       [[-0.1599,  0.1074, -0.1189],\n                        [-0.2305,  0.0861, -0.1031],\n                        [-0.1562, -0.4728, -0.0727]],\n              \n                       [[ 0.0309, -0.0422, -0.2782],\n                        [ 0.0527, -0.0402, -0.4096],\n                        [-0.3021, -0.3468, -0.1999]],\n              \n                       ...,\n              \n                       [[ 0.1486,  0.0577, -0.0536],\n                        [ 0.0864, -0.0338,  0.0923],\n                        [ 0.2729,  0.1982, -0.0472]],\n              \n                       [[-0.2874, -0.2644, -0.0645],\n                        [ 0.0205,  0.0582,  0.0170],\n                        [-0.3090, -0.4234, -0.1781]],\n              \n                       [[ 0.0391,  0.1372,  0.0219],\n                        [ 0.0512, -0.2994, -0.1701],\n                        [ 0.1392,  0.0703,  0.1700]]],\n              \n              \n                      [[[ 0.0945,  0.0802, -0.0073],\n                        [-0.0855,  0.1229,  0.2171],\n                        [ 0.1960,  0.1924,  0.1890]],\n              \n                       [[ 0.3611, -0.0309,  0.0456],\n                        [ 0.2918,  0.0304,  0.0117],\n                        [ 0.5901,  0.1631,  0.1500]],\n              \n                       [[ 0.0985, -0.0080,  0.1270],\n                        [ 0.2111,  0.0401,  0.1669],\n                        [ 0.3162,  0.2967,  0.2921]],\n              \n                       ...,\n              \n                       [[-0.1883, -0.2569, -0.4042],\n                        [-0.4123, -0.2636, -0.0761],\n                        [-0.2031, -0.1402,  0.0607]],\n              \n                       [[ 0.1161, -0.0635,  0.0389],\n                        [-0.0977, -0.2324, -0.1314],\n                        [ 0.2131,  0.3044,  0.2866]],\n              \n                       [[-0.2194, -0.2922, -0.3208],\n                        [-0.2944, -0.4574, -0.3890],\n                        [ 0.0121, -0.1157,  0.2076]]]], device='cuda:0')),\n             ('layer1.1.bn2.weight',\n              tensor([1.1662, 0.7847, 0.8813, 0.3937, 0.8732, 0.7163, 0.8956, 0.3797, 1.0972,\n                      0.9821, 0.5811, 0.8568, 0.8461, 0.8716, 0.5332, 0.3543, 1.0858, 0.7028,\n                      0.5554, 1.2839, 0.9882, 0.5413, 0.4726, 0.7272, 1.1586, 0.8066, 0.8768,\n                      0.4450, 1.0919, 0.6457, 0.8878, 1.0013, 0.8360, 0.9032, 0.8876, 0.9757,\n                      1.0017, 0.5237, 0.6766, 0.4725, 1.1520, 0.7066, 0.5532, 0.8200, 0.9604,\n                      0.9361, 0.8968, 0.7181, 0.9679, 0.6175, 0.8113, 1.0396, 0.9937, 1.0550,\n                      0.9592, 0.1828, 0.5443, 1.0778, 0.8921, 0.5898, 0.3261, 0.7757, 0.9092,\n                      0.3694], device='cuda:0')),\n             ('layer1.1.bn2.bias',\n              tensor([-0.0591, -0.2085, -0.2621, -0.3147, -0.3277,  0.0951, -0.3928, -0.3824,\n                      -0.3322, -0.1420,  0.0848, -0.0518, -0.1486, -0.1483, -0.2785, -0.3443,\n                      -0.3985, -0.1475, -0.3031, -0.3196, -0.2551, -0.1296, -0.4551, -0.2659,\n                      -0.2555, -0.0712,  0.0884, -0.5341, -0.6327, -0.3495, -0.0994, -0.1097,\n                       0.1248,  0.0592, -0.3532, -0.0070, -0.1313, -0.6668,  0.3094, -0.4191,\n                      -0.1842, -0.3645, -0.1507,  0.1600,  0.0812, -0.0669,  0.0471, -0.0983,\n                      -0.0794, -0.2463, -0.1553, -0.4235, -0.2474, -0.6987, -0.4313, -0.2597,\n                      -0.2872, -0.4237,  0.0260, -0.2089, -0.3535, -0.3046,  0.1733, -0.5107],\n                     device='cuda:0')),\n             ('layer1.1.bn2.running_mean',\n              tensor([ 0.3532, -3.9964, -5.3720, -4.5834, -0.7266,  0.9841, -4.7325, -2.6370,\n                      -5.8566, -6.5114, -4.0452, -4.2026, -4.1413, -4.0219,  1.8142,  2.5172,\n                      -3.7827, -0.3181, -5.2852, -8.0488,  1.7650, -7.2943,  1.9898,  0.7263,\n                       1.3867, -7.1632, -3.0689,  4.9640, 12.8287,  1.6406, -0.8866,  6.6029,\n                       0.7819, -2.4947, -0.2777, -1.6836,  2.2841, 10.6599, -0.3888, -0.1717,\n                      -1.8804, -5.4870, -5.7786, -0.8273, -1.0336, -1.3553, -3.4353, -2.5926,\n                      -6.7107,  0.4927, -2.8582, -2.6462, -1.9641,  9.7791, -3.4165,  1.0784,\n                       0.0435, -1.5428, -4.2863, -4.7231, -0.7837, -4.2469, -4.6810, -0.1585],\n                     device='cuda:0')),\n             ('layer1.1.bn2.running_var',\n              tensor([ 95.8222,  94.9123,  81.3225,  34.9760,  97.3774, 111.6778,  39.0460,\n                       84.5693,  29.5982,  73.1518,  77.4941,  56.2650,  98.2065, 113.2620,\n                       84.4797,  50.3843,  71.6951,  30.3761,  43.3884,  38.6041,  98.0666,\n                      133.0618,  80.8136,  58.4289,  38.9896,  99.7901,  94.2730,  44.1881,\n                       40.1413,  74.9935,  68.2120,  40.4717,  77.4538,  94.7080,  98.5074,\n                       74.0962,  94.2007,  38.5614, 107.9688,  49.6882,  45.0829,  56.6502,\n                      133.2994, 107.8808,  64.3869, 130.6116,  83.9622,  69.4454,  53.5551,\n                       54.6222,  70.7157,  49.0775,  81.6554,  63.8936,  83.6785,  73.9929,\n                       36.3185,  79.0402,  41.4992, 118.3804,  83.2280,  70.5181,  64.1687,\n                       95.2968], device='cuda:0')),\n             ('layer1.1.bn2.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer2.0.identity_downsample.0.weight',\n              tensor([[[[ 1.1931e-01]],\n              \n                       [[-3.4676e-01]],\n              \n                       [[-1.2305e-01]],\n              \n                       ...,\n              \n                       [[ 1.0100e-01]],\n              \n                       [[-2.7997e-01]],\n              \n                       [[-4.7405e-04]]],\n              \n              \n                      [[[-1.7559e-01]],\n              \n                       [[-1.2051e-01]],\n              \n                       [[ 3.1227e-02]],\n              \n                       ...,\n              \n                       [[ 1.2746e-01]],\n              \n                       [[-1.4227e-01]],\n              \n                       [[ 5.2202e-02]]],\n              \n              \n                      [[[ 7.5087e-02]],\n              \n                       [[-1.8733e-01]],\n              \n                       [[-1.9047e-01]],\n              \n                       ...,\n              \n                       [[-2.1907e-01]],\n              \n                       [[ 9.5268e-02]],\n              \n                       [[-1.4824e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 6.1280e-02]],\n              \n                       [[ 1.4367e-02]],\n              \n                       [[ 4.5934e-02]],\n              \n                       ...,\n              \n                       [[-1.0339e-01]],\n              \n                       [[-1.6276e-01]],\n              \n                       [[-7.4713e-02]]],\n              \n              \n                      [[[-2.0112e-02]],\n              \n                       [[ 2.5839e-01]],\n              \n                       [[ 3.6773e-02]],\n              \n                       ...,\n              \n                       [[ 1.5725e-02]],\n              \n                       [[-2.7675e-01]],\n              \n                       [[ 2.5401e-01]]],\n              \n              \n                      [[[ 2.5304e-01]],\n              \n                       [[-3.2292e-02]],\n              \n                       [[-1.6931e-01]],\n              \n                       ...,\n              \n                       [[ 4.9177e-01]],\n              \n                       [[-5.2357e-02]],\n              \n                       [[-2.6410e-01]]]], device='cuda:0')),\n             ('layer2.0.identity_downsample.1.weight',\n              tensor([1.1788, 0.8266, 0.7401, 0.8255, 1.0491, 1.1074, 1.0601, 1.2303, 1.0906,\n                      1.0595, 1.0938, 1.1002, 0.6832, 1.0489, 0.8484, 0.8428, 0.8321, 1.0706,\n                      1.3936, 1.1875, 1.0533, 0.9133, 1.3895, 0.8937, 1.0236, 0.9920, 0.8520,\n                      1.0448, 1.3544, 0.8728, 1.0942, 1.3312, 0.7646, 0.6905, 0.8146, 1.3507,\n                      1.1941, 0.8894, 1.3611, 1.1713, 1.0248, 1.2575, 0.8747, 1.0921, 0.7085,\n                      1.2665, 1.2868, 0.9123, 0.8123, 0.8064, 0.9651, 1.2545, 1.2006, 0.8507,\n                      0.9976, 0.7889, 0.9322, 1.3566, 1.2952, 1.2664, 0.6481, 1.1329, 0.9696,\n                      0.9608, 1.3088, 1.1375, 1.1774, 0.8887, 0.9008, 1.1397, 1.0744, 1.0760,\n                      0.8024, 1.2675, 1.5632, 0.7943, 1.1106, 0.9493, 0.6950, 1.2145, 0.8511,\n                      1.1710, 0.6972, 1.2332, 0.9771, 1.1690, 0.9876, 1.2143, 1.4074, 0.9945,\n                      1.3570, 1.1304, 1.2049, 1.0168, 0.9397, 1.0824, 1.2661, 1.0418, 0.7577,\n                      0.8697, 0.8674, 0.8484, 0.6880, 1.2860, 1.2001, 1.0073, 1.2141, 1.2076,\n                      1.0634, 0.7038, 0.7981, 0.8076, 0.8084, 0.8786, 0.8740, 0.9371, 0.7860,\n                      1.1444, 1.3686, 0.8320, 1.2524, 1.3955, 1.0349, 1.1988, 1.3544, 0.9656,\n                      1.0210, 0.9646], device='cuda:0')),\n             ('layer2.0.identity_downsample.1.bias',\n              tensor([-0.2339, -0.2094, -0.3815, -0.1874, -0.2868, -0.1927, -0.0253, -0.3702,\n                      -0.5284,  0.0418, -0.1311, -0.1052, -0.4340, -0.3318, -0.2440, -0.1768,\n                      -0.2524, -0.1374, -0.0902, -0.1147, -0.0775, -0.4016, -0.3018, -0.1562,\n                      -0.1041, -0.2133, -0.1233, -0.3602,  0.0192, -0.4430, -0.1703,  0.1092,\n                      -0.3613, -0.1243, -0.2960, -0.0294, -0.3702, -0.0668, -0.2700, -0.0519,\n                      -0.3283,  0.0816, -0.2697, -0.0352, -0.2523,  0.0511, -0.1655, -0.1442,\n                      -0.2936, -0.3292, -0.0689, -0.3132, -0.0697, -0.1826, -0.2233, -0.2166,\n                      -0.2958, -0.0928, -0.3085, -0.0651, -0.3593, -0.0317, -0.3622, -0.1762,\n                       0.1782, -0.1859, -0.1988,  0.0482, -0.2351, -0.2108, -0.1019, -0.0997,\n                      -0.2085, -0.3903, -0.0748, -0.2283, -0.2263, -0.0864, -0.1156, -0.1803,\n                      -0.1550, -0.1897, -0.1944, -0.1470, -0.2721, -0.0283, -0.0138, -0.3738,\n                      -0.2868, -0.3596, -0.0773, -0.2948, -0.1887, -0.5778, -0.2414,  0.0389,\n                      -0.1090, -0.1522, -0.1252, -0.0190, -0.2215, -0.2365, -0.3247, -0.1299,\n                      -0.1108, -0.3038, -0.0883, -0.2468,  0.0467, -0.0953, -0.3670, -0.2969,\n                      -0.1369, -0.1184, -0.1923, -0.2759, -0.3361, -0.2045, -0.3099, -0.2977,\n                      -0.1238, -0.0744, -0.2073, -0.2416, -0.2388,  0.0280, -0.2217, -0.3200],\n                     device='cuda:0')),\n             ('layer2.0.identity_downsample.1.running_mean',\n              tensor([-1.9591, -1.4822, -0.4409,  4.9500, -0.7658, -0.0304, -1.2899, -0.5155,\n                       3.2048,  2.0357, -1.9625,  0.5230,  4.5911, -1.2773,  2.2934, -0.6931,\n                       5.4859, -0.7190,  1.3924, -0.4222,  0.3960,  1.4903,  0.3474, -0.6641,\n                      -1.3438,  6.1606,  1.6427,  0.2821,  3.0136, -0.1906, -1.2684, -0.4644,\n                       4.8380,  5.2365,  5.7169,  0.9168,  1.8499, -2.0027,  1.1272,  1.0678,\n                      -2.6636,  2.0106, -3.4157, -0.3567, -0.3455,  1.4010, -0.0321,  5.4138,\n                       2.7895, -3.2005,  0.3304, -0.4228, -0.2610,  3.3447,  0.0386, -2.7082,\n                       4.3881,  4.9031,  4.4252, -3.0924,  0.9932, -1.8719,  1.8373, -0.4661,\n                       0.8738,  1.2385,  1.9556, -2.6887, -1.5234, -0.1728, -3.0599,  3.4546,\n                       0.0909,  3.2656, -2.2839, -4.0071,  2.8847,  0.0163,  0.3908, -2.0770,\n                       4.9423,  0.9038,  1.5470,  0.8480,  2.1574, -3.4939,  1.9733,  1.1954,\n                      -1.5291, -0.3750, -5.2681,  2.4366, -1.5551,  1.2318,  3.5935,  3.6886,\n                      -2.2652, -0.4013,  5.3458, -0.9583,  2.3101,  2.5710,  2.0631,  1.8378,\n                      -2.9803,  2.1606, -0.4501, -2.5034, -1.1441,  3.0772,  4.1834, -1.5477,\n                       3.2443,  0.4130,  1.2296,  3.4539,  4.7797,  1.5099,  1.1684,  0.0175,\n                       2.1704,  3.5749, -0.0341,  1.5970,  3.9467, -0.8907, -1.2427, -0.5337],\n                     device='cuda:0')),\n             ('layer2.0.identity_downsample.1.running_var',\n              tensor([2.2218, 2.9488, 4.6773, 5.1279, 1.7103, 1.4080, 2.5514, 3.2863, 3.2065,\n                      4.2431, 3.0870, 1.3097, 3.8166, 3.0829, 3.0694, 3.0884, 4.2738, 2.7193,\n                      1.7007, 1.9192, 2.6772, 2.1628, 1.3860, 1.7132, 1.6129, 5.4803, 2.7952,\n                      1.4233, 3.9317, 3.9122, 2.2799, 1.6534, 3.4927, 5.2323, 4.1181, 2.4103,\n                      1.4515, 4.1685, 3.7036, 4.0985, 2.4495, 4.7096, 3.0769, 3.6136, 4.8101,\n                      2.4137, 1.8709, 4.1394, 1.5370, 3.5339, 2.1769, 2.9333, 0.9294, 3.9486,\n                      2.1515, 7.3337, 3.6789, 4.0684, 3.8296, 3.3493, 4.1631, 3.3056, 1.6128,\n                      6.0494, 2.8548, 1.1604, 2.2561, 5.1912, 3.7006, 2.2587, 2.9007, 3.9054,\n                      4.8861, 2.2370, 2.3313, 4.5107, 2.1199, 6.2717, 3.3455, 2.4569, 4.8486,\n                      1.2003, 5.2739, 3.1538, 2.1065, 3.2759, 3.3705, 2.1850, 3.8486, 2.2557,\n                      4.4364, 1.7764, 1.2803, 2.0632, 4.0696, 4.4554, 1.9186, 3.0911, 4.7322,\n                      6.1066, 5.1767, 2.4069, 3.8955, 2.1026, 2.8872, 3.1326, 1.6924, 2.4647,\n                      3.5552, 3.8771, 3.3229, 2.3171, 4.3481, 3.7730, 2.8877, 4.1646, 3.6502,\n                      3.7847, 1.4874, 4.1872, 1.7990, 3.7510, 2.9097, 2.1618, 3.6169, 5.7109,\n                      3.7767, 1.7339], device='cuda:0')),\n             ('layer2.0.identity_downsample.1.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer2.0.conv1.weight',\n              tensor([[[[-0.1473, -0.0645,  0.2107],\n                        [-0.3006, -0.0956, -0.1470],\n                        [ 0.0740, -0.1149,  0.0779]],\n              \n                       [[ 0.0613,  0.7343,  0.1807],\n                        [-0.4177, -0.2146, -0.4191],\n                        [ 0.0557, -0.0430, -0.0283]],\n              \n                       [[-0.0734, -0.2944, -0.3078],\n                        [-0.2470, -0.2355, -0.0979],\n                        [ 0.0679,  0.1361,  0.0400]],\n              \n                       ...,\n              \n                       [[-0.3840,  0.2268, -0.0269],\n                        [ 0.4091, -0.1308, -0.0397],\n                        [ 0.4257,  0.3066, -0.2229]],\n              \n                       [[-0.4249, -0.6875,  0.1074],\n                        [ 0.0460,  0.1324,  0.0854],\n                        [ 0.1045, -0.0369,  0.0248]],\n              \n                       [[-0.5717, -0.3840, -0.5579],\n                        [ 0.0373,  0.0150, -0.2107],\n                        [ 0.1273,  0.2095,  0.1473]]],\n              \n              \n                      [[[-0.3951, -0.2697, -0.2864],\n                        [ 0.3253, -0.3027,  0.1782],\n                        [-0.1343,  0.0764, -0.2883]],\n              \n                       [[-0.2866,  0.0077,  0.0666],\n                        [-0.2969,  0.3400,  0.4358],\n                        [-0.1857,  0.6929,  0.0205]],\n              \n                       [[-0.0181, -0.1933,  0.1411],\n                        [-0.1305, -0.0886, -0.0733],\n                        [-0.5354,  0.1685,  0.5634]],\n              \n                       ...,\n              \n                       [[-0.3164,  0.1491, -0.2728],\n                        [ 0.1506,  0.2309, -0.1966],\n                        [ 0.0053,  0.1509, -0.0034]],\n              \n                       [[ 0.1975,  0.4375,  0.2625],\n                        [-0.0306,  0.4220, -0.6127],\n                        [-0.0333, -0.0487,  0.2452]],\n              \n                       [[-0.1990, -0.0542,  0.3331],\n                        [ 0.0623,  0.1998,  0.2168],\n                        [-0.0261,  0.1525,  0.1670]]],\n              \n              \n                      [[[-0.2056, -0.0281, -0.2032],\n                        [ 0.0633, -0.1114,  0.0959],\n                        [ 0.0956,  0.2218,  0.3516]],\n              \n                       [[-0.2080, -0.0691, -0.0367],\n                        [-0.2679,  0.4827,  0.4014],\n                        [-0.3034,  0.2068, -0.0183]],\n              \n                       [[ 0.1223, -0.1605, -0.1589],\n                        [-0.1452, -0.0660, -0.1577],\n                        [-0.0499,  0.1654,  0.5029]],\n              \n                       ...,\n              \n                       [[-0.2837,  0.0517, -0.1793],\n                        [ 0.1331,  0.0784, -0.1889],\n                        [ 0.0743,  0.3710,  0.2762]],\n              \n                       [[ 0.2684,  0.3054,  0.2477],\n                        [-0.0034,  0.3323, -0.3179],\n                        [ 0.1004, -0.1020,  0.0227]],\n              \n                       [[-0.0264, -0.1098,  0.1679],\n                        [-0.0342, -0.2661,  0.1510],\n                        [-0.1605,  0.0897,  0.2596]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.2988, -0.1936,  0.0549],\n                        [ 0.1733,  0.3659,  0.1416],\n                        [ 0.1114,  0.1838, -0.2603]],\n              \n                       [[-0.2003,  0.1438, -0.3541],\n                        [-0.5347, -0.0515, -0.1766],\n                        [-0.2137,  0.0636, -0.2495]],\n              \n                       [[ 0.3520, -0.0526,  0.0924],\n                        [-0.3675, -0.6278,  0.0082],\n                        [-0.2631, -0.0075, -0.1555]],\n              \n                       ...,\n              \n                       [[ 0.1765,  0.4886,  0.2031],\n                        [-0.3713,  0.1341,  0.1397],\n                        [ 0.0024,  0.0929, -0.0043]],\n              \n                       [[-0.0798, -0.1347, -0.2273],\n                        [ 0.0230, -0.1058,  0.0303],\n                        [ 0.2533,  0.3295,  0.0847]],\n              \n                       [[ 0.2188,  0.0837, -0.1165],\n                        [-0.2888, -0.2772, -0.2053],\n                        [ 0.0292,  0.0085, -0.0156]]],\n              \n              \n                      [[[-0.2055, -0.2554, -0.3652],\n                        [-0.4645,  0.0638,  0.1495],\n                        [ 0.0342, -0.2266, -0.0466]],\n              \n                       [[ 0.2315,  0.0330,  0.0558],\n                        [-0.2074,  0.0435,  0.2198],\n                        [ 0.1786,  0.2061, -0.0223]],\n              \n                       [[ 0.1120,  0.0012,  0.1939],\n                        [-0.0225, -0.0500,  0.0920],\n                        [-0.0377,  0.2636,  0.1261]],\n              \n                       ...,\n              \n                       [[ 0.1053,  0.2949,  0.2347],\n                        [ 0.1591, -0.2036, -0.1760],\n                        [ 0.1220, -0.0177, -0.2737]],\n              \n                       [[ 0.0772,  0.2071,  0.0302],\n                        [ 0.2520,  0.1045,  0.1616],\n                        [ 0.0379,  0.1724,  0.1347]],\n              \n                       [[-0.2798, -0.1516,  0.1738],\n                        [-0.1500, -0.2222, -0.1459],\n                        [-0.0503,  0.1140, -0.0101]]],\n              \n              \n                      [[[ 0.0426,  0.2216,  0.2798],\n                        [ 0.1664,  0.4246, -0.1284],\n                        [ 0.1238,  0.1678,  0.1915]],\n              \n                       [[-0.1112,  0.0374,  0.1449],\n                        [-0.1968,  0.3054, -0.1177],\n                        [ 0.0960,  0.1585, -0.0258]],\n              \n                       [[ 0.2635, -0.0590, -0.0924],\n                        [-0.2460, -0.0223,  0.1542],\n                        [-0.2469, -0.1548,  0.4084]],\n              \n                       ...,\n              \n                       [[ 0.1164, -0.3908,  0.2048],\n                        [-0.2205, -0.0854, -0.1132],\n                        [-0.1025, -0.2461,  0.2349]],\n              \n                       [[ 0.1229,  0.0682, -0.1057],\n                        [-0.2343, -0.1524, -0.1229],\n                        [-0.1752, -0.0452,  0.0522]],\n              \n                       [[-0.1858, -0.2270,  0.0016],\n                        [-0.2911, -0.1478, -0.1303],\n                        [ 0.1673, -0.1630,  0.0476]]]], device='cuda:0')),\n             ('layer2.0.bn1.weight',\n              tensor([0.7993, 1.3021, 1.0648, 0.8413, 1.0238, 1.0397, 0.8845, 1.0898, 0.5724,\n                      0.8955, 0.9852, 0.8924, 1.1738, 0.6969, 0.8018, 0.8237, 0.8650, 0.9404,\n                      0.6309, 1.1209, 1.0718, 0.7642, 1.2698, 1.1558, 0.7002, 0.8859, 0.8214,\n                      0.6678, 0.6617, 0.6976, 0.6278, 0.9391, 1.0958, 0.8289, 0.9217, 0.7055,\n                      1.2984, 0.8112, 1.1086, 0.2578, 0.7880, 0.6378, 1.2641, 0.5648, 1.1171,\n                      0.9803, 0.9109, 1.1619, 0.9973, 0.8673, 0.8876, 0.6764, 1.2715, 0.8494,\n                      0.7434, 0.8692, 0.8595, 1.3567, 1.0915, 0.6009, 0.7794, 0.7377, 1.3025,\n                      0.9197, 1.0791, 0.8848, 1.1027, 0.9363, 0.7656, 0.6476, 0.5935, 0.9086,\n                      1.0538, 0.5991, 0.7072, 0.8046, 1.0882, 1.3339, 0.9611, 1.0759, 0.9688,\n                      0.6891, 0.6435, 0.9484, 0.8820, 0.8745, 0.6767, 0.9222, 1.1182, 0.7414,\n                      0.9205, 0.8500, 0.7349, 1.0033, 1.0246, 0.6487, 1.1021, 0.8837, 1.1520,\n                      0.6193, 0.8667, 1.0911, 0.8306, 1.2587, 0.9624, 0.8661, 0.9540, 0.8191,\n                      0.9809, 0.9317, 1.0540, 0.6106, 0.9388, 0.8551, 0.6158, 1.0394, 1.3353,\n                      1.0710, 0.8542, 0.9055, 0.7366, 1.0316, 0.9982, 0.5602, 0.7171, 1.0722,\n                      0.9155, 1.0967], device='cuda:0')),\n             ('layer2.0.bn1.bias',\n              tensor([-0.4009, -0.3385, -0.2006, -0.2562, -0.4609, -0.3427, -0.3654, -0.2360,\n                      -0.3798, -0.5524, -0.3478, -0.4909, -0.2321, -0.2783, -0.3553, -0.4101,\n                      -0.4957, -0.4269, -0.3704, -0.3346, -0.5142, -0.4767, -0.3657, -0.2784,\n                      -0.2684, -0.2696, -0.3709, -0.2510, -0.3078, -0.2871, -0.5311, -0.3732,\n                      -0.4372, -0.3538, -0.4189, -0.4283, -0.5344, -0.2525, -0.5135, -0.3035,\n                      -0.3155, -0.3226, -0.3590, -0.3834, -0.2817, -0.3979, -0.3610, -0.2292,\n                      -0.2434, -0.3031, -0.4377, -0.3113, -0.2935, -0.5091, -0.4440, -0.2757,\n                      -0.3772, -0.3861, -0.4550, -0.3423, -0.2614, -0.4206, -0.0200, -0.4759,\n                      -0.0468, -0.3760, -0.3319, -0.3723, -0.4076, -0.3339, -0.5027, -0.3170,\n                      -0.5558, -0.3589, -0.2700, -0.2321, -0.1971,  0.0202, -0.1832, -0.4445,\n                      -0.2857, -0.2002, -0.4257, -0.3408, -0.2657, -0.3372, -0.5696, -0.3477,\n                      -0.4120, -0.2715, -0.2881, -0.4299, -0.2754, -0.1516, -0.2939, -0.3581,\n                      -0.2269, -0.4468, -0.2976, -0.3408, -0.4004, -0.2618, -0.3658, -0.3512,\n                      -0.3030, -0.3721, -0.5166, -0.4153, -0.2951, -0.4137, -0.5220, -0.5534,\n                      -0.0503, -0.3623, -0.5047, -0.2560, -0.2635, -0.4294, -0.3375, -0.3065,\n                      -0.3576, -0.2603, -0.3683, -0.1873, -0.3776, -0.4340, -0.4586, -0.3822],\n                     device='cuda:0')),\n             ('layer2.0.bn1.running_mean',\n              tensor([ -6.0739,  -6.8563,  -9.6299,  -5.4285,  -4.7087,  -3.4593,  -7.0190,\n                       -8.3050, -15.6750,   5.1807, -10.5067,  -0.6863,  -4.1234, -18.0910,\n                       -5.9505,  -5.0311, -13.1355,   7.5290, -16.6945,  -2.8195,   6.6857,\n                       -6.9237,  -2.1471,   0.8890,  -8.7204,  -7.7679,   5.0695, -11.1418,\n                      -18.7802, -20.7115, -11.2330,  -2.8123,  -7.8735,  -4.9583,   4.1521,\n                        0.1379,   1.9102, -11.0477,  -4.5059,   4.1867,   0.4920,  -6.6543,\n                       -7.9559, -16.8341,  -2.6339,  -1.1893,  -8.0262,  -6.5259, -10.4603,\n                       -9.3296,  -3.5971,   3.5273,  -6.0723,   8.2650,  -8.5869, -11.6947,\n                       -9.3202,  -5.5035,   0.2893,  -6.0568, -18.9657,  -5.5787,  -6.0829,\n                        9.7045,   3.8560,  -1.7655, -17.2526, -11.2580, -15.6901, -11.8909,\n                        9.8573, -11.0565,  -3.8074,   1.3596, -11.0746, -10.8744,  -3.3027,\n                       -8.2901,  -3.8688,  -3.4264,  -1.5679, -15.1935,   8.2364,  -4.4146,\n                      -12.8750,  -9.9226,  -6.0273,  -8.3341,  -6.7995,  -1.1341,   0.3542,\n                       -4.6301,  -7.5391,  -7.0722,  -3.7880, -16.1242,  -0.1943,  -9.4558,\n                       -8.3957,   0.1611,  -9.2021,  -8.9615,  -5.6032,  -3.4912,  -1.1924,\n                       -8.4917,  -8.8500,   7.3171,  -5.1403,  -9.3625,  -5.6551,  -1.4472,\n                       -3.1911,   2.6943,  22.1358,  -8.8407,  -5.9658,  -7.7100,  -5.4464,\n                      -16.3168,   6.5428,  11.0207, -11.6359,  -6.4341,  -9.7188,   5.1869,\n                       -2.1916,   0.2121], device='cuda:0')),\n             ('layer2.0.bn1.running_var',\n              tensor([ 69.2511,  68.4994, 142.4525,  98.5443,  45.0945, 222.9978, 126.8043,\n                       63.8708, 184.1526,  35.5381, 132.5287,  24.9742, 144.9305, 206.7325,\n                       75.8498,  67.2787,  88.7069,  43.9920, 198.9012,  58.0526,  29.8792,\n                       54.0143,  62.3202,  65.9255,  75.8704, 136.1953, 135.1442,  96.3540,\n                      170.5941, 329.0961, 137.0763,  71.5600,  36.8399, 101.5792,  34.8491,\n                       42.9102,  41.7791, 153.0383,  59.9501,  76.3328,  58.1862,  92.2829,\n                       78.4092, 253.9906,  32.0614,  42.0691,  64.0762,  73.0661,  52.1026,\n                      114.6324,  57.5334,  58.9394,  64.7320,  74.6616,  90.2359, 150.2823,\n                      193.9863,  93.4637,  69.9020, 229.8009, 256.4165,  63.5412,  40.1537,\n                       37.9901,  69.4285,  46.8044, 158.3881,  95.4742, 115.4213, 220.3468,\n                       96.8979, 175.7697,  45.0512,  76.4186, 218.0594, 195.4665,  76.2891,\n                       60.9423, 129.1530,  50.9503,  59.0815, 277.7975,  90.4172,  80.4392,\n                      313.0995, 153.3733,  92.0403,  60.8797,  78.1944,  88.7130, 123.5153,\n                       60.1493,  81.9721,  59.7918, 114.9609, 226.2326,  34.6100,  59.2375,\n                       82.7033,  90.6664,  74.1258,  64.7366,  93.1491,  70.2315, 116.7080,\n                       47.8656,  46.6803,  38.2345,  52.4497,  59.7871, 164.9709,  65.7399,\n                       59.5349,  39.1249, 107.8443, 133.5353,  34.5490,  66.7174, 111.0172,\n                      146.9239,  71.3153, 102.5509,  79.8565, 136.8537, 102.2041,  42.5481,\n                      175.8978,  48.8444], device='cuda:0')),\n             ('layer2.0.bn1.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer2.0.conv2.weight',\n              tensor([[[[ 2.1386e-01,  2.7254e-01, -1.8056e-03],\n                        [-5.9566e-02,  8.2635e-02, -1.1027e-01],\n                        [ 6.0358e-02,  3.2336e-03, -6.4902e-02]],\n              \n                       [[ 2.7010e-01, -7.1064e-02, -2.1522e-01],\n                        [ 3.9594e-02, -1.8406e-02,  1.3700e-01],\n                        [-8.8838e-03, -2.8389e-01, -3.4048e-02]],\n              \n                       [[ 4.2557e-02, -1.6154e-01,  1.5505e-01],\n                        [ 7.0640e-02, -2.9445e-02,  2.6322e-02],\n                        [-8.3044e-02,  1.2102e-01, -3.0066e-01]],\n              \n                       ...,\n              \n                       [[ 2.4531e-01, -8.5325e-02,  1.1851e-01],\n                        [ 3.4772e-02, -1.5614e-01, -3.7031e-04],\n                        [-1.4213e-01, -9.1414e-02,  6.0987e-02]],\n              \n                       [[-3.8711e-02, -1.0134e-01, -1.6020e-01],\n                        [-9.0380e-02, -2.8383e-01,  1.5240e-02],\n                        [ 5.5267e-02, -1.4987e-01,  3.6222e-01]],\n              \n                       [[ 4.3182e-01,  1.2568e-01,  6.5092e-01],\n                        [ 1.0748e-01,  2.4135e-01,  3.3441e-01],\n                        [-4.8548e-01, -3.9931e-01,  1.5191e-01]]],\n              \n              \n                      [[[-5.1081e-02, -8.0742e-02,  3.6258e-02],\n                        [ 4.5074e-02,  1.8697e-02, -2.2784e-01],\n                        [ 1.1150e-01, -9.0420e-02, -8.3666e-03]],\n              \n                       [[-1.8092e-01,  1.6882e-01,  3.7480e-01],\n                        [ 5.3074e-01, -1.2419e-01, -9.8892e-02],\n                        [-1.2802e-01, -2.0224e-01, -2.8623e-02]],\n              \n                       [[ 9.3102e-02,  1.4127e-01,  4.0762e-02],\n                        [ 3.6080e-01,  2.6492e-01,  2.8060e-01],\n                        [-1.2135e-02, -3.5161e-02,  6.7354e-03]],\n              \n                       ...,\n              \n                       [[-4.2791e-01, -1.2830e-01, -1.0055e-01],\n                        [ 1.3558e-01,  2.1439e-01, -1.9556e-02],\n                        [-5.8847e-01, -1.9992e-01, -5.0053e-02]],\n              \n                       [[ 5.8031e-02, -2.7954e-01, -3.0574e-01],\n                        [-1.8055e-01, -7.3456e-02, -2.6818e-02],\n                        [ 1.9620e-01, -8.2417e-03, -4.2555e-01]],\n              \n                       [[-3.2389e-01, -1.2680e-01, -3.8378e-02],\n                        [ 5.2537e-02,  7.0288e-02, -2.1124e-01],\n                        [ 3.6513e-04,  2.4242e-01,  8.9224e-02]]],\n              \n              \n                      [[[ 1.6557e-01, -1.7480e-01,  9.7310e-03],\n                        [-3.9049e-01,  8.9672e-02,  2.2143e-01],\n                        [-1.0302e-01, -4.9949e-01, -2.6089e-02]],\n              \n                       [[-5.5471e-02,  7.6776e-02, -9.1193e-02],\n                        [ 1.1825e-01, -2.6243e-02,  4.1889e-01],\n                        [ 1.4766e-02,  1.8068e-02,  4.6085e-01]],\n              \n                       [[-4.6206e-02,  4.6933e-02, -3.4441e-02],\n                        [ 1.5081e-03, -1.0009e-01, -5.7616e-02],\n                        [-1.9614e-02, -1.3177e-01, -3.5740e-02]],\n              \n                       ...,\n              \n                       [[ 5.3298e-02, -1.9435e-01, -1.0609e-01],\n                        [ 8.2773e-02,  4.6847e-02,  9.9420e-02],\n                        [-4.1579e-01, -4.7652e-01, -2.7824e-01]],\n              \n                       [[-1.9582e-02,  2.8972e-01, -3.0036e-01],\n                        [ 1.4180e-02, -1.0008e-01,  2.6054e-01],\n                        [ 3.0916e-01,  1.3332e-01,  3.3777e-01]],\n              \n                       [[-4.2419e-02,  6.5168e-02,  1.4430e-01],\n                        [ 5.7708e-02,  1.9048e-03,  1.5284e-01],\n                        [-5.7793e-02,  2.2772e-01, -5.3325e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 6.8583e-01,  1.9058e-02,  2.4300e-01],\n                        [ 2.4520e-01, -5.6070e-02, -3.3160e-01],\n                        [-8.3727e-02,  1.0392e-01, -2.9110e-03]],\n              \n                       [[-4.6367e-01, -1.5920e-01, -3.8551e-01],\n                        [ 4.2405e-01, -9.2350e-02,  1.2697e-01],\n                        [ 8.8892e-02,  6.4613e-02, -1.6301e-01]],\n              \n                       [[-2.4694e-01,  5.3298e-02, -2.2563e-01],\n                        [-9.6252e-02,  1.2978e-01,  2.7781e-01],\n                        [ 9.3867e-02, -3.4030e-02, -1.0038e-01]],\n              \n                       ...,\n              \n                       [[-1.6158e-01, -6.2058e-02,  1.2936e-01],\n                        [ 7.6599e-03, -1.3494e-01, -6.3050e-02],\n                        [-5.5646e-02, -3.0905e-01, -1.0889e-01]],\n              \n                       [[ 4.3744e-01,  3.0641e-01,  1.1344e-01],\n                        [-9.0718e-02, -1.1799e-01, -1.4865e-01],\n                        [-3.6311e-01, -1.3862e-01, -2.9342e-01]],\n              \n                       [[-1.6073e-01,  2.7299e-01,  1.1164e-01],\n                        [ 8.8954e-03, -2.3799e-01,  1.8626e-01],\n                        [-1.0754e-01,  1.7235e-01,  3.4673e-02]]],\n              \n              \n                      [[[ 3.4602e-01,  1.2404e-01,  3.3393e-01],\n                        [-5.1133e-01, -1.8736e-01, -1.7379e-01],\n                        [ 1.6503e-01, -6.6200e-03,  2.1548e-01]],\n              \n                       [[-5.2730e-01,  8.7991e-02, -1.3443e-01],\n                        [ 2.5513e-01, -3.7843e-02,  1.1624e-01],\n                        [-6.4410e-02, -1.2641e-01, -1.5160e-01]],\n              \n                       [[-3.3389e-01,  1.8653e-01, -7.2257e-02],\n                        [-1.4996e-01,  6.2924e-02,  1.9053e-01],\n                        [ 9.4775e-02, -1.1099e-01, -1.4504e-01]],\n              \n                       ...,\n              \n                       [[-4.8766e-02,  1.3615e-01, -3.6568e-01],\n                        [-1.0078e-01, -2.8557e-02, -1.9326e-02],\n                        [-5.4821e-01, -5.4365e-01, -3.5546e-01]],\n              \n                       [[ 1.1971e-01,  2.7186e-01,  4.1715e-01],\n                        [ 9.7511e-02, -3.1840e-01, -3.8266e-01],\n                        [-9.5645e-02,  4.1121e-02, -2.4429e-02]],\n              \n                       [[-1.1845e-01,  3.7671e-02,  2.4903e-01],\n                        [-1.2938e-01, -2.5426e-01,  8.2049e-02],\n                        [ 5.0435e-02, -9.1141e-02, -1.1098e-01]]],\n              \n              \n                      [[[-7.5211e-02, -2.2109e-01, -3.0475e-01],\n                        [ 1.0445e-01,  1.5397e-01, -2.7997e-01],\n                        [ 8.7680e-02, -9.3379e-02, -9.5936e-02]],\n              \n                       [[ 1.2347e-01, -1.3883e-01,  2.0391e-01],\n                        [ 1.6426e-01, -5.4439e-02, -1.4489e-02],\n                        [-2.8806e-01, -1.2204e-01, -9.3472e-02]],\n              \n                       [[ 1.4175e-01, -6.6542e-02,  7.7992e-02],\n                        [ 2.2251e-01,  9.8464e-02,  3.0564e-01],\n                        [ 6.9356e-02,  1.3542e-01,  7.2386e-02]],\n              \n                       ...,\n              \n                       [[-3.6249e-01,  6.5925e-02,  2.3216e-01],\n                        [ 9.7559e-02, -6.3841e-03,  1.3929e-01],\n                        [-1.2650e-01,  1.6262e-01,  2.5842e-01]],\n              \n                       [[ 8.0666e-03, -8.6067e-02, -1.5839e-01],\n                        [-4.6964e-03, -9.8562e-02, -4.3669e-02],\n                        [ 1.0120e-01,  9.9962e-02, -2.7921e-01]],\n              \n                       [[ 1.2343e-01, -3.1138e-01, -3.1611e-01],\n                        [-9.9413e-02,  1.1429e-01, -1.0508e-01],\n                        [-1.5569e-01,  1.3722e-01,  2.8832e-01]]]], device='cuda:0')),\n             ('layer2.0.bn2.weight',\n              tensor([0.9440, 1.0770, 0.8930, 0.8403, 1.0286, 0.7578, 0.8158, 1.0612, 1.5494,\n                      0.8640, 0.9440, 1.1594, 0.6851, 1.3522, 1.1964, 0.8008, 0.5705, 1.1938,\n                      0.9989, 0.7985, 1.0763, 1.2185, 1.0700, 1.0141, 0.8756, 0.7696, 0.8614,\n                      1.2587, 0.8151, 0.8561, 0.9619, 0.9222, 0.6562, 0.6270, 0.8247, 1.0492,\n                      1.1872, 1.3598, 0.7856, 1.0200, 1.4790, 1.2233, 0.9616, 1.0889, 0.8583,\n                      0.9035, 1.0658, 0.6072, 0.8601, 1.0258, 1.0654, 0.6773, 1.0592, 0.5926,\n                      0.7379, 1.1466, 0.8891, 1.2943, 0.7828, 1.0963, 0.7827, 1.0305, 1.5502,\n                      1.0031, 1.0192, 0.9398, 1.3684, 1.1442, 1.0669, 0.5017, 0.9856, 0.9914,\n                      1.0804, 0.9399, 0.9339, 1.0967, 1.2594, 1.0710, 0.9678, 0.8479, 0.7576,\n                      1.0575, 0.9566, 1.3171, 0.7922, 0.8576, 0.8197, 0.9440, 1.2730, 1.1365,\n                      0.8805, 1.3309, 1.1875, 0.9842, 0.7703, 0.9288, 0.7874, 1.3800, 0.8526,\n                      1.1562, 1.1524, 0.4599, 0.8154, 1.1336, 0.9827, 1.1258, 1.0036, 1.1420,\n                      1.1504, 0.7285, 1.0356, 1.0896, 0.9556, 0.7444, 0.7408, 0.9125, 0.8336,\n                      1.0463, 1.5563, 1.1218, 0.9116, 1.2425, 0.6101, 1.0039, 1.2808, 1.0371,\n                      0.9635, 1.1532], device='cuda:0')),\n             ('layer2.0.bn2.bias',\n              tensor([-0.2339, -0.2094, -0.3815, -0.1874, -0.2868, -0.1927, -0.0253, -0.3702,\n                      -0.5284,  0.0418, -0.1311, -0.1052, -0.4340, -0.3318, -0.2440, -0.1768,\n                      -0.2524, -0.1374, -0.0902, -0.1147, -0.0775, -0.4016, -0.3018, -0.1562,\n                      -0.1041, -0.2133, -0.1233, -0.3602,  0.0192, -0.4430, -0.1703,  0.1092,\n                      -0.3613, -0.1243, -0.2960, -0.0294, -0.3702, -0.0668, -0.2700, -0.0519,\n                      -0.3283,  0.0816, -0.2697, -0.0352, -0.2523,  0.0511, -0.1655, -0.1442,\n                      -0.2936, -0.3292, -0.0689, -0.3132, -0.0697, -0.1826, -0.2233, -0.2166,\n                      -0.2958, -0.0928, -0.3085, -0.0651, -0.3593, -0.0317, -0.3622, -0.1762,\n                       0.1782, -0.1859, -0.1988,  0.0482, -0.2351, -0.2108, -0.1019, -0.0997,\n                      -0.2085, -0.3903, -0.0748, -0.2283, -0.2263, -0.0864, -0.1156, -0.1803,\n                      -0.1550, -0.1897, -0.1944, -0.1470, -0.2721, -0.0283, -0.0138, -0.3738,\n                      -0.2868, -0.3596, -0.0773, -0.2948, -0.1887, -0.5778, -0.2414,  0.0389,\n                      -0.1090, -0.1522, -0.1252, -0.0190, -0.2215, -0.2365, -0.3247, -0.1299,\n                      -0.1108, -0.3038, -0.0883, -0.2468,  0.0467, -0.0953, -0.3670, -0.2969,\n                      -0.1369, -0.1184, -0.1923, -0.2759, -0.3361, -0.2045, -0.3099, -0.2977,\n                      -0.1238, -0.0744, -0.2073, -0.2416, -0.2388,  0.0280, -0.2217, -0.3200],\n                     device='cuda:0')),\n             ('layer2.0.bn2.running_mean',\n              tensor([-0.3208, -1.2628,  2.0084, -2.5070,  2.0422, -1.7824, -2.6827,  4.2551,\n                       2.0051, -4.2272,  0.5011,  0.2033, -5.9999, -2.0692,  2.0471, -3.6791,\n                      -4.8348, -0.1958, -6.4881, -1.2327,  1.0314, -1.1011,  0.1373, -4.8387,\n                      -3.4238, -0.6179, -3.3765, -1.1575, -2.8405, -0.1096, -0.0554, -7.1381,\n                      -1.2926, -2.5290, -0.7705, -4.7021, -0.1887, -2.2891, -2.1481, -0.2473,\n                      -1.7218, -1.8280,  3.9496, -4.6721,  1.7969, -3.4996,  0.6586, -3.4177,\n                      -3.6632,  2.9012, -2.6062,  0.5097, -0.1238, -5.9928, -2.5728, -2.6596,\n                       0.5289, -1.7233, -0.0503,  2.4341,  0.3727,  0.8128,  1.0606,  0.4864,\n                      -3.8728, -0.7982,  2.5344, -0.4270,  3.2937, -4.0386, -5.2342, -0.1649,\n                       1.0207,  0.0971,  0.5706,  4.5906, -6.1025, -1.7906, -1.1817, -7.6655,\n                      -4.1103, -1.5554,  1.3027, -5.5679, -2.1149,  0.6177, -6.4166,  4.3841,\n                       0.8068,  0.1051, -4.1841, -1.7519, -0.6621, -2.2002,  0.9783, -4.4824,\n                      -1.8085, -2.9983, -3.6899, -2.1195,  1.4808, -0.9074, -2.1052, -1.9349,\n                      -0.7541, -5.5402, -2.4747, -0.8293, -3.4873, -2.3334, -1.8241, -0.8393,\n                      -1.4122, -6.0595, -2.1811, -1.4748, -1.1661, -4.7579, -1.7276,  1.1304,\n                      -2.9548, -3.0209, -5.1067,  4.3523, -3.4546, -2.4431, -2.4429, -0.0588],\n                     device='cuda:0')),\n             ('layer2.0.bn2.running_var',\n              tensor([ 23.2796,  35.0989,  45.4819,  36.6588,  11.0569,  41.0438,  50.5341,\n                       10.4428,  34.0872,  42.3671,  17.7926,  24.2943,  51.4951,  14.9756,\n                       24.5273,  46.9101,  50.3842,  31.7454,  44.2709,  48.5709,  11.6534,\n                       39.0069,   9.1901,  49.9651,  34.0288,  26.0890,  23.4039,  48.5210,\n                       42.2278,   8.7214,  13.6354, 111.8364,  78.0414,  82.4819,  17.8125,\n                       34.8740,  10.6292,   8.7123,  42.2183,  22.5465,   8.2468,  33.7656,\n                       55.0963,  31.3935,  43.5109,  37.9579,  39.0303,  74.0462,   9.1807,\n                       25.6226,  33.1999,  46.8979,  20.5633, 150.2822,  37.5545,  16.9764,\n                       14.6483,  12.7195,  15.7029,  22.6961,  39.7215,  16.6261,  29.2797,\n                       15.8115,  34.2836,  13.6479,  24.9059,  13.0683,  17.5304,  93.4931,\n                       57.1395,  16.4740,  24.0911,  21.6348,  48.3302,  43.6098,  24.7008,\n                       31.5946,  14.0963,  98.7630,  71.9122,  15.3386,  19.2893,  37.8871,\n                       20.9368,  28.8403,  40.7721,  33.4226,   9.5309,  18.5796,  31.7875,\n                       16.5389,  19.1103,  38.8161,  24.7111,  31.1301,  44.1602,  10.2164,\n                       14.8160,  22.1890,  16.2912,  82.4583,  71.3501,  29.0361,  22.6724,\n                       21.8921,  42.9965,  12.3764,  20.8894,  43.4117,   9.7809,  23.8602,\n                       21.3143,  41.7745,  22.4696,  30.4031,  10.7356,  46.2189,  14.8790,\n                       31.7505,  39.4713,  21.8902,  55.3889,  32.2931,  33.9133,  38.5607,\n                       11.7014,  18.8107], device='cuda:0')),\n             ('layer2.0.bn2.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer2.1.conv1.weight',\n              tensor([[[[-7.8208e-02, -1.8625e-01,  3.2914e-02],\n                        [-1.3525e-01, -2.6515e-01, -8.2033e-02],\n                        [ 1.3860e-01,  1.3482e-02, -5.9482e-02]],\n              \n                       [[ 6.3812e-02,  2.8136e-01, -2.6711e-01],\n                        [-6.0573e-02, -3.1450e-02,  4.6166e-02],\n                        [ 1.0269e-01, -1.2564e-01, -2.9657e-03]],\n              \n                       [[-1.6989e-02, -1.0881e-01, -5.0463e-01],\n                        [ 1.3064e-01,  1.2452e-01, -5.4348e-01],\n                        [-4.5886e-02,  6.5840e-02,  1.3352e-01]],\n              \n                       ...,\n              \n                       [[ 7.2058e-02, -1.3810e-01, -5.5527e-01],\n                        [ 1.4240e-01,  4.9476e-02, -2.9642e-01],\n                        [ 6.6834e-02,  4.2881e-02,  1.1508e-01]],\n              \n                       [[ 1.7265e-02,  1.2823e-01, -4.6380e-01],\n                        [ 1.7247e-01,  3.1427e-02, -3.5403e-01],\n                        [ 2.0749e-01,  1.7873e-01,  3.2029e-02]],\n              \n                       [[-1.3785e-01,  1.2208e-01, -1.8535e-01],\n                        [-4.3852e-02, -1.8566e-01,  3.8494e-02],\n                        [-1.6256e-01,  1.7022e-02, -4.5646e-02]]],\n              \n              \n                      [[[ 1.2536e-01, -1.5934e-01, -1.9815e-01],\n                        [ 2.8466e-02, -1.8240e-01, -2.6546e-01],\n                        [-1.3519e-01,  1.7548e-01, -1.7674e-01]],\n              \n                       [[ 1.0931e-02,  4.3287e-01, -9.9778e-02],\n                        [-2.4908e-01, -8.1397e-03, -3.4602e-01],\n                        [ 3.0688e-02,  1.5261e-01,  1.6994e-01]],\n              \n                       [[ 3.5238e-04,  1.5047e-01,  1.7437e-01],\n                        [ 2.3819e-01, -9.8043e-02,  2.5198e-01],\n                        [-3.6592e-02,  1.1103e-01,  1.1989e-01]],\n              \n                       ...,\n              \n                       [[ 2.2710e-01,  2.6513e-01, -1.7349e-01],\n                        [-1.3768e-01, -2.6024e-03, -5.1335e-01],\n                        [-7.1860e-02, -2.1642e-01, -4.2310e-01]],\n              \n                       [[ 2.2948e-01,  1.5673e-01, -1.3657e-01],\n                        [ 1.1012e-01, -3.5796e-02, -7.1421e-02],\n                        [-5.7206e-02, -6.9541e-02, -2.5398e-01]],\n              \n                       [[-1.1792e-02,  3.0999e-01,  1.4707e-01],\n                        [-6.1676e-02, -9.7832e-02,  6.3348e-02],\n                        [-5.2319e-02,  4.3278e-02,  1.8797e-01]]],\n              \n              \n                      [[[ 7.2279e-02, -1.5730e-01,  1.6834e-01],\n                        [ 4.4083e-03, -3.3374e-02, -5.9811e-02],\n                        [ 1.2869e-01, -1.8793e-01,  1.3630e-01]],\n              \n                       [[-3.5998e-01,  2.2384e-01, -2.3200e-01],\n                        [ 2.0527e-01, -1.9886e-01, -2.8718e-02],\n                        [ 3.1867e-01,  2.5917e-01,  1.5499e-01]],\n              \n                       [[ 4.9203e-02,  1.7887e-01,  1.9907e-02],\n                        [-2.2963e-01,  6.0812e-02, -3.3204e-01],\n                        [-1.0039e-01, -2.1488e-01, -7.7911e-02]],\n              \n                       ...,\n              \n                       [[-2.8986e-01,  2.5650e-01,  8.1563e-02],\n                        [-3.3076e-02, -9.2803e-02, -2.8180e-01],\n                        [ 2.3930e-01,  8.3887e-02, -3.1367e-01]],\n              \n                       [[-3.2743e-02,  1.4361e-02, -8.8837e-03],\n                        [ 2.5499e-01, -7.4422e-02,  1.3371e-01],\n                        [ 6.8948e-02, -2.2721e-02, -2.8406e-01]],\n              \n                       [[ 1.6664e-01, -1.4389e-01, -5.3381e-01],\n                        [ 1.6082e-01, -1.3852e-01,  4.9269e-02],\n                        [ 5.0729e-02, -7.5627e-02, -5.6576e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 8.3327e-02, -8.5748e-02, -5.7972e-02],\n                        [ 2.1405e-02, -1.4252e-01,  4.2936e-02],\n                        [-1.7278e-01,  1.2917e-01, -3.6828e-01]],\n              \n                       [[-1.6319e-01,  3.0986e-01,  1.0831e-01],\n                        [ 2.6115e-02, -1.6680e-01, -7.4707e-02],\n                        [-2.9968e-02, -1.2394e-01,  7.2713e-02]],\n              \n                       [[ 9.5147e-02, -9.7597e-02, -3.6882e-02],\n                        [-6.0330e-02, -1.2353e-02,  1.4455e-01],\n                        [ 1.8383e-01, -3.6034e-02,  1.3585e-02]],\n              \n                       ...,\n              \n                       [[ 1.1245e-01, -2.9956e-01, -2.1397e-01],\n                        [-2.9422e-01, -4.4178e-02, -2.5730e-01],\n                        [ 1.0396e-01, -1.3154e-01,  2.7174e-02]],\n              \n                       [[-1.2701e-01, -2.4233e-01, -2.0841e-01],\n                        [ 8.4516e-02, -1.7147e-01, -1.8237e-01],\n                        [-5.3439e-02,  7.6541e-02,  2.9494e-02]],\n              \n                       [[-5.3353e-02,  4.1048e-01,  3.9708e-02],\n                        [-2.1352e-01, -3.6190e-01, -3.0220e-02],\n                        [-2.0471e-01, -3.5008e-01,  3.0040e-01]]],\n              \n              \n                      [[[-8.4724e-02, -1.8252e-01,  8.9525e-02],\n                        [ 5.5794e-02,  1.0357e-01, -3.1171e-01],\n                        [-2.2523e-01, -1.7964e-02, -8.8612e-02]],\n              \n                       [[-1.5489e-01, -8.9367e-02, -4.8637e-03],\n                        [-7.4291e-02, -3.4869e-01, -8.2073e-02],\n                        [ 7.7657e-02,  1.3286e-01,  6.1141e-02]],\n              \n                       [[ 9.4104e-02, -4.3950e-01,  7.6410e-02],\n                        [-4.4537e-01,  1.1344e-02,  5.6870e-02],\n                        [-1.7776e-01, -5.1366e-01,  2.4402e-02]],\n              \n                       ...,\n              \n                       [[-6.1821e-03,  1.1635e-01, -2.1084e-02],\n                        [-2.0179e-01,  5.5212e-02, -2.1521e-01],\n                        [-2.8102e-01, -4.1439e-01, -2.2884e-01]],\n              \n                       [[ 2.8137e-02, -4.0090e-02, -1.9425e-01],\n                        [-1.9266e-01,  4.4478e-02, -1.0098e-01],\n                        [-2.2816e-01, -3.7450e-01, -1.8972e-01]],\n              \n                       [[-9.7822e-05, -1.7642e-01,  7.5630e-03],\n                        [-2.1613e-01, -5.2488e-01, -1.1954e-01],\n                        [ 8.4974e-03,  5.3865e-02,  1.9957e-01]]],\n              \n              \n                      [[[ 1.4705e-01,  1.1395e-01, -1.8097e-01],\n                        [-1.4360e-02,  2.6631e-02, -4.7317e-02],\n                        [-5.7281e-02,  1.3846e-02,  4.9053e-02]],\n              \n                       [[ 1.3635e-01,  2.1442e-01, -1.4026e-01],\n                        [-4.0234e-02,  1.5049e-02, -4.0509e-01],\n                        [-5.2095e-01, -3.1249e-01, -4.4790e-01]],\n              \n                       [[-1.0810e-01,  6.6973e-02, -7.8880e-02],\n                        [-1.9560e-01,  2.8786e-01,  3.3196e-01],\n                        [ 1.3279e-02, -8.6145e-02, -1.3535e-01]],\n              \n                       ...,\n              \n                       [[ 1.5191e-01, -2.5470e-02, -1.9227e-01],\n                        [-2.3232e-02,  2.7612e-02, -1.1956e-01],\n                        [ 1.0646e-03, -2.5682e-01, -7.8399e-02]],\n              \n                       [[ 2.0383e-01,  4.8450e-02,  1.8022e-01],\n                        [-6.0350e-02, -8.7006e-02, -2.6522e-01],\n                        [ 2.0859e-01,  7.8849e-02,  1.5802e-01]],\n              \n                       [[ 3.0324e-01, -1.4633e-01, -2.1501e-01],\n                        [ 3.7183e-02, -5.8576e-02, -2.2293e-01],\n                        [-2.7216e-02, -4.3925e-01, -6.4437e-01]]]], device='cuda:0')),\n             ('layer2.1.bn1.weight',\n              tensor([0.4146, 1.2742, 0.5087, 0.8716, 0.7059, 1.0931, 0.5255, 0.7326, 0.6427,\n                      0.9038, 1.0745, 1.1245, 0.9477, 0.6689, 0.8205, 1.1084, 1.1709, 1.0616,\n                      0.9278, 0.8402, 0.5227, 0.9891, 1.0842, 0.8559, 0.6101, 0.7136, 0.6556,\n                      1.1901, 0.8033, 0.7310, 0.7303, 0.8064, 0.7169, 0.7331, 1.7166, 0.3504,\n                      1.2232, 0.7946, 0.8871, 1.0588, 0.4917, 1.2324, 0.7468, 0.6230, 0.4833,\n                      0.6657, 1.3814, 0.5371, 0.3549, 0.9249, 1.2891, 0.8884, 0.6813, 0.5801,\n                      0.6194, 1.1111, 1.0573, 0.8074, 0.7905, 1.2663, 0.7443, 0.6315, 0.4775,\n                      0.9302, 1.1767, 1.1063, 1.1329, 0.7962, 1.0494, 0.4763, 0.6222, 0.8774,\n                      1.3194, 1.0089, 1.3146, 1.0657, 0.4179, 1.0323, 1.0352, 0.9512, 1.3538,\n                      1.1647, 0.7052, 1.1503, 0.4861, 0.9685, 0.8981, 0.9117, 0.4257, 1.2595,\n                      0.6822, 0.8028, 0.7764, 0.8163, 0.5710, 1.1857, 0.7477, 0.7442, 1.0833,\n                      0.5275, 0.7747, 0.8866, 0.8028, 0.6889, 1.2829, 1.0762, 0.6467, 0.6422,\n                      1.1046, 0.6226, 0.9927, 1.1614, 1.0853, 1.0927, 0.9691, 1.1439, 0.6306,\n                      0.5334, 1.0399, 1.2289, 0.4738, 1.2456, 0.7143, 1.1070, 0.7034, 0.8742,\n                      0.3986, 0.8804], device='cuda:0')),\n             ('layer2.1.bn1.bias',\n              tensor([-0.2488, -0.3272, -0.4082, -0.3293, -0.2378, -0.0446, -0.3557, -0.3405,\n                      -0.2945, -0.3818, -0.3111, -0.5471, -0.1202, -0.3709, -0.1838, -0.2785,\n                      -0.3288, -0.6489, -0.5542, -0.4803, -0.2297, -0.1101, -0.1880, -0.1828,\n                      -0.3947, -0.3051, -0.2376, -0.3774, -0.2769, -0.1753, -0.4498, -0.1731,\n                      -0.2178, -0.5086, -0.5313, -0.5640, -0.3692, -0.2364, -0.3153, -0.3750,\n                      -0.3788, -0.3331, -0.4096, -0.1709, -0.2602, -0.2729, -0.3599, -0.2606,\n                      -0.4745, -0.2544, -0.3513, -0.3167, -0.6162, -0.3265, -0.3316, -0.2007,\n                      -0.2736, -0.4973, -0.2363, -0.4355, -0.5283, -0.3136, -0.2210, -0.4583,\n                      -0.3839, -0.2016, -0.2321, -0.1445, -0.2599, -0.2697, -0.2596, -0.1969,\n                      -0.4918, -0.3120, -0.4232, -0.2499, -0.2134, -0.4812, -0.4144, -0.3724,\n                      -0.1713, -0.1878, -0.2133, -0.3260, -0.3624, -0.3679, -0.3318, -0.3966,\n                      -0.3207, -0.1209, -0.3224, -0.3858, -0.2595, -0.4365, -0.1890, -0.5937,\n                      -0.6618, -0.4473, -0.2107, -0.3012, -0.2828, -0.3557, -0.2093, -0.2573,\n                      -0.3502, -0.2321, -0.1828, -0.2181, -0.2688, -0.3217, -0.1827, -0.3730,\n                      -0.3893, -0.1156, -0.2665, -0.7799, -0.2272, -0.2935, -0.2929, -0.5508,\n                      -0.3922, -0.3056, -0.3733, -0.3497, -0.3231, -0.2838, -0.3807, -0.2038],\n                     device='cuda:0')),\n             ('layer2.1.bn1.running_mean',\n              tensor([-14.3836, -11.6456,  -9.2953,   1.1938, -10.6726, -12.4827,  -5.7696,\n                       -8.9497,   0.7805,  -5.3122,  -0.8957,  -1.5754,  -7.9826,   8.0242,\n                       -9.6724, -10.2794,  -2.0281,   1.3060,  -6.3117,  -7.0064,  -8.3112,\n                      -17.8224,  -6.2099,  -9.9992,  -4.3972,  -9.1559, -17.1181,  -8.0325,\n                      -14.8899,  -2.8495, -11.4189,  -5.6989,  -7.4819,  -3.5647,   4.9340,\n                       -5.5021,  -5.3394,   0.4402,  -9.4267,  -7.0111,  -0.3603,  -6.3656,\n                       -9.4709,  -5.3328, -20.8646, -17.8242,  -0.4742,  -6.1632, -12.3002,\n                      -12.1556,   0.8412, -14.5655, -12.5296, -19.9214, -13.9986,  -6.5008,\n                        4.9270,   0.5987,  -3.8049,   5.1078,   4.9003,  -2.0009, -21.1943,\n                       -6.0097,  -0.3200,  -9.7110,  -5.5717,  -3.6370,  -6.9104, -12.4258,\n                      -26.0003,  -5.9684,  -4.0464,  -2.6407,  -4.7339,  -0.4933, -15.1226,\n                        2.0073,  -3.6381,   1.7394,  -3.9396,  -6.9946,  -8.5696,  -0.4968,\n                      -11.8686,  -0.3979,  -4.7481,  -3.5095, -18.9721,  -0.5915,  -8.9764,\n                      -20.4054,  -2.7003, -15.2189, -18.2467,  -1.4299,  -1.4653,  -6.4565,\n                        2.7101, -17.9322,   0.0904,  -2.4996,  -9.5779,  -3.1785,   3.9486,\n                       -2.2517,  -5.8645, -13.0619,  -6.5204, -15.3852,  -3.7009,  -0.1912,\n                       -4.8299,  -2.1831, -13.6558,   3.7362, -11.1896, -12.7100,   0.3727,\n                        0.7979, -13.6090,  -3.8910, -17.0599,  -9.2577,   3.5654, -10.8128,\n                      -29.9787,  -8.6916], device='cuda:0')),\n             ('layer2.1.bn1.running_var',\n              tensor([ 983.9244,  200.3429,  281.1678,  263.3916,  307.8321,  365.1101,\n                       420.9926,  335.2718,  379.8621,  248.4878,  225.4132,  110.0921,\n                       151.8623,  264.2898,  303.3646,  303.2932,  202.9285,  125.0129,\n                       210.9547,  236.3255,  454.5840,  247.1264,  180.1458,  444.4251,\n                       244.0208,  399.2625,  614.4171,  197.9382,  320.9742,  364.6351,\n                       261.2361,  461.4767,  266.0939,  255.8959,  129.0598,  609.2875,\n                        92.1693,  363.1537,  280.0829,  274.9407,  368.2556,  211.9740,\n                       245.9897,  379.0521,  684.2632,  416.8547,  146.4241,  770.2861,\n                       722.8925,  380.6928,  222.3750,  285.0599,  172.8560,  648.3199,\n                       569.4247,  197.4831,  157.0859,  208.2131,  292.2129,  176.0398,\n                       148.1302,  313.5922,  947.4924,  222.5117,  159.5147,  224.8348,\n                       288.3503,  327.3854,  214.0894,  572.7854,  793.8167,  260.6215,\n                       134.4552,  284.2643,  152.0300,  178.1254, 1056.5426,  127.5953,\n                       169.6847,  249.8654,  138.7749,  158.8708,  412.1458,  229.7410,\n                       374.1438,  268.6976,  207.8425,  179.9905,  802.9720,  240.6949,\n                       352.9196,  341.4654,  292.4533,  303.2235,  916.6530,  156.1921,\n                       164.5620,  248.2305,  256.7422,  584.5981,  317.0032,  270.7318,\n                       284.9406,  427.6448,  159.7553,  244.9218,  494.0580,  415.5669,\n                       196.7448,  456.0169,  267.2830,  191.9802,  248.5074,  296.2554,\n                       200.9459,  151.5449,  450.8504,  627.9692,  272.3372,  204.0738,\n                       838.1648,  116.5641,  316.9037,  228.5338,  346.4802,  350.7924,\n                      1029.9320,  255.9788], device='cuda:0')),\n             ('layer2.1.bn1.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer2.1.conv2.weight',\n              tensor([[[[-1.0291e-01,  6.8479e-03, -9.7015e-02],\n                        [-3.8464e-01, -4.0816e-01, -2.6935e-01],\n                        [-7.6801e-02, -3.1598e-01,  2.8497e-02]],\n              \n                       [[-3.8180e-02,  1.6165e-01, -1.4765e-01],\n                        [-2.7120e-01,  5.5123e-02, -2.2352e-02],\n                        [-4.9140e-01, -1.5060e-01, -1.0942e-01]],\n              \n                       [[ 1.7766e-01,  5.4987e-03,  4.0232e-02],\n                        [-1.1114e-01, -6.0935e-02, -4.8094e-02],\n                        [-1.4926e-01, -5.1758e-01, -1.2511e-01]],\n              \n                       ...,\n              \n                       [[-2.8435e-01,  2.2950e-01, -3.8562e-01],\n                        [-1.7907e-01, -2.6520e-01, -1.0517e-01],\n                        [ 3.0977e-02,  2.7453e-01,  6.2829e-02]],\n              \n                       [[ 2.3334e-02, -3.8585e-02,  9.6516e-02],\n                        [ 3.4870e-02, -8.1886e-02, -1.3979e-01],\n                        [-5.3701e-02, -3.5517e-02, -1.6380e-01]],\n              \n                       [[ 5.0994e-02, -1.0197e-02,  1.4596e-01],\n                        [-2.4906e-01, -5.1301e-01,  6.1827e-02],\n                        [-1.9847e-01, -5.2430e-01, -2.4583e-02]]],\n              \n              \n                      [[[ 1.4240e-02,  2.3884e-02, -1.6892e-01],\n                        [ 8.6716e-02, -1.0121e-01, -2.8362e-01],\n                        [ 9.0486e-02, -1.6618e-02, -7.2095e-02]],\n              \n                       [[-3.3697e-03,  2.0280e-01,  2.7159e-01],\n                        [ 3.2743e-01,  8.2155e-02,  5.3155e-02],\n                        [ 7.3284e-02,  1.7556e-01, -6.0060e-02]],\n              \n                       [[ 2.5973e-01,  1.8188e-01, -1.8065e-01],\n                        [ 1.1993e-01,  3.0819e-03, -3.8932e-02],\n                        [ 9.7383e-02, -5.0811e-02, -2.0533e-01]],\n              \n                       ...,\n              \n                       [[ 7.8809e-02, -1.6553e-01, -1.4888e-01],\n                        [ 3.9009e-01, -2.4455e-05,  1.7882e-03],\n                        [ 2.6839e-01,  1.5200e-02,  4.0132e-03]],\n              \n                       [[-5.2362e-02, -1.2987e-01,  1.1995e-01],\n                        [ 6.7235e-02,  1.1209e-01, -9.4726e-03],\n                        [-2.0284e-01,  3.7748e-02,  1.2031e-01]],\n              \n                       [[ 2.4330e-01,  2.0340e-02,  4.8471e-02],\n                        [-1.3405e-01, -4.6404e-01, -4.6201e-01],\n                        [ 2.9996e-02, -8.4805e-02, -3.8335e-01]]],\n              \n              \n                      [[[ 3.8177e-01,  6.1707e-03, -8.3015e-03],\n                        [-1.2947e-02, -5.9768e-02,  1.8003e-01],\n                        [ 1.2975e-01,  7.9764e-03,  6.6076e-02]],\n              \n                       [[-9.3502e-02, -1.6729e-01,  3.8358e-01],\n                        [ 2.1522e-01,  2.6362e-01,  3.3378e-01],\n                        [ 1.7864e-01,  1.0339e-01,  1.7892e-01]],\n              \n                       [[ 1.4426e-01,  2.0017e-01,  2.6817e-01],\n                        [ 2.4267e-01, -3.9436e-02,  2.8136e-01],\n                        [ 5.7175e-02, -5.5118e-02,  2.6796e-01]],\n              \n                       ...,\n              \n                       [[ 3.3831e-02,  2.6376e-01,  1.1605e-02],\n                        [ 1.3386e-01,  3.6043e-01, -7.1870e-02],\n                        [-6.9402e-02,  2.0720e-01, -1.6025e-02]],\n              \n                       [[ 6.9731e-02,  7.3225e-02,  1.7654e-01],\n                        [-3.2952e-02, -1.4090e-02,  1.1256e-02],\n                        [-1.1263e-01,  1.0303e-01, -2.2919e-02]],\n              \n                       [[-2.0190e-01,  1.4776e-01,  2.7573e-01],\n                        [ 8.4718e-02,  2.6288e-02,  3.1407e-01],\n                        [-7.7263e-02,  7.0200e-02,  1.7058e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 7.3723e-02,  3.3382e-01,  1.1939e-01],\n                        [ 3.3015e-01,  2.3973e-01,  1.4068e-01],\n                        [ 1.0125e-01,  3.4645e-01,  1.8289e-01]],\n              \n                       [[ 4.0058e-03,  4.6009e-01, -3.4647e-01],\n                        [ 1.7505e-01, -4.4541e-02,  2.8029e-01],\n                        [ 3.6101e-01,  1.7406e-01,  1.0504e-01]],\n              \n                       [[ 2.0504e-01,  3.5622e-01, -6.8100e-02],\n                        [ 3.4327e-01,  1.9159e-02,  1.6624e-01],\n                        [ 3.0849e-01,  1.1777e-01,  1.5655e-03]],\n              \n                       ...,\n              \n                       [[ 6.0063e-02,  2.7375e-01, -1.3361e-01],\n                        [ 2.5517e-01, -1.7673e-02,  3.3407e-01],\n                        [ 4.5133e-01,  7.5551e-02,  2.8579e-02]],\n              \n                       [[ 1.9831e-01, -2.2442e-02, -7.2815e-02],\n                        [-2.8918e-02, -1.6550e-03,  8.3001e-02],\n                        [ 6.7677e-03,  4.7742e-02,  3.1308e-02]],\n              \n                       [[ 3.1048e-01,  3.1961e-02,  5.5249e-02],\n                        [ 1.8907e-01,  1.6966e-01,  1.0230e-01],\n                        [ 3.8880e-01,  2.9097e-01, -1.1166e-01]]],\n              \n              \n                      [[[ 6.8107e-03, -1.3574e-01, -9.7098e-02],\n                        [-1.5107e-02, -3.3726e-01, -7.6917e-02],\n                        [ 7.0441e-02,  1.6424e-02,  9.6681e-02]],\n              \n                       [[ 3.5700e-01,  3.7734e-01, -6.9197e-02],\n                        [-1.3183e-01, -2.9332e-01,  7.3790e-02],\n                        [ 4.2114e-02,  1.1681e-02,  8.7199e-02]],\n              \n                       [[-6.6812e-02,  1.8966e-01, -6.6777e-02],\n                        [-5.2886e-02, -7.4424e-02,  2.5435e-02],\n                        [ 1.2953e-01, -9.5496e-02, -5.9930e-02]],\n              \n                       ...,\n              \n                       [[ 1.4993e-01,  1.0908e-01, -2.9542e-01],\n                        [-4.1838e-01, -1.9126e-01,  8.1429e-02],\n                        [-2.5365e-01,  2.2964e-01, -2.8396e-01]],\n              \n                       [[ 9.2373e-02, -1.7762e-01, -4.4990e-02],\n                        [-3.6753e-02,  6.4880e-02, -5.5702e-02],\n                        [ 1.6991e-01,  4.6403e-01,  1.9144e-02]],\n              \n                       [[-1.9088e-01, -1.2004e-01, -8.3663e-02],\n                        [ 3.3291e-02, -3.8354e-01, -6.9263e-02],\n                        [-5.4405e-03, -5.7586e-02, -2.8273e-02]]],\n              \n              \n                      [[[ 9.2221e-02, -1.0793e-01, -6.1033e-02],\n                        [ 2.7820e-01, -3.8493e-03,  1.1539e-03],\n                        [ 4.3354e-02, -5.3561e-03,  9.0632e-02]],\n              \n                       [[ 2.6618e-01, -1.3867e-01,  1.9751e-01],\n                        [ 6.9374e-01, -3.3530e-01,  1.2258e-01],\n                        [ 4.1823e-02,  2.6013e-01, -3.4284e-01]],\n              \n                       [[ 1.4361e-01,  3.9007e-03,  7.9538e-02],\n                        [ 2.8148e-01, -2.7022e-01, -1.1818e-02],\n                        [ 2.0440e-02,  8.7825e-02, -1.7496e-01]],\n              \n                       ...,\n              \n                       [[ 1.3704e-01,  2.2367e-01, -1.4237e-01],\n                        [ 8.8215e-02, -1.2951e-01,  1.3631e-01],\n                        [ 2.0881e-02,  1.3305e-01,  1.7804e-01]],\n              \n                       [[-1.0644e-01,  2.8378e-02,  1.2387e-01],\n                        [-1.2382e-01, -5.8165e-02, -1.0025e-01],\n                        [-9.4124e-02, -2.5104e-03, -1.1272e-01]],\n              \n                       [[ 2.8924e-01,  1.6164e-01,  1.0048e-01],\n                        [ 7.1857e-02, -1.3417e-02, -1.5745e-01],\n                        [-9.0494e-02,  9.6915e-02,  1.1272e-01]]]], device='cuda:0')),\n             ('layer2.1.bn2.weight',\n              tensor([0.4384, 0.5018, 0.8332, 0.7665, 0.9086, 0.6960, 0.8752, 1.2863, 0.8057,\n                      0.1596, 0.9700, 0.8580, 0.5722, 0.8348, 0.9199, 0.5158, 0.6620, 0.7890,\n                      0.6945, 0.3822, 0.8324, 0.6326, 0.6704, 0.7654, 0.6911, 0.8886, 0.9397,\n                      0.6159, 0.6521, 0.6401, 1.0258, 0.7482, 0.7864, 0.6564, 0.7910, 0.6234,\n                      0.7978, 1.0855, 0.8213, 0.6077, 1.2053, 0.7916, 0.6734, 0.9959, 1.2424,\n                      0.5409, 0.6187, 0.8935, 0.8720, 0.6140, 0.6760, 0.7623, 0.3537, 0.8522,\n                      1.0737, 0.6630, 0.9137, 0.9938, 0.5312, 0.4596, 0.9629, 0.8111, 0.6299,\n                      0.3320, 0.7779, 0.6536, 0.7686, 0.5359, 0.7125, 0.4210, 0.6128, 0.8860,\n                      0.4289, 0.9671, 0.3664, 0.5826, 0.7086, 0.4869, 0.7835, 0.5440, 0.7174,\n                      0.5729, 1.3314, 0.5686, 1.0040, 0.5527, 0.5561, 0.7453, 1.1770, 0.5826,\n                      0.7954, 1.0907, 0.7858, 0.4245, 1.0243, 0.9385, 0.3976, 1.1765, 0.6916,\n                      0.4131, 1.0402, 0.7702, 0.6638, 0.8037, 0.3966, 0.6153, 0.4954, 0.6139,\n                      0.5951, 0.3236, 0.7292, 0.6080, 0.8607, 0.1889, 0.7265, 0.9736, 0.8434,\n                      0.6023, 0.8067, 0.8917, 0.7955, 0.5228, 0.7269, 0.3214, 0.5899, 0.3875,\n                      0.7196, 0.5657], device='cuda:0')),\n             ('layer2.1.bn2.bias',\n              tensor([-0.3571, -0.3389, -0.2644, -0.2243, -0.1884, -0.3132, -0.1736, -0.1623,\n                      -0.5368, -0.0294, -0.0639, -0.0173, -0.3620, -0.3435, -0.4047, -0.3732,\n                      -0.3309, -0.4517, -0.2772, -0.2629, -0.2561, -0.7106, -0.5633, -0.3948,\n                      -0.3486, -0.2678, -0.0910, -0.3459, -0.3122, -0.5363, -0.2329, -0.1499,\n                      -0.4217, -0.2032, -0.4069, -0.2875, -0.4851, -0.2822, -0.4434, -0.2723,\n                      -0.4477,  0.0200, -0.3863,  0.0489, -0.2671, -0.3108, -0.3600, -0.4975,\n                      -0.0297, -0.0940, -0.4334, -0.2356, -0.3841, -0.5043, -0.1065, -0.3177,\n                      -0.4967, -0.2679, -0.3640, -0.1485, -0.3842, -0.2491, -0.2339, -0.1721,\n                      -0.1957, -0.6525, -0.1131, -0.1282, -0.3103, -0.4830, -0.4931, -0.1732,\n                      -0.3826, -0.3309, -0.2959, -0.3608, -0.2206, -0.3304,  0.0178, -0.4308,\n                      -0.2306, -0.2083, -0.2590, -0.1934, -0.0785, -0.3348, -0.2014, -0.2629,\n                      -0.0715, -0.5247, -0.4673, -0.0969, -0.2961, -0.8529, -0.3482, -0.0349,\n                      -0.4599, -0.3736, -0.2016, -0.1163, -0.3668, -0.2726, -0.2543, -0.3042,\n                      -0.2522, -0.1457, -0.2102, -0.4779, -0.1842, -0.1446, -0.3869, -0.4243,\n                       0.0397,  0.0039, -0.2838, -0.3275, -0.5567, -0.1528, -0.4821, -0.4091,\n                      -0.4427, -0.1329, -0.2171, -0.2630, -0.3893, -0.2631, -0.1124, -0.4092],\n                     device='cuda:0')),\n             ('layer2.1.bn2.running_mean',\n              tensor([ -8.9323,  -0.7964,   7.4470,   2.0628,  -5.2790, -11.4683,  -2.0999,\n                       -5.7960,  14.3708,  -5.7705,  -5.4215,  -6.9858,  -9.1862,  -2.8580,\n                        4.3031,  -1.0882,  -0.1081,  -0.1974,  -2.4851, -13.6987,   3.4178,\n                       -0.2778,  -2.4944,  -5.5702,   1.4713,  -5.2371,  -6.6761,  19.7089,\n                       -3.6611,  -1.2523,   2.2065,  -6.0305,  -3.7454,  -4.7823,  -1.0353,\n                       -6.7759,   0.7628,   2.1336,   1.8896,  14.0442,  -3.5662,  -4.0075,\n                        8.6736,  -3.2567,  -6.7108,   3.9775, -11.3672,  -1.3347,  -2.6133,\n                        3.4767,  -1.7608, -10.4068, -10.8202,  -4.4051,  -4.7476, -11.1836,\n                        1.5757,   0.9848,   0.3380,  14.3650,  -1.2958,  -8.6000,  -2.6549,\n                        0.7290,  -1.0426,  -5.8152, -10.8488,  11.9925,  -1.3500,   7.6211,\n                       -7.1858,  -3.9483,  -9.3554,  -2.0796,  -1.3172,   7.4527,   7.6834,\n                        7.2471,  -7.6214,  -9.0510, -11.0007,   7.1386,  -4.4336,   5.6996,\n                       -1.7889,   4.0178,  14.7864,  11.8403,  -2.9151,  -9.9588,   1.9900,\n                       -4.6287,  -6.9169,   7.7900,  -3.7224,  -1.5563, -10.6375,  -3.3160,\n                       -8.2294,  -7.3993,   2.5311,  -7.5258,  13.3583, -10.8180, -11.1441,\n                        6.9704, -14.1099,  -4.3034,   9.7482,  -1.2645,  -2.5073, -13.6822,\n                       -8.0116,  -8.6196, -11.7017,  -3.0547,  -1.7069,   6.1837,   0.1918,\n                       -1.3024,  -0.6233,  -8.8327, -12.3298,  -1.8467,  -8.1369,  17.2423,\n                       -7.2822,   0.4472], device='cuda:0')),\n             ('layer2.1.bn2.running_var',\n              tensor([175.7634, 152.3940, 116.3468, 132.7298,  94.6391, 160.9006,  80.5053,\n                       90.7218, 156.0105, 469.4639, 105.9778, 107.1464, 149.8619,  50.5076,\n                       54.7285, 121.0524, 110.9677,  91.8072,  73.2300, 247.9712,  51.6174,\n                      157.3106,  77.1094,  85.4341, 106.4589, 104.5274, 152.7812, 268.6018,\n                       80.0560,  62.0881,  90.6739,  99.3105, 104.0276, 111.9210, 135.8006,\n                      145.6656, 112.7907,  57.1181, 112.7048, 149.8317,  65.2838, 137.4801,\n                      133.5071, 100.3290,  92.6983, 170.3421, 242.4223,  72.1467,  96.0274,\n                      100.5731, 107.1947, 160.6289, 227.0192, 138.8331,  80.7678, 185.3863,\n                       52.1785,  81.6003, 144.6617, 237.1388, 119.0957, 161.4553,  41.2028,\n                      180.9325, 104.6755,  97.3488, 121.5834, 192.9131,  75.6934, 155.6289,\n                      113.4698,  91.3632, 176.8611, 124.2819, 196.0320, 233.7075, 143.3431,\n                      260.8891, 120.7189,  90.4930, 125.3940, 199.4739,  72.7822, 178.4830,\n                       92.0619, 148.4836, 270.8366, 149.2349,  34.7994, 142.7654,  78.5763,\n                       78.7187, 179.2730, 167.6300,  77.7470,  90.4871, 257.9274,  90.5511,\n                      210.6621, 272.6058,  59.7982, 125.5192, 193.7350, 145.9872, 150.1307,\n                      118.3762, 242.6340, 146.1272, 172.1981, 250.6707, 114.4699, 173.1115,\n                      156.8812, 414.8380, 125.4528,  63.1147,  60.8433, 212.6621, 115.5004,\n                       41.2161, 131.2915, 115.8301, 162.0480, 325.4672, 160.7753, 274.7084,\n                      129.0528,  96.8633], device='cuda:0')),\n             ('layer2.1.bn2.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer3.0.identity_downsample.0.weight',\n              tensor([[[[-0.2324]],\n              \n                       [[-0.3486]],\n              \n                       [[ 0.0403]],\n              \n                       ...,\n              \n                       [[ 0.1064]],\n              \n                       [[ 0.1168]],\n              \n                       [[-0.1171]]],\n              \n              \n                      [[[ 0.2100]],\n              \n                       [[-0.4756]],\n              \n                       [[-0.0845]],\n              \n                       ...,\n              \n                       [[ 0.0307]],\n              \n                       [[ 0.1728]],\n              \n                       [[ 0.3949]]],\n              \n              \n                      [[[ 0.2780]],\n              \n                       [[-0.0722]],\n              \n                       [[-0.7201]],\n              \n                       ...,\n              \n                       [[-0.0492]],\n              \n                       [[ 0.2334]],\n              \n                       [[ 0.3731]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.3587]],\n              \n                       [[ 0.1098]],\n              \n                       [[-0.2574]],\n              \n                       ...,\n              \n                       [[-0.1148]],\n              \n                       [[-0.1246]],\n              \n                       [[ 0.3262]]],\n              \n              \n                      [[[-0.2296]],\n              \n                       [[-0.0312]],\n              \n                       [[ 0.2712]],\n              \n                       ...,\n              \n                       [[ 0.0916]],\n              \n                       [[ 0.2267]],\n              \n                       [[-0.0970]]],\n              \n              \n                      [[[-0.1172]],\n              \n                       [[-0.0809]],\n              \n                       [[-0.3117]],\n              \n                       ...,\n              \n                       [[-0.0484]],\n              \n                       [[ 0.3752]],\n              \n                       [[ 0.1262]]]], device='cuda:0')),\n             ('layer3.0.identity_downsample.1.weight',\n              tensor([1.0110, 1.1395, 1.0547, 0.7854, 0.8006, 1.1053, 0.7924, 0.8783, 0.6286,\n                      0.8244, 0.6362, 0.7662, 1.0991, 0.9049, 0.8779, 1.2514, 0.7538, 0.8646,\n                      0.7979, 0.6267, 0.7728, 0.8792, 0.8956, 0.6413, 1.0593, 1.0547, 0.8370,\n                      1.1882, 1.0417, 0.5223, 0.9144, 1.1003, 0.7491, 0.7764, 0.9362, 0.6358,\n                      0.9125, 0.9830, 0.6457, 0.9206, 0.7970, 0.8457, 0.8920, 0.8126, 1.1941,\n                      0.6434, 1.0715, 1.1067, 1.0674, 0.7608, 0.8135, 0.8063, 0.7512, 1.2338,\n                      1.0257, 0.7586, 0.8596, 1.1126, 1.0423, 0.8830, 1.1789, 0.8190, 0.7059,\n                      1.1505, 1.0330, 0.8157, 0.8601, 0.8702, 0.8648, 1.1654, 1.3694, 0.6397,\n                      0.6183, 0.7492, 0.8654, 0.5240, 0.8240, 0.9513, 0.5572, 0.8676, 1.2742,\n                      0.9260, 1.1154, 1.0457, 0.9870, 0.7558, 0.6644, 0.6627, 0.7137, 0.7094,\n                      0.7628, 0.6649, 1.1556, 0.7373, 0.5943, 0.8870, 0.8963, 0.9954, 1.1543,\n                      0.9657, 0.8512, 0.9000, 1.1019, 0.7394, 0.7577, 0.5706, 1.1241, 0.8411,\n                      0.6200, 0.8282, 0.9993, 1.0041, 0.8985, 0.6412, 0.6356, 0.8120, 1.0009,\n                      0.8325, 0.9415, 0.8204, 0.7056, 1.1133, 0.9792, 0.9166, 0.7404, 0.5663,\n                      0.6711, 0.7501, 1.1041, 0.7127, 0.6379, 0.8489, 0.6053, 0.7570, 0.9078,\n                      0.6805, 0.6854, 1.0355, 0.9390, 0.8255, 0.6387, 0.6754, 0.5413, 1.0473,\n                      1.0336, 1.0836, 0.7546, 0.9681, 0.6752, 0.6844, 1.1205, 0.7040, 0.8292,\n                      0.5731, 0.7578, 0.9365, 0.9330, 0.5886, 0.9081, 0.8673, 0.7184, 0.6741,\n                      0.9678, 0.6043, 0.8760, 0.7688, 0.5810, 0.6999, 1.1494, 0.8801, 0.5997,\n                      1.2259, 0.7263, 1.0361, 0.7745, 0.9061, 1.2793, 1.0779, 0.9536, 1.0542,\n                      0.9287, 0.7468, 0.7900, 0.8052, 0.9726, 0.7021, 0.8286, 0.7297, 1.1142,\n                      0.8593, 0.7978, 1.0046, 0.7316, 0.8176, 0.7516, 0.6589, 0.7976, 0.5564,\n                      0.7660, 1.0840, 0.6119, 0.8986, 0.6854, 0.8500, 0.4777, 0.7434, 0.9622,\n                      0.8458, 0.9550, 0.7282, 0.7279, 0.7806, 0.7078, 0.5368, 0.8720, 0.5681,\n                      0.7578, 0.9615, 0.9842, 0.7728, 0.7150, 0.6506, 0.6411, 0.9888, 0.7260,\n                      1.0064, 0.8130, 0.9282, 0.8560, 1.1452, 0.8130, 1.0029, 0.9228, 1.2558,\n                      0.6579, 0.7794, 0.7985, 1.0015, 0.9176, 0.4640, 0.9338, 1.1228, 1.1530,\n                      1.1781, 0.6888, 0.8013, 0.8654, 0.8595, 0.9083, 0.8556, 1.0315, 0.7554,\n                      0.8243, 0.7436, 0.6881, 0.7716], device='cuda:0')),\n             ('layer3.0.identity_downsample.1.bias',\n              tensor([-0.1598, -0.5020, -0.3155, -0.3373, -0.3653, -0.2048, -0.0854, -0.0590,\n                      -0.2315, -0.2423, -0.3688, -0.3579, -0.1339, -0.2603, -0.2034, -0.2973,\n                      -0.3014, -0.2065, -0.2660, -0.3527, -0.2248, -0.4519, -0.1645, -0.2955,\n                      -0.2540, -0.2666, -0.2765, -0.3058, -0.2859, -0.2838, -0.3805, -0.1928,\n                      -0.1489, -0.3225, -0.2606, -0.3590, -0.3775, -0.3338, -0.4669, -0.2022,\n                      -0.2932, -0.3413, -0.3683, -0.4085, -0.2868, -0.4991, -0.2058, -0.2613,\n                      -0.0839, -0.3820, -0.2747, -0.3277, -0.2588, -0.3615, -0.2582, -0.5516,\n                      -0.4159, -0.3339, -0.1113, -0.5540, -0.1968, -0.2065, -0.2999, -0.3183,\n                      -0.3719, -0.2590, -0.2928, -0.2906, -0.3611, -0.3589, -0.2565, -0.3915,\n                      -0.4936, -0.3145, -0.3409, -0.5051, -0.3106, -0.1937, -0.1833, -0.4259,\n                      -0.2671, -0.0213, -0.0480, -0.2678, -0.2239, -0.2247, -0.4833, -0.2741,\n                      -0.2628, -0.2860, -0.3714, -0.4992, -0.0241, -0.2949, -0.2723, -0.1069,\n                      -0.0491, -0.2731, -0.2656, -0.3144, -0.3158, -0.1934, -0.4995, -0.4504,\n                      -0.2895, -0.3173, -0.3787, -0.2003, -0.2767, -0.4729, -0.0396, -0.2869,\n                      -0.2156, -0.3900, -0.3241, -0.1065, -0.3935, -0.0775, -0.2297, -0.2795,\n                      -0.3000, -0.0593, -0.2699, -0.1157, -0.2184, -0.4691, -0.2285, -0.2837,\n                      -0.3087, -0.2945, -0.4994, -0.3198, -0.3566, -0.2660, -0.3853, -0.3551,\n                      -0.4367, -0.2355, -0.2520, -0.3040, -0.3676, -0.2999, -0.3887, -0.1437,\n                      -0.3174, -0.2960, -0.2199, -0.0294, -0.1826, -0.3475, -0.5019, -0.2671,\n                      -0.1684, -0.5330, -0.2976, -0.3120, -0.3235, -0.3301, -0.4263, -0.1808,\n                      -0.2569, -0.3319, -0.2428, -0.3011, -0.1274, -0.3338, -0.4148, -0.2811,\n                      -0.0906, -0.1335, -0.4549, -0.3030, -0.2185, -0.2085, -0.3071, -0.1888,\n                      -0.2945, -0.3181, -0.2550, -0.1858, -0.2753, -0.2285, -0.2684, -0.0734,\n                      -0.2103, -0.2634, -0.1734, -0.3141, -0.1835, -0.2596, -0.3595, -0.3341,\n                      -0.4142, -0.1692, -0.3745, -0.3288, -0.1361, -0.3446, -0.3437, -0.1796,\n                      -0.2345, -0.0325, -0.3363, -0.0846, -0.5425, -0.3133, -0.2466, -0.1898,\n                      -0.3338, -0.1465, -0.2444, -0.3874, -0.3227, -0.4193, -0.1250, -0.5395,\n                      -0.2608, -0.4238, -0.1146, -0.1087, -0.1033, -0.3587, -0.3139, -0.2272,\n                      -0.2677, -0.2635, -0.2974, -0.1805, -0.2290, -0.2758, -0.2828, -0.3772,\n                      -0.2408, -0.1249, -0.2319, -0.1282, -0.3765, -0.0712, -0.3245, -0.3796,\n                      -0.3432, -0.1864, -0.2523, -0.2239, -0.3583, -0.1181, -0.2984, -0.3103,\n                      -0.2460, -0.0957, -0.1480, -0.1370, -0.2615, -0.3192, -0.3645, -0.2009],\n                     device='cuda:0')),\n             ('layer3.0.identity_downsample.1.running_mean',\n              tensor([ 2.0363, -2.6729, -3.9781, -4.3259,  2.9041, -3.8040, -1.3146,  1.7176,\n                       3.3793, -2.8358, -1.5781, -4.4218,  0.1271,  0.4363, -4.9090, -3.1546,\n                      -4.1949, -3.7005,  1.1343, -3.4276, -5.9460, -1.6473,  2.7169,  0.6855,\n                      -3.1307, -4.9908,  0.2969, -5.0175, -1.8345, -2.7162,  0.2558,  0.3408,\n                       2.9858, -2.7489,  0.9771, -3.4474, -3.6957,  1.5310, -1.2672, -5.1201,\n                      -5.7915,  2.4775, -4.4765, -4.3927, -5.0681, -3.1846, -1.1397, -3.9612,\n                       0.8699, -2.3309, -3.0784, -3.2451, -3.7406, -4.5243, -5.2055, -2.1191,\n                      -3.5291,  3.1646, -1.1934,  1.3860, -4.8264, -4.2833, -2.7439, -4.7311,\n                      -5.3480, -4.8796, -2.3717, -4.2362, -2.6213, -3.3701, -3.4539,  2.3056,\n                       0.1516, -4.6279,  3.3661,  3.5376, -3.5318, -1.6200, -4.9445,  3.6382,\n                      -5.7963,  1.8691,  1.0710, -4.2799,  1.1309, -5.6278, -4.8724, -5.0051,\n                      -4.1873, -3.0099, -3.9776, -2.2191,  0.4951, -4.9583, -4.6179,  1.5869,\n                       1.6873, -3.5592, -3.4304, -4.9264, -1.3958,  1.9301, -2.6503, -0.9624,\n                       2.0002,  2.6117, -2.9357, -2.8792,  2.8506,  4.9169, -0.2248, -1.5068,\n                      -0.1291, -2.5171, -3.9459,  2.2221, -2.8965,  1.1108, -4.0801, -4.6486,\n                      -2.8274, -0.1876, -2.5396,  0.3089, -4.0210, -2.0935,  3.2063, -5.4562,\n                      -0.5312, -4.0520, -2.6989, -2.8295, -4.2387,  2.6304, -2.9411,  3.1475,\n                       1.6313, -2.9247, -4.4119,  1.8512, -1.1153, -3.8623,  4.0433,  0.1281,\n                      -2.4087, -3.2098,  2.9553, -0.7187, -4.5875, -0.7575,  0.5031, -5.7019,\n                      -4.7041, -3.8759, -4.9910, -1.3297,  1.3795, -3.5302,  3.4815,  0.4448,\n                      -3.7766, -3.2548,  0.4437, -0.4135,  3.0750, -3.9642, -2.7505,  0.1938,\n                      -4.6577, -0.1214,  3.9395, -4.8381, -5.0850,  1.1594, -5.8549, -2.2899,\n                      -4.3189,  3.4920, -0.4963, -0.1834, -4.5232,  1.2520, -5.3618,  1.4857,\n                       1.6413,  4.5382,  2.7684,  1.7597, -3.2912,  2.7431,  2.6688,  1.5893,\n                       2.7939,  2.1581, -4.7889, -1.4327,  2.6988, -5.2693,  1.8340,  1.3520,\n                       3.1507,  3.0900, -3.0439,  3.0931,  0.0998, -4.0514, -3.0434,  0.9263,\n                      -1.8377, -3.3865, -0.2227,  0.7273, -3.3922,  3.1599, -1.7056,  2.1842,\n                      -1.9880, -4.0061,  1.1827, -3.2437, -4.2349, -0.1700, -2.6324, -3.8858,\n                      -2.9823, -3.5431, -4.6620, -4.8483, -3.9122, -3.8404,  2.4179, -4.8832,\n                      -5.6739, -3.4976, -2.7999,  2.3120, -0.8153, -0.5427, -5.4870, -4.5268,\n                      -2.5545, -4.2592, -3.2883,  0.8088, -2.5465,  2.6452, -4.2654, -4.8703,\n                      -1.9079,  3.3882, -1.0042,  2.1827,  1.1053, -3.5716, -1.1763, -3.4567],\n                     device='cuda:0')),\n             ('layer3.0.identity_downsample.1.running_var',\n              tensor([ 28.0436,  12.3217,  50.5201,  66.1733,  42.4067,  59.3046,  28.1831,\n                       34.4700,  43.2237,  35.5897,  34.0173,  72.8711,  17.6014,  17.7855,\n                       84.7399,  33.2255,  61.0833,  31.2652,  28.9278,  16.9614,  63.3646,\n                       14.5241,  38.9503,  27.9415,  41.3132,  55.2865,  25.3643,  57.0374,\n                       14.7717,  50.9512,   8.9203,  19.6119,  48.0625,  33.1001,  21.7843,\n                       67.3322,  20.2633,  34.7881,  45.5073,  92.8984, 123.4126,  33.9044,\n                       54.0161,  37.5983,  46.6547,  37.6576,  21.4491,  38.7386,  18.5815,\n                       38.3374,  19.0006,  31.3030,  42.4692,  56.3841,  76.0621,  33.1036,\n                       34.3068,  33.7564,  18.9833,  13.1664,  47.9734,  30.4058,  33.4690,\n                       45.3213,  91.5079,  32.5392,  23.7677,  36.6338,  35.5819,  39.6885,\n                       53.6517,  54.1596,  32.5656,  45.8911,  45.7694,  33.4012,  14.6368,\n                       17.8221,  59.9469,  20.9514,  49.0208,  33.1034,  21.4913,  35.0413,\n                       12.7196, 124.4615,  67.1154,  77.4338,  60.0313,  20.5230,  23.2126,\n                       57.8995,  21.9714,  35.8514,  73.6134,  28.5999,  32.0823,  45.8228,\n                       30.7529,  61.9681,  28.5495,  35.4564,  54.3293,  25.2068,  18.1916,\n                       26.7753,  23.6316,  48.0255,  51.1733,  32.2245,  26.1420,  22.8188,\n                       17.7017,  31.4762,  53.5274,  34.0286,  39.3453,  39.7593,  64.7925,\n                       80.2761,  26.9713,  27.5822,  17.6354,  28.7978,  30.0823,  25.1558,\n                       58.0981,  45.4612,  16.1692,  51.0333,  47.5606,  24.5383,  61.8904,\n                       50.0374,  41.2237,  48.1498,  57.2882,  37.4260,  31.5069,  28.4432,\n                       14.7149,  67.0491,  39.7326,  14.3071,  26.6028,  40.5421,  55.6573,\n                       42.3689,  60.0903,  24.7338,  15.4477,  53.8889,  23.4519,  74.6580,\n                       30.8415,  26.7286,  23.8610,  52.4784,  45.4128,  29.3441,  29.8459,\n                       54.1192,  29.9103,  31.5571,  59.3570,  36.7776,  59.1163,  25.1001,\n                       84.5421,  29.5083,  43.6995,  43.3239,  46.1241,  12.5563, 118.7822,\n                       18.2649,  31.6827,  23.6270,  21.5077,  15.3498,  75.2244,  36.8507,\n                       79.5850,  33.4482,  46.1601,  71.8300,  36.1444,  61.8626,  46.6094,\n                       56.9968,  46.6292,  19.3908,  21.9085,  36.5468,  48.8296,  20.7764,\n                       70.3426,  90.9620,  43.8396,  40.7947,  49.5419,  39.8654,  41.8293,\n                       38.9828,  31.3433,  67.8535,  35.7740,  31.9670,  31.3371,  38.5342,\n                       19.8809,  37.7379,  57.8840,  41.6928,  45.6059,  18.1769,  18.3317,\n                       44.4413,  34.1668,  22.1927,  67.2679,  36.7108,  26.7313,  43.8247,\n                       20.4491,  36.0300,  51.1367, 102.1457,  68.4997,  20.1801,  21.0628,\n                      101.8597, 101.6215,  35.5941,  61.3988,  45.4270,  12.8766,  41.9768,\n                       81.4619,  89.9214,  25.8568,  49.5478,  48.1688,  30.0538,  13.8002,\n                       31.0301,  65.4167,  65.3979,  23.3597,  42.0864,  19.6566,  45.8343,\n                       31.5320,  52.2518,  14.6077,  45.6970], device='cuda:0')),\n             ('layer3.0.identity_downsample.1.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer3.0.conv1.weight',\n              tensor([[[[ 1.8950e-01,  2.3107e-01, -3.2085e-01],\n                        [ 5.8924e-02, -1.3453e-01,  3.1285e-02],\n                        [-2.0388e-01,  3.9651e-02,  1.3766e-01]],\n              \n                       [[-1.4168e-01, -3.5605e-02, -3.7698e-02],\n                        [-3.0537e-01, -2.7403e-01, -2.2026e-02],\n                        [ 7.2676e-02, -9.7618e-02, -2.2530e-01]],\n              \n                       [[-3.2461e-02,  8.4475e-02,  1.5974e-02],\n                        [-5.7476e-02,  8.4337e-02, -1.2538e-01],\n                        [-5.2375e-01, -1.2175e-01,  5.1584e-03]],\n              \n                       ...,\n              \n                       [[ 9.4057e-02, -1.0129e-01, -1.6656e-02],\n                        [-2.5468e-01, -3.3866e-01, -1.9656e-02],\n                        [-2.5902e-01, -3.4638e-01, -6.1768e-02]],\n              \n                       [[-9.2127e-02,  7.4849e-02,  6.1899e-02],\n                        [-9.2760e-02, -3.6183e-02,  5.6711e-03],\n                        [-2.8556e-01, -6.0082e-02,  8.7855e-02]],\n              \n                       [[-3.7198e-01, -2.8568e-01,  2.3533e-01],\n                        [-2.7433e-01, -2.4372e-02, -1.3510e-01],\n                        [ 1.2897e-01,  1.2238e-01,  3.9874e-02]]],\n              \n              \n                      [[[-1.5486e-01, -2.0291e-01,  6.2170e-02],\n                        [-1.1358e-01,  1.3009e-01,  1.6735e-01],\n                        [ 2.7295e-01, -5.9600e-03,  1.8729e-01]],\n              \n                       [[ 2.9717e-01,  1.8928e-01, -1.0696e-02],\n                        [ 1.3154e-01, -5.2824e-02, -1.3259e-01],\n                        [ 3.1867e-01, -3.6600e-02, -1.4178e-01]],\n              \n                       [[ 2.8360e-03,  1.5639e-01,  6.1672e-03],\n                        [ 8.0066e-02,  4.4034e-02,  7.0969e-02],\n                        [ 6.4266e-02,  3.3090e-02, -1.0507e-01]],\n              \n                       ...,\n              \n                       [[ 1.0461e-01,  1.0646e-01,  7.7038e-02],\n                        [ 1.1308e-01,  1.1605e-01,  1.1108e-01],\n                        [ 1.3630e-01,  5.1513e-03, -2.0623e-01]],\n              \n                       [[-7.6510e-02, -2.2076e-01, -1.8300e-02],\n                        [ 5.1077e-02,  1.3842e-01, -3.8153e-02],\n                        [ 2.1785e-01,  2.7478e-01, -1.7164e-03]],\n              \n                       [[ 3.8132e-01,  3.6104e-02,  1.3307e-01],\n                        [ 8.4721e-02,  1.7680e-01, -1.6992e-01],\n                        [-1.3381e-02,  5.4631e-02,  1.0128e-01]]],\n              \n              \n                      [[[ 6.7176e-02, -2.4413e-01,  3.0677e-01],\n                        [ 2.9211e-01, -2.7564e-01,  4.0020e-01],\n                        [ 4.4499e-02,  5.9940e-02,  2.4925e-01]],\n              \n                       [[-6.6248e-02,  9.6827e-02, -4.4984e-01],\n                        [-2.7393e-01,  5.8490e-02, -2.9567e-02],\n                        [ 5.9142e-02,  2.2831e-01, -1.1799e-01]],\n              \n                       [[-2.1834e-01, -9.1353e-02, -1.3597e-01],\n                        [-1.3293e-01,  1.3896e-01, -2.8304e-02],\n                        [-5.2205e-02,  2.5504e-01, -3.7941e-01]],\n              \n                       ...,\n              \n                       [[ 1.7562e-01,  1.1285e-01,  5.5539e-02],\n                        [-2.0215e-02,  1.1465e-01,  9.2760e-02],\n                        [-3.8968e-02,  2.1533e-01,  5.4690e-02]],\n              \n                       [[-3.2280e-03, -1.4849e-02, -1.0875e-02],\n                        [-2.8281e-02, -9.1248e-02,  9.7739e-02],\n                        [-1.8691e-01, -1.5883e-01,  1.4516e-01]],\n              \n                       [[-5.1086e-02, -2.8439e-01, -9.9295e-03],\n                        [-3.0785e-01, -2.5202e-01, -2.1442e-02],\n                        [ 4.2372e-02,  1.0315e-01, -6.7928e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 9.1674e-02, -8.1234e-02,  2.5089e-01],\n                        [-2.8961e-01, -3.1199e-01, -2.1953e-02],\n                        [ 1.1320e-01,  1.8481e-01,  2.1440e-01]],\n              \n                       [[-1.0781e-01,  5.4785e-02, -1.2559e-04],\n                        [ 7.5464e-02, -5.2995e-02,  1.0393e-03],\n                        [ 6.8733e-02, -7.3171e-02,  7.2849e-02]],\n              \n                       [[ 2.1107e-02, -2.1081e-01,  4.5895e-01],\n                        [ 3.0163e-01, -1.4389e-02,  8.0219e-02],\n                        [ 3.5681e-01, -1.3600e-01,  3.2551e-01]],\n              \n                       ...,\n              \n                       [[-1.2535e-01,  7.3198e-02, -5.6967e-02],\n                        [-4.5035e-02,  1.3973e-01,  4.2756e-02],\n                        [-2.5708e-01,  1.5596e-01,  2.5939e-02]],\n              \n                       [[-1.1657e-01, -4.0822e-02, -1.3083e-01],\n                        [-6.7979e-02,  1.7242e-01,  2.4923e-01],\n                        [ 9.6353e-02,  2.9048e-01,  9.4250e-02]],\n              \n                       [[-1.7951e-01,  1.1048e-01,  2.8770e-02],\n                        [-7.4212e-02, -3.4089e-01,  6.8699e-02],\n                        [ 1.0174e-01, -3.9679e-01,  1.8885e-01]]],\n              \n              \n                      [[[ 1.6952e-02,  6.6012e-02, -1.6681e-02],\n                        [ 3.3472e-01,  1.9064e-01,  2.0976e-01],\n                        [-3.0496e-01, -2.5568e-01, -3.5769e-01]],\n              \n                       [[ 8.9127e-02,  1.0712e-01, -1.4664e-01],\n                        [-2.0052e-01, -3.6111e-01, -1.5657e-01],\n                        [-6.9118e-03, -2.9612e-02,  1.0141e-01]],\n              \n                       [[-3.7647e-01, -2.1377e-01, -2.5979e-01],\n                        [-4.5947e-02, -1.8772e-01, -9.2856e-02],\n                        [-2.1868e-02, -1.3805e-01,  3.2679e-02]],\n              \n                       ...,\n              \n                       [[ 5.1855e-02,  1.7604e-02, -4.7860e-01],\n                        [-2.4972e-01, -2.3013e-01, -2.3926e-02],\n                        [-1.9852e-01,  2.1644e-01,  7.9234e-02]],\n              \n                       [[ 2.4114e-01,  2.3215e-01,  7.3038e-02],\n                        [ 4.1427e-02, -1.0243e-01, -2.4355e-02],\n                        [-4.2213e-01,  3.7470e-01,  2.6867e-01]],\n              \n                       [[ 1.7736e-01,  2.3378e-02, -8.7009e-02],\n                        [-2.4593e-01, -2.9044e-01, -1.1001e-01],\n                        [-9.9904e-02, -3.9902e-01, -2.0892e-01]]],\n              \n              \n                      [[[ 1.7409e-01, -4.2906e-01,  4.6935e-01],\n                        [-2.1578e-01, -1.8023e-01, -5.9426e-02],\n                        [ 5.9849e-02,  2.0211e-01,  9.1607e-02]],\n              \n                       [[-6.9057e-03,  1.3880e-01,  4.8373e-02],\n                        [ 9.0739e-02, -2.2964e-01, -8.3817e-03],\n                        [ 1.3751e-01,  1.1732e-01,  7.3743e-03]],\n              \n                       [[ 9.1162e-02, -1.4207e-01,  1.0123e-01],\n                        [-8.5804e-03,  3.2759e-02, -4.1171e-02],\n                        [ 1.1859e-01,  1.7646e-01,  2.2767e-01]],\n              \n                       ...,\n              \n                       [[ 9.8734e-02,  8.8759e-02,  5.3525e-02],\n                        [ 1.2057e-01,  1.0439e-01, -1.8067e-01],\n                        [ 3.9313e-02,  1.2021e-02,  3.9332e-02]],\n              \n                       [[-1.3784e-01, -1.2089e-02, -3.2757e-01],\n                        [-1.6974e-01,  7.8153e-02, -6.2202e-02],\n                        [-2.0263e-01,  4.2790e-01, -1.2493e-01]],\n              \n                       [[-3.2458e-01,  2.7521e-01, -4.4166e-01],\n                        [ 1.3086e-01, -3.4008e-01, -1.5110e-03],\n                        [-1.8086e-02, -1.0472e-01, -8.1979e-02]]]], device='cuda:0')),\n             ('layer3.0.bn1.weight',\n              tensor([0.5348, 1.2199, 1.1276, 1.0836, 0.9243, 1.0710, 0.8872, 0.9910, 1.0346,\n                      1.3325, 0.7824, 0.7119, 1.2211, 0.9895, 1.0111, 0.8774, 0.6554, 0.9241,\n                      0.4870, 0.4834, 0.8415, 0.9071, 0.7496, 1.0844, 0.4407, 0.9133, 0.8439,\n                      0.9784, 1.2299, 1.2080, 0.5618, 0.9526, 1.2463, 1.2158, 0.3085, 1.3526,\n                      0.5470, 0.5764, 0.9113, 0.8254, 0.5573, 0.8846, 1.0699, 0.6231, 0.8912,\n                      0.9288, 0.9746, 0.8637, 1.1359, 0.8365, 0.8739, 0.9019, 1.0690, 1.1654,\n                      0.9398, 0.5230, 0.5865, 1.2166, 0.8862, 0.9628, 1.0062, 1.0011, 0.6480,\n                      0.9015, 1.1171, 0.8563, 0.9554, 0.8509, 0.6952, 0.6389, 1.1520, 0.6199,\n                      0.9504, 1.0585, 0.7351, 0.9658, 0.9263, 0.7688, 0.7157, 0.4254, 1.1587,\n                      0.6893, 0.9065, 1.0771, 1.0955, 1.3203, 0.7834, 0.7074, 1.2556, 0.9587,\n                      0.7458, 0.5411, 0.6538, 0.6178, 0.9857, 0.9830, 1.1304, 0.5569, 0.6122,\n                      0.8401, 0.9982, 0.8465, 0.8078, 1.0687, 0.6655, 0.7036, 1.0139, 0.7852,\n                      1.0004, 0.7534, 0.8160, 0.9653, 1.0888, 0.8644, 1.2439, 1.1116, 0.7031,\n                      1.0185, 0.9651, 0.8075, 1.2286, 0.8359, 0.7632, 1.3908, 0.4710, 0.8219,\n                      1.2016, 0.7612, 0.3869, 1.2738, 0.8336, 1.0749, 0.7260, 0.6527, 0.9127,\n                      1.0610, 0.8690, 1.3389, 0.8782, 0.6425, 0.9572, 1.4151, 0.7015, 1.1002,\n                      1.2696, 0.8125, 0.9998, 0.5797, 0.8921, 1.1725, 0.6888, 0.9434, 0.6563,\n                      0.8226, 1.0955, 1.0056, 0.9841, 0.8674, 0.9932, 1.3156, 0.9951, 1.2685,\n                      0.6647, 0.7582, 0.5587, 0.9574, 0.7261, 1.2944, 1.5112, 0.6973, 0.8936,\n                      1.4035, 0.8927, 1.0194, 0.9378, 0.3824, 0.9341, 1.1490, 0.9875, 1.0217,\n                      0.7729, 0.7386, 0.8766, 0.6489, 0.8148, 0.6870, 0.8253, 0.9740, 0.6312,\n                      0.8425, 0.5362, 0.9177, 1.2446, 0.6550, 0.5244, 0.9853, 0.4698, 0.8754,\n                      1.2161, 1.1302, 0.9708, 0.7371, 0.9329, 0.9441, 0.9857, 1.2725, 1.2918,\n                      0.9645, 1.2963, 1.2955, 0.8725, 1.1570, 0.5430, 1.1574, 1.1305, 1.1983,\n                      1.0573, 0.7966, 0.9574, 1.1606, 0.9851, 1.0425, 0.7693, 0.5265, 0.9347,\n                      0.8861, 1.0765, 0.8967, 0.6748, 0.7612, 0.9038, 1.1600, 1.0164, 0.9158,\n                      0.8241, 0.8180, 0.8479, 0.9377, 0.7613, 0.7480, 0.9192, 0.7591, 0.6660,\n                      1.0510, 1.2089, 0.8441, 0.6125, 0.7648, 1.1987, 0.8337, 0.5792, 1.3078,\n                      1.1374, 1.0263, 0.7979, 0.7879], device='cuda:0')),\n             ('layer3.0.bn1.bias',\n              tensor([-0.3778, -0.1297, -0.3114, -0.4697, -0.4549, -0.3539, -0.3178, -0.4919,\n                      -0.1185, -0.3729, -0.3464, -0.3833, -0.2139, -0.3186, -0.3623, -0.5218,\n                      -0.3643, -0.1392, -0.3726, -0.3666, -0.4377, -0.2510, -0.4126, -0.1982,\n                      -0.0881, -0.2270, -0.2234, -0.0669, -0.0384, -0.4503, -0.4435, -0.4217,\n                      -0.3657, -0.4408, -0.3694, -0.1011, -0.3636, -0.4760, -0.3799, -0.3328,\n                      -0.3996, -0.4467, -0.4451, -0.4169, -0.2662, -0.2523, -0.4721, -0.3317,\n                      -0.4760, -0.4353, -0.6003, -0.1734, -0.3961, -0.1894, -0.4786, -0.5161,\n                      -0.3295, -0.4745, -0.3670, -0.6711, -0.3533, -0.6125, -0.2261, -0.2415,\n                      -0.1944, -0.4258, -0.2305, -0.2893, -0.3053, -0.2541, -0.4815, -0.3547,\n                      -0.1253, -0.1979, -0.1903, -0.1736, -0.2880, -0.2818, -0.4435, -0.4345,\n                      -0.4485, -0.4039, -0.4345, -0.2850, -0.2509, -0.3975, -0.5414, -0.2303,\n                      -0.2662, -0.1298, -0.4356, -0.3991, -0.4638, -0.4306, -0.4389, -0.5366,\n                      -0.2314, -0.2043, -0.3707, -0.1648, -0.0444, -0.4021, -0.3534, -0.2784,\n                      -0.5013, -0.3166, -0.1486, -0.4521, -0.3929, -0.2814, -0.4855, -0.3590,\n                      -0.5078, -0.3386, -0.2746, -0.4300, -0.3927, -0.2906, -0.4211, -0.4721,\n                       0.0180, -0.3556, -0.2700, -0.1645, -0.3934, -0.4749, -0.2798, -0.4005,\n                      -0.4290, -0.0773, -0.5514, -0.1592, -0.2824, -0.3556, -0.3649, -0.0646,\n                      -0.4176, -0.3093, -0.1104, -0.3952, -0.0738, -0.1827, -0.3127, -0.4144,\n                      -0.2728, -0.2717, -0.2107, -0.4418, -0.3823, -0.4530, -0.3393, -0.2932,\n                      -0.4075, -0.3256, -0.4320, -0.3463, -0.0924, -0.4942,  0.0032, -0.1557,\n                      -0.1840, -0.4977, -0.5234, -0.4762, -0.3482, -0.6592, -0.4662, -0.2098,\n                      -0.4126, -0.4858, -0.3254, -0.2314, -0.2146, -0.2337, -0.2882, -0.3706,\n                      -0.2627, -0.4834, -0.3975, -0.3472, -0.6808, -0.5669, -0.5035, -0.3872,\n                      -0.2521, -0.3961, -0.4893, -0.3217, -0.2426, -0.4214, -0.4865, -0.3834,\n                      -0.2225, -0.3427, -0.3806, -0.2261, -0.2821, -0.6764, -0.2254, -0.4027,\n                      -0.2692, -0.4357, -0.4618, -0.6279, -0.1860, -0.4143, -0.3654, -0.3739,\n                      -0.3753, -0.6170, -0.2679, -0.4065, -0.2845, -0.4977, -0.4961, -0.4001,\n                      -0.2318, -0.4329, -0.0921, -0.0925, -0.4255, -0.3461, -0.3347, -0.2394,\n                      -0.1544, -0.5369, -0.1029, -0.2812, -0.3828, -0.3077, -0.3391, -0.4410,\n                      -0.3774, -0.4960, -0.3507, -0.3847, -0.4214, -0.4050, -0.4540, -0.2348,\n                      -0.5240, -0.4020, -0.1365, -0.5150, -0.3086, -0.5223, -0.3243, -0.2887,\n                      -0.2363, -0.2500, -0.4111, -0.2148, -0.2817, -0.5456, -0.3534, -0.5309],\n                     device='cuda:0')),\n             ('layer3.0.bn1.running_mean',\n              tensor([-1.0802e+01,  8.5850e+00, -1.2748e+01,  1.3825e+00, -9.1455e+00,\n                      -5.5373e+00, -4.6559e+00,  4.7750e+00, -1.1600e+01, -9.4428e+00,\n                       8.2524e+00, -1.0943e+01, -9.7346e+00,  1.7060e-01, -8.0112e+00,\n                       4.5100e-01,  8.4673e+00, -1.7738e+01, -4.7893e+00, -1.3388e+01,\n                       6.8934e-01,  3.3178e+00, -1.4648e+01, -1.2971e+01, -2.3674e+00,\n                      -1.3971e+01, -1.7326e+01, -6.3441e+00, -5.0965e+00,  1.3062e-02,\n                       1.2804e+01,  1.3267e+00, -6.2748e+00, -8.0199e+00, -1.3799e+01,\n                      -6.9293e+00, -1.5802e+01,  4.4236e+00, -1.5245e+00,  8.7808e+00,\n                      -4.3416e+00, -3.1473e-01,  8.1520e+00,  1.5306e+01, -1.3803e+01,\n                      -1.3731e+01, -1.3187e+00, -1.0038e+01,  1.1615e+00,  8.1658e+00,\n                       5.1780e+00, -1.6595e+01, -1.5386e+00, -9.2604e+00,  1.2447e+00,\n                      -7.9899e+00, -7.3435e+00, -2.8421e+00, -3.2217e+00, -2.1088e+00,\n                       3.3753e+00,  1.1716e+01, -9.4477e+00, -1.4832e+01, -1.3915e+01,\n                      -4.2257e+00,  8.8965e-01, -7.6229e+00, -1.4700e+01, -2.2659e+00,\n                       4.9621e-01,  1.0445e+01, -1.1825e+01,  4.8185e+00, -9.6212e+00,\n                      -1.4573e+01,  2.5722e+00, -3.7860e+00,  7.0591e+00, -1.0820e+01,\n                       1.0195e+00, -1.0541e+01,  6.5753e+00, -7.7725e+00, -1.4457e+01,\n                      -6.2672e+00,  5.8065e+00, -1.9421e+01, -9.3793e+00, -1.1289e+01,\n                       1.3111e+01,  1.5820e+00,  1.1391e+01, -7.8827e+00,  1.9472e+00,\n                      -2.4950e+00, -1.1584e+01,  4.1063e+00, -1.6956e+01,  9.7953e+00,\n                      -1.1779e+01, -2.2696e+00,  2.8073e+00, -1.1011e+01, -5.6922e+00,\n                      -1.3129e+01, -1.0210e+01, -9.7487e+00,  2.5918e+00, -2.0303e+01,\n                       1.0699e+01, -1.1682e+01,  7.3505e+00, -8.7186e+00, -2.1250e+00,\n                      -8.5305e-02, -5.3899e+00, -6.8372e+00, -8.4932e-01,  3.6509e+00,\n                      -8.9221e+00,  4.1337e+00,  7.4329e+00, -1.2211e+01, -1.0665e+01,\n                      -3.6881e+00,  2.0105e+00, -7.8978e+00, -8.7446e+00, -5.2912e+00,\n                       5.8427e+00, -1.5267e+01, -1.2433e+01,  9.7715e+00,  8.9082e+00,\n                      -1.2034e+01,  7.6809e+00,  7.9438e-01, -1.1503e+01,  3.8325e+00,\n                      -1.2573e+01, -1.4620e+00, -1.0304e+01, -6.5393e+00, -9.0805e+00,\n                      -1.6297e+01, -3.5761e+00, -2.2217e-02, -1.0083e+01,  3.9778e+00,\n                      -1.1070e+01, -2.1546e+00, -9.9175e+00, -2.1354e+00,  3.3174e+00,\n                      -9.3332e+00, -4.2210e+00,  6.8964e+00, -1.8202e+01, -1.2419e+01,\n                      -4.2716e+00, -1.3087e+00,  2.9756e+00,  5.8817e+00,  7.9929e-01,\n                       1.5409e+00, -2.9747e+00, -2.0661e+00, -3.3534e+00,  7.4677e+00,\n                       3.3630e+00, -3.2975e+00, -3.6931e+00, -4.8633e+00, -4.4952e-01,\n                      -2.1449e+01,  7.7930e+00,  8.8107e+00,  8.9568e+00,  2.1110e-01,\n                      -8.3944e+00, -4.9590e+00, -3.7662e+00, -2.4062e+00,  1.1945e+01,\n                       4.1695e+00,  2.6097e+00,  5.0566e+00,  9.0278e+00,  8.6452e+00,\n                      -7.7093e+00, -5.8396e+00, -8.1125e+00, -1.6910e+00,  1.0283e+01,\n                      -1.4683e+01, -2.3016e+00, -7.0737e+00, -9.8168e+00,  3.8686e+00,\n                      -6.3080e+00, -9.7076e+00,  2.9026e+00,  1.5062e+00, -8.0988e+00,\n                       6.0736e-01,  7.9924e+00, -5.7276e+00, -1.0092e+01, -6.8268e+00,\n                      -1.0008e+01,  6.7982e+00, -1.6863e+00, -2.6227e+00, -3.3842e+00,\n                      -3.9752e+00, -4.9176e+00,  1.3555e+01, -7.1663e+00, -1.3643e+01,\n                       1.1111e+00, -8.4763e+00, -1.6851e+01, -2.1259e+01, -1.5839e+01,\n                      -8.0643e-01, -1.3158e+01, -7.3149e+00,  1.2656e+01,  4.0727e+00,\n                       8.5927e+00, -1.3956e+01,  1.6941e+00,  8.6734e+00, -9.8502e+00,\n                      -2.2333e+00, -1.0858e+01, -7.0275e+00, -1.1298e+01, -1.7787e+01,\n                       2.7943e-01,  1.1450e+01, -2.0498e+01,  4.2141e+00, -4.3395e+00,\n                       1.3994e+00, -1.2625e+01, -1.2111e+01, -2.8727e+00,  4.1135e+00,\n                      -1.1750e+01, -2.5528e+00, -1.1966e+01,  1.1469e+01, -1.5705e+01,\n                       2.4568e+00], device='cuda:0')),\n             ('layer3.0.bn1.running_var',\n              tensor([330.6390, 210.0226, 184.2930,  87.1668, 122.1033, 164.0797, 201.1836,\n                      182.8237, 246.7152, 141.5156, 218.6216, 269.7341, 138.7847, 183.6978,\n                      173.8933, 192.9192, 249.8579, 330.8569, 267.0629, 480.4074, 183.8811,\n                      244.4442, 227.2772, 229.5939, 265.5320, 296.2055, 274.0794, 135.0205,\n                      125.4395, 165.2919, 305.6118, 195.9313, 190.4284, 110.8502, 385.4159,\n                      132.7099, 279.9087, 191.3656, 150.6969, 163.5806, 184.3961, 106.5542,\n                      186.9863, 356.3482, 272.3983, 151.0096, 156.1559, 159.2874, 113.7723,\n                      168.4393, 130.7992, 263.9915, 121.9941, 197.9795, 151.6022, 187.0258,\n                      249.7384,  90.0435, 145.4733, 111.4382, 189.0707, 153.6851, 201.2508,\n                      343.6097, 183.9607, 154.8542, 249.5193, 218.1779, 260.3137, 359.6687,\n                      153.4243, 239.0461, 302.7842, 225.9483, 218.0760, 304.9765, 198.2377,\n                      186.9137, 199.6451, 344.3173, 137.4290, 255.9337, 171.1112, 122.9274,\n                      137.4733, 134.0467, 193.1669, 206.4307, 153.5950, 234.9880, 222.3534,\n                      225.0162, 255.1527, 248.1915, 182.7811, 151.4039, 171.8546, 297.5886,\n                      357.0996, 278.1389, 301.2028, 214.6360, 208.0390, 172.7641, 112.4067,\n                      263.0851, 247.1033, 109.9567, 155.1467, 415.4455, 140.1564, 154.7335,\n                      113.9565, 121.1132,  64.5690,  96.3498, 195.8673, 156.4779, 109.5107,\n                      174.2322, 149.3499, 201.5314, 267.7877, 148.1340, 313.7831, 157.3324,\n                      114.3878,  88.3754, 358.4250, 110.3964, 173.6732, 220.0403, 236.9349,\n                      269.3072, 136.8645, 311.4015, 204.0613, 169.6732, 358.1752, 163.6312,\n                      249.3614,  99.9308, 250.1825, 108.6118, 172.7968, 262.4275, 113.8679,\n                      207.9660, 155.4912, 164.8661, 300.5391, 195.5252, 269.2885, 188.1232,\n                      134.5017, 178.0470, 218.1151, 159.2110, 261.8055, 182.5795, 165.7193,\n                       95.4228, 180.2918, 151.1012, 286.6491, 156.6698, 135.4705,  77.6446,\n                      118.7159, 264.3637, 235.4826, 185.0030, 190.3685, 204.8286, 182.0361,\n                      480.4233, 176.7711, 105.4091, 153.9814, 131.4735, 173.1410, 136.0326,\n                      156.9806, 252.3658, 187.5042, 159.9317, 113.2745, 197.6266, 369.2366,\n                      176.7917, 255.4153, 100.6347,  79.8077, 253.8481, 227.0300, 207.7027,\n                      419.9333, 116.8501, 173.3681, 224.5919, 217.0295, 154.3197, 135.9251,\n                       97.9152, 120.8635, 190.7498, 115.0327, 193.5587, 151.4747, 171.2331,\n                      141.0203, 168.4595, 266.1137, 133.8352, 138.9286, 168.6059, 138.0200,\n                      249.0542, 224.1447, 174.7648, 159.7018, 139.1831, 198.5629, 572.2534,\n                      332.2002, 180.8483, 133.4882, 130.2193, 247.8080, 208.7165, 212.9037,\n                      153.1619, 165.6181, 220.9792, 210.8660, 127.4825, 130.7975, 117.6265,\n                      150.9956, 295.0720, 173.1630, 174.2937, 437.6568, 164.1806, 151.8574,\n                      133.6217, 258.0763, 228.1033,  70.1692, 209.0521, 191.4746, 139.3979,\n                      175.6438, 197.4200, 329.5515, 201.0282], device='cuda:0')),\n             ('layer3.0.bn1.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer3.0.conv2.weight',\n              tensor([[[[ 3.0377e-02,  8.3115e-02, -4.1922e-01],\n                        [-3.0142e-01,  1.5329e-01,  1.1415e-01],\n                        [-1.7317e-01, -7.7124e-02, -5.5737e-02]],\n              \n                       [[-1.6754e-01,  7.2074e-02, -1.7059e-01],\n                        [-3.3447e-01, -9.3667e-02, -5.5262e-01],\n                        [-3.3794e-01, -3.8901e-02, -2.9552e-01]],\n              \n                       [[-2.5207e-02, -2.4333e-03,  3.3874e-01],\n                        [ 4.2119e-04, -2.4528e-01, -2.7554e-02],\n                        [-1.3990e-01, -3.7713e-01,  8.2544e-02]],\n              \n                       ...,\n              \n                       [[-1.0709e-01, -2.4184e-01, -1.8961e-02],\n                        [ 9.9839e-02, -3.9850e-01,  5.3409e-02],\n                        [-2.3164e-01, -5.3812e-02,  6.9378e-02]],\n              \n                       [[ 1.3316e-01,  1.0379e-01, -1.5732e-01],\n                        [ 2.2889e-01, -1.3269e-01, -1.5015e-01],\n                        [ 1.0385e-01, -1.9755e-01, -8.8610e-02]],\n              \n                       [[-1.7277e-01,  6.7629e-02, -2.8405e-01],\n                        [-1.5566e-01, -5.6009e-01, -4.0345e-01],\n                        [ 7.7470e-02,  7.1164e-02, -1.1371e-01]]],\n              \n              \n                      [[[ 7.4534e-02, -3.3709e-01, -3.8282e-01],\n                        [-4.1826e-02, -4.6030e-01,  8.0263e-02],\n                        [-3.3971e-01,  3.6236e-01, -3.5275e-01]],\n              \n                       [[ 2.0089e-01,  1.4316e-01,  4.1477e-02],\n                        [-4.7449e-01, -1.6305e-01, -1.4601e-01],\n                        [-1.0882e-01,  1.1201e-01, -8.8491e-03]],\n              \n                       [[ 8.6883e-02,  1.7526e-02,  3.2644e-02],\n                        [-5.5272e-01, -3.3363e-02, -5.9591e-01],\n                        [-3.7981e-01, -1.2017e-01, -4.1747e-01]],\n              \n                       ...,\n              \n                       [[-1.7475e-01, -1.5987e-01, -2.8329e-01],\n                        [-9.5892e-02, -9.5936e-02,  2.5938e-01],\n                        [ 2.0469e-02, -1.2857e-01, -3.4742e-01]],\n              \n                       [[-1.7720e-01,  1.1494e-01, -7.0242e-02],\n                        [-4.3810e-02,  9.1959e-04,  5.1147e-02],\n                        [-3.4412e-01, -3.2826e-01,  7.1579e-03]],\n              \n                       [[ 1.0111e-01,  1.0334e-01,  1.3973e-01],\n                        [-2.7603e-01,  5.6910e-02,  1.0180e-01],\n                        [-5.8266e-02, -3.6203e-01,  9.0657e-02]]],\n              \n              \n                      [[[-7.3874e-02, -4.8722e-01, -4.1567e-01],\n                        [ 3.1702e-02, -3.2086e-01, -1.7945e-01],\n                        [ 2.6907e-02,  1.2643e-01, -4.8419e-01]],\n              \n                       [[-1.6172e-01, -3.1226e-01, -4.8682e-01],\n                        [-3.8004e-01, -8.8941e-02, -1.2134e-01],\n                        [-7.5811e-02, -1.5259e-01, -2.2438e-01]],\n              \n                       [[ 4.1228e-01, -1.8112e-01,  3.4986e-01],\n                        [-8.1339e-02, -6.2486e-02,  3.5063e-02],\n                        [ 2.8636e-01, -9.1503e-02,  2.6377e-01]],\n              \n                       ...,\n              \n                       [[-2.6321e-01,  7.1809e-02, -5.7680e-01],\n                        [-1.3311e-01, -5.2752e-03,  3.7260e-02],\n                        [-3.0552e-01, -2.9805e-01, -2.7108e-01]],\n              \n                       [[ 2.9557e-03, -1.0368e-01, -1.6217e-01],\n                        [-8.6579e-02,  6.8528e-04, -9.7365e-02],\n                        [ 1.0669e-01, -6.6561e-02, -1.9421e-02]],\n              \n                       [[ 1.9294e-01, -6.5681e-02,  6.3916e-03],\n                        [ 7.7270e-02, -1.1689e-01, -1.4341e-01],\n                        [-4.9626e-02, -3.1487e-01, -1.6285e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-9.6362e-02, -2.9375e-01, -6.3907e-01],\n                        [-3.9768e-02, -3.3516e-01, -7.0439e-02],\n                        [ 4.0193e-02,  1.2341e-01, -3.8409e-01]],\n              \n                       [[-2.2882e-01, -1.2704e-01, -3.4097e-01],\n                        [-8.8611e-02, -7.5308e-02, -9.9710e-03],\n                        [ 5.7935e-02, -1.9519e-01, -1.0021e-01]],\n              \n                       [[ 7.1163e-02, -2.6055e-01,  2.0760e-01],\n                        [-5.4145e-02, -1.9373e-01,  6.1840e-02],\n                        [ 1.4348e-01, -4.9131e-02,  2.7658e-01]],\n              \n                       ...,\n              \n                       [[-1.7969e-01,  1.2212e-01, -1.9483e-01],\n                        [ 7.2463e-02, -9.8624e-02,  1.2995e-02],\n                        [-1.2093e-01, -1.6275e-01,  3.7400e-03]],\n              \n                       [[ 1.7423e-01,  2.3300e-02, -2.9325e-01],\n                        [-9.7852e-02, -5.7938e-02, -1.7732e-01],\n                        [-3.9809e-02, -2.7977e-01,  3.4336e-01]],\n              \n                       [[-2.8507e-02, -1.1469e-01, -2.3468e-02],\n                        [-2.9424e-01, -1.1742e-01, -2.9796e-01],\n                        [-8.9552e-02, -3.6888e-01, -1.1722e-01]]],\n              \n              \n                      [[[-3.4097e-01, -2.0472e-02, -3.3559e-02],\n                        [-2.1509e-01, -3.2666e-02,  1.7774e-01],\n                        [-1.3413e-01,  1.2593e-01,  2.8927e-02]],\n              \n                       [[-7.4076e-02, -2.8626e-01,  2.9195e-01],\n                        [-1.1732e-01, -1.2469e-01, -1.0873e-01],\n                        [ 2.4902e-02, -2.2352e-01, -1.1667e-02]],\n              \n                       [[-9.9617e-02,  2.3816e-01,  5.8085e-02],\n                        [-1.7305e-01,  2.5215e-01, -1.0332e-01],\n                        [-1.9872e-01, -2.9875e-02, -1.1786e-01]],\n              \n                       ...,\n              \n                       [[-1.5669e-01, -1.7912e-01, -2.7470e-01],\n                        [-2.7574e-01, -1.9388e-01, -1.3054e-01],\n                        [-3.1160e-01, -2.9463e-01, -7.3402e-02]],\n              \n                       [[-2.4325e-01, -4.5575e-02,  1.5656e-01],\n                        [-8.6813e-02,  5.9631e-02,  1.9557e-02],\n                        [ 7.9436e-03, -2.1456e-02,  2.1046e-02]],\n              \n                       [[-1.7177e-01, -7.8492e-02, -3.3288e-01],\n                        [-6.6999e-02, -1.0349e-01, -6.7128e-02],\n                        [-8.2018e-02, -2.2507e-02,  9.5441e-02]]],\n              \n              \n                      [[[-2.9090e-01, -1.3008e-01,  1.7609e-01],\n                        [-9.5815e-02, -1.5515e-01,  8.2584e-02],\n                        [-4.8893e-01, -2.0081e-01, -5.9667e-03]],\n              \n                       [[-2.4109e-02, -5.4721e-02, -3.2236e-01],\n                        [-2.4098e-01, -1.2562e-01, -1.2048e-01],\n                        [ 3.3588e-02, -4.9425e-01,  9.1871e-02]],\n              \n                       [[ 2.1242e-01,  2.1037e-01, -9.0084e-02],\n                        [-2.7277e-01, -1.2811e-01,  4.0071e-02],\n                        [-4.0113e-01,  5.4264e-02, -1.3007e-01]],\n              \n                       ...,\n              \n                       [[-3.1572e-01,  5.0079e-02, -1.9160e-01],\n                        [-1.9430e-01,  1.2746e-01,  3.5392e-02],\n                        [-1.8494e-01,  2.4142e-01, -4.3591e-02]],\n              \n                       [[ 4.0189e-01, -1.8748e-02,  2.1586e-01],\n                        [ 9.3946e-03, -1.2519e-02, -5.0839e-02],\n                        [-7.3023e-02, -6.9427e-02, -1.2437e-01]],\n              \n                       [[-3.1375e-02,  2.0483e-01,  1.5842e-01],\n                        [-8.4840e-02,  1.6326e-01,  1.6798e-02],\n                        [-1.0161e-01, -4.4391e-01,  9.3768e-02]]]], device='cuda:0')),\n             ('layer3.0.bn2.weight',\n              tensor([0.8520, 0.8365, 0.8512, 0.8077, 0.8577, 0.7588, 1.0765, 0.8343, 0.6375,\n                      1.0637, 0.5137, 0.4976, 1.0262, 0.7436, 0.7039, 1.0567, 0.7781, 0.9091,\n                      0.7507, 0.7161, 0.7664, 0.6229, 0.8814, 0.9317, 0.4193, 0.5413, 1.4231,\n                      0.7167, 0.7325, 0.8737, 0.8346, 0.8459, 1.1106, 0.6734, 0.9860, 0.6758,\n                      0.7369, 0.9277, 0.4721, 0.6426, 0.7169, 1.0188, 0.5616, 0.6689, 0.6521,\n                      0.5908, 0.9935, 0.8372, 0.9706, 0.7241, 0.8175, 0.4794, 0.6389, 0.8688,\n                      0.6684, 0.5055, 0.6295, 1.0287, 0.9587, 1.1466, 0.9259, 0.8691, 0.7938,\n                      0.4069, 0.6998, 0.7511, 0.8463, 0.8496, 0.8063, 0.8493, 0.5630, 0.9451,\n                      0.4806, 0.8051, 0.8118, 0.5098, 0.7266, 1.1254, 0.8024, 0.8656, 0.4464,\n                      0.8688, 0.8223, 0.5143, 0.7426, 0.5840, 0.5471, 0.7104, 0.7689, 0.6591,\n                      0.8024, 0.6308, 1.0308, 0.7302, 0.6819, 0.8885, 0.8568, 0.7586, 0.5190,\n                      0.6379, 0.8658, 0.9211, 0.8894, 0.6258, 0.6403, 0.5667, 0.5497, 0.6291,\n                      1.0335, 0.6838, 1.2905, 1.0199, 1.2895, 0.6210, 0.6590, 0.7918, 0.8097,\n                      0.8955, 0.6602, 0.7902, 0.6704, 0.9999, 0.8135, 0.8618, 0.9349, 0.5648,\n                      0.9867, 0.7164, 0.9664, 0.6511, 0.7286, 0.7909, 0.8361, 0.9220, 0.5892,\n                      0.9790, 0.9363, 0.9659, 0.7658, 1.1524, 0.6847, 0.9920, 0.5040, 1.1126,\n                      0.6476, 0.3695, 0.9410, 0.8221, 0.9436, 1.0532, 0.8877, 0.6706, 0.8156,\n                      0.6727, 0.7338, 0.9339, 0.9039, 0.5887, 0.8461, 0.8695, 0.7140, 0.6702,\n                      1.2431, 0.5442, 1.0030, 0.6829, 0.6069, 0.5890, 0.9551, 0.6982, 0.9195,\n                      0.4497, 0.6654, 1.0488, 0.5797, 0.8441, 0.5907, 0.7864, 0.9364, 1.0567,\n                      0.5989, 1.1802, 0.7947, 0.7926, 1.2462, 0.9424, 0.7994, 1.0062, 0.8654,\n                      1.1065, 0.9886, 0.9292, 0.9084, 0.8085, 0.7033, 0.7076, 1.3146, 0.6361,\n                      1.0322, 1.0362, 1.1639, 0.8780, 0.7070, 0.8213, 0.5242, 0.7555, 0.7598,\n                      0.8090, 0.8028, 0.8703, 0.5844, 1.0491, 0.7302, 0.8428, 0.7611, 0.5643,\n                      0.7566, 0.7220, 0.9313, 0.8555, 1.0256, 1.1022, 0.6734, 0.5002, 0.7006,\n                      1.1080, 0.7364, 0.5937, 0.7736, 0.4310, 0.8734, 0.7107, 0.4551, 1.0945,\n                      0.7955, 1.1608, 0.8796, 0.7607, 0.6164, 0.5521, 0.9266, 0.4557, 0.8773,\n                      0.9311, 0.7675, 0.7862, 0.4866, 0.6972, 0.7976, 0.8554, 1.1520, 0.8904,\n                      1.0672, 0.7699, 0.6318, 0.8334], device='cuda:0')),\n             ('layer3.0.bn2.bias',\n              tensor([-0.1598, -0.5020, -0.3155, -0.3373, -0.3653, -0.2048, -0.0854, -0.0590,\n                      -0.2315, -0.2423, -0.3688, -0.3579, -0.1339, -0.2603, -0.2034, -0.2973,\n                      -0.3014, -0.2065, -0.2660, -0.3527, -0.2248, -0.4519, -0.1645, -0.2955,\n                      -0.2540, -0.2666, -0.2765, -0.3058, -0.2859, -0.2838, -0.3805, -0.1928,\n                      -0.1489, -0.3225, -0.2606, -0.3590, -0.3775, -0.3338, -0.4669, -0.2022,\n                      -0.2932, -0.3413, -0.3683, -0.4085, -0.2868, -0.4991, -0.2058, -0.2613,\n                      -0.0839, -0.3820, -0.2747, -0.3277, -0.2588, -0.3615, -0.2582, -0.5516,\n                      -0.4159, -0.3339, -0.1113, -0.5540, -0.1968, -0.2065, -0.2999, -0.3183,\n                      -0.3719, -0.2590, -0.2928, -0.2906, -0.3611, -0.3589, -0.2565, -0.3915,\n                      -0.4936, -0.3145, -0.3409, -0.5051, -0.3106, -0.1937, -0.1833, -0.4259,\n                      -0.2671, -0.0213, -0.0480, -0.2678, -0.2239, -0.2247, -0.4833, -0.2741,\n                      -0.2628, -0.2860, -0.3714, -0.4992, -0.0241, -0.2949, -0.2723, -0.1069,\n                      -0.0491, -0.2731, -0.2656, -0.3144, -0.3158, -0.1934, -0.4995, -0.4504,\n                      -0.2895, -0.3173, -0.3787, -0.2003, -0.2767, -0.4729, -0.0396, -0.2869,\n                      -0.2156, -0.3900, -0.3241, -0.1065, -0.3935, -0.0775, -0.2297, -0.2795,\n                      -0.3000, -0.0593, -0.2699, -0.1157, -0.2184, -0.4691, -0.2285, -0.2837,\n                      -0.3087, -0.2945, -0.4994, -0.3198, -0.3566, -0.2660, -0.3853, -0.3551,\n                      -0.4367, -0.2355, -0.2520, -0.3040, -0.3676, -0.2999, -0.3887, -0.1437,\n                      -0.3174, -0.2960, -0.2199, -0.0294, -0.1826, -0.3475, -0.5019, -0.2671,\n                      -0.1684, -0.5330, -0.2976, -0.3120, -0.3235, -0.3301, -0.4263, -0.1808,\n                      -0.2569, -0.3319, -0.2428, -0.3011, -0.1274, -0.3338, -0.4148, -0.2811,\n                      -0.0906, -0.1335, -0.4549, -0.3030, -0.2185, -0.2085, -0.3071, -0.1888,\n                      -0.2945, -0.3181, -0.2550, -0.1858, -0.2753, -0.2285, -0.2684, -0.0734,\n                      -0.2103, -0.2634, -0.1734, -0.3141, -0.1835, -0.2596, -0.3595, -0.3341,\n                      -0.4142, -0.1692, -0.3745, -0.3288, -0.1361, -0.3446, -0.3437, -0.1796,\n                      -0.2345, -0.0325, -0.3363, -0.0846, -0.5425, -0.3133, -0.2466, -0.1898,\n                      -0.3338, -0.1465, -0.2444, -0.3874, -0.3227, -0.4193, -0.1250, -0.5395,\n                      -0.2608, -0.4238, -0.1146, -0.1087, -0.1033, -0.3587, -0.3139, -0.2272,\n                      -0.2677, -0.2635, -0.2974, -0.1805, -0.2290, -0.2758, -0.2828, -0.3772,\n                      -0.2408, -0.1249, -0.2319, -0.1282, -0.3765, -0.0712, -0.3245, -0.3796,\n                      -0.3432, -0.1864, -0.2523, -0.2239, -0.3583, -0.1181, -0.2984, -0.3103,\n                      -0.2460, -0.0957, -0.1480, -0.1370, -0.2615, -0.3192, -0.3645, -0.2009],\n                     device='cuda:0')),\n             ('layer3.0.bn2.running_mean',\n              tensor([-2.4758e+01, -6.3442e+00, -1.8578e+01, -1.5976e+01, -1.7849e+01,\n                       1.0213e+01,  2.0361e+01,  1.4116e+01,  1.8443e+01,  6.3455e+00,\n                       1.4031e+01,  2.5341e+01, -1.1676e+01, -1.4854e+01,  1.3303e+01,\n                       7.8322e+00, -6.5360e+00,  5.7958e+00, -6.7851e+00, -1.5054e+01,\n                      -6.5041e+00,  1.4160e+00,  2.2691e+01, -1.7349e+01,  1.5426e+01,\n                       6.0627e+00, -1.2863e+01, -3.4626e+00,  1.0784e+01, -9.3960e+00,\n                       5.2366e-01, -2.0450e+01, -3.1472e+01, -1.5021e+01, -5.7713e+00,\n                      -9.3679e+00, -7.1271e+00,  3.7397e+00,  2.2377e+01,  7.9637e+00,\n                       2.6625e+01, -1.5435e+01,  1.9046e+01, -7.1221e+00,  1.0353e+00,\n                       2.4860e+01, -1.6707e+01,  2.0474e+01, -1.6947e+01, -1.4156e+01,\n                      -2.9264e+01,  1.1715e+01, -6.4170e+00,  8.6595e+00,  2.3442e+01,\n                       7.1793e+00,  7.8664e+00, -2.1993e+01, -1.6784e+01, -4.4399e+00,\n                       1.2478e+01, -1.9032e+01, -1.6022e+01,  6.6545e+00,  2.4590e+01,\n                      -1.3103e+01, -9.5887e+00, -1.0591e+01, -8.3405e+00,  8.0747e+00,\n                       1.3224e+01, -1.2772e+01,  9.6769e+00, -1.4127e+01, -2.0075e+01,\n                       9.1103e+00, -1.8865e+01, -1.7719e+01,  4.7850e+00,  4.5002e+00,\n                      -1.1953e+01,  1.2606e+01, -2.2754e+01,  1.6318e+01,  3.1235e-01,\n                       7.7601e-03, -1.8053e+01, -2.0910e+01, -1.3877e+01, -2.1772e+01,\n                      -3.9982e+00,  1.6119e+01, -2.2855e+01, -2.1224e+01,  1.5171e+01,\n                       5.2575e+00,  1.4987e+01, -9.0895e+00, -2.0040e+00,  1.9025e+01,\n                      -1.5407e+01, -2.2596e+01,  4.8311e+00,  3.2342e+01, -3.5427e+00,\n                       6.3242e+00, -6.8427e+00, -2.8419e+01, -8.8364e+00, -6.2246e+00,\n                      -2.7237e+01, -2.6794e+01, -1.4039e+01, -6.5083e+00, -2.0081e+01,\n                       1.8223e+01,  6.4026e+00,  9.4650e+00,  1.8013e+01,  7.9916e-01,\n                      -9.6391e+00, -1.6316e+00, -1.3343e+01,  2.1053e+01, -2.0955e+01,\n                      -1.5891e+01, -2.4441e+01, -1.0891e+01, -1.1104e+01, -1.6456e+01,\n                      -1.2898e+01, -1.9691e+01, -2.3848e+01, -1.5301e+01,  1.4794e+01,\n                      -2.2768e+01, -8.8114e+00, -2.9618e+00,  3.3229e+00, -2.2142e+01,\n                      -1.7794e+01, -1.6511e+01,  1.1517e+01, -2.3922e+01,  1.2077e+01,\n                       4.3323e+00, -1.9890e+01, -1.4845e+01, -1.3898e+01,  1.4612e+01,\n                      -1.0062e+01, -2.4435e+01, -1.1580e+01,  4.4553e+00, -1.1582e+01,\n                      -2.9911e-01, -1.3007e+01, -2.6935e+01, -4.8886e+00, -1.9887e+01,\n                      -2.5029e+01, -1.1596e+01, -8.8823e+00,  1.7122e+01, -2.5383e+01,\n                      -1.9457e+01, -1.2426e+01,  8.2130e+00, -6.6834e+00,  1.0533e+01,\n                      -1.5742e+01,  1.4859e+01, -1.1814e+01,  6.2596e+00,  1.9456e+01,\n                      -2.3707e+01,  5.5991e+00,  1.3099e+01, -2.1741e+01, -2.2282e+01,\n                       2.7881e+01, -2.0461e+01, -1.3403e+01,  1.5606e+01, -2.0249e+01,\n                      -1.1425e+01,  1.6507e+01, -1.8439e+01, -7.6838e+00, -1.6912e+01,\n                      -8.6801e+00, -1.3707e+01, -4.4080e+00,  1.7435e+01, -2.1570e+01,\n                      -1.5364e+01, -1.8935e+01, -2.3393e+01, -1.4547e+01, -2.2797e+01,\n                      -1.6262e+01,  1.5142e+01, -2.3675e+01,  1.8572e+01, -2.7913e+00,\n                      -1.3088e+01, -2.4095e+01,  1.5558e+01, -1.7260e+01,  8.0507e+00,\n                      -1.6823e+01, -1.1199e+01, -1.1528e+01, -1.4389e+01, -9.5350e+00,\n                       2.1692e+01,  1.7444e+01,  1.4535e+01,  2.0195e+01,  3.6815e+00,\n                       9.7101e+00, -1.8783e+01, -2.1614e+01,  1.2013e+01, -4.6871e+00,\n                       8.6318e+00, -5.9180e+00,  1.7896e+01, -2.4990e+01,  1.9056e+01,\n                      -1.5950e+01,  1.5696e+01,  1.0640e+01,  3.2467e+00, -2.3892e+01,\n                      -2.3236e+01, -1.7056e+01, -8.1146e+00,  2.5110e+01, -2.0973e+01,\n                      -4.4704e+00,  1.3234e+01, -1.7890e+01, -2.3120e+01, -2.6535e+01,\n                       1.6841e+01,  3.0760e+01,  2.4662e+01, -1.9231e+01,  1.9891e+01,\n                      -2.1930e+01, -2.4090e+01, -2.1840e+01, -2.1737e+01, -6.2089e+00,\n                      -1.6809e+01], device='cuda:0')),\n             ('layer3.0.bn2.running_var',\n              tensor([1433.4554,  177.6398,  240.9722,  926.1517, 1432.1996,  836.7487,\n                      1293.1550, 1792.7281, 1688.7617,  553.7997, 2259.7527, 1891.1310,\n                       481.4340,  273.7928,  737.0355,  804.7105,  912.3840,  564.3579,\n                       466.6146,  705.9650,  892.4883,  112.3882,  895.0490,  808.2080,\n                      1171.3440, 1572.9564,  629.8215,  977.6778, 1020.0248, 1091.4744,\n                       935.2924,  941.8453,  983.5978, 1169.4380,  780.7299,  824.7579,\n                       258.0398,  753.4781, 2499.6077, 1747.1050, 1788.6089, 2014.1261,\n                      1458.3453, 1402.8951, 1535.5773, 1189.4138, 1566.7196, 1160.3906,\n                       792.9619, 1537.2100,  716.8909,  840.0063,  559.2241,  398.4052,\n                      1393.9928,  498.5945,  433.1846, 1280.1465,  653.4949,  445.3286,\n                       990.2741,  749.2499,  446.2900,  928.4023, 1649.5068,  636.1223,\n                       171.3752,  559.4837,  629.9456,  594.3685, 2069.4321, 1351.4855,\n                       289.9120,  223.5579, 1336.3705,  379.0641,  832.7574, 1015.6523,\n                      1155.3931,  601.7694,  742.4855, 1688.7809,  982.9873, 1623.0563,\n                       754.8444,  732.1833, 1743.2029,  653.8849,  887.3058,  973.3318,\n                       315.2994,  631.6400,  769.4228,  780.9323, 1609.1110,  531.0247,\n                      1741.3949,  304.6711, 1371.2533, 1806.2446, 1755.0018, 1283.8333,\n                       165.7259, 1009.2278, 1050.0765,  953.7764,  352.2543,  660.3998,\n                      1483.0177, 1282.9241, 1119.5859, 2147.4302,  627.3945,  500.3334,\n                       204.1759, 1649.3477,  813.1140,  476.6204, 1718.5076,  123.7189,\n                       375.6464,  281.5821,  670.1432,  963.9521,  329.6804, 1490.5767,\n                       658.8610,  666.2631, 1126.9397,  165.5354, 2236.5039, 1184.5121,\n                       342.4038,  328.4664, 1029.2136, 1516.1935, 1369.5862,  309.6919,\n                       672.5437, 1790.6024, 1455.2495,  176.3390, 1030.1138, 1032.9536,\n                       478.8832, 1451.6769, 1475.4384,  615.1050,  153.2435,  701.0204,\n                      1095.8461, 1247.6892,  767.9518,  415.2315,  728.8824,  180.6409,\n                       417.6985, 2549.8738, 1331.7418,  844.5767,  748.4318,  909.7653,\n                       768.7909, 1707.8251, 1748.7250, 1286.4336, 1067.2816,  456.3517,\n                       806.1841, 1972.0718, 1697.3917, 2426.5120, 1059.2993,  352.0782,\n                      1325.9542,  356.7975,  865.2295,  625.1159, 1806.2089, 1190.2330,\n                      2009.9307,  641.4055, 1254.0511, 1880.7087,  933.8651, 1087.4731,\n                      1069.7615, 2170.9307,  338.7489,  667.7463, 2345.5388,  641.1356,\n                       842.1234,  801.4702, 1338.6127,  326.7577,  755.1907, 1593.4871,\n                      2030.1967, 1572.5352,  791.5233, 1639.0793, 2015.3229, 2003.2189,\n                       363.0366,  824.6001, 2295.7852,  672.0762, 1739.7899,  718.4902,\n                      1175.4700,  928.2506, 1075.1698, 1546.1777,  727.5210,  327.2011,\n                       953.6828, 1098.4381,  880.5837,  320.0925,  735.5041, 1338.2277,\n                       959.7216, 1803.2217,  354.6313,  647.2289,  543.6601, 2590.3525,\n                       834.0538, 1997.0269, 1016.6661, 1149.1375, 1233.6085,  321.0318,\n                       575.5513,  670.3523,  424.5695, 1092.7062, 3482.4707, 1802.6674,\n                       548.5134, 1833.2646,  293.9687,  907.6639, 1044.7228, 1793.9463,\n                      1964.8024, 2044.7417,  700.2903, 1899.4182, 1530.3385, 1167.2479,\n                       496.3497,  246.5031,  612.9998,  167.3122], device='cuda:0')),\n             ('layer3.0.bn2.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer3.1.conv1.weight',\n              tensor([[[[ 0.0275, -0.0842, -0.0877],\n                        [ 0.0525, -0.0382, -0.0762],\n                        [-0.0128, -0.1855, -0.1236]],\n              \n                       [[ 0.1359,  0.0770,  0.0288],\n                        [-0.1301, -0.0779,  0.1282],\n                        [-0.0129, -0.1092, -0.0116]],\n              \n                       [[ 0.2080,  0.1985,  0.1136],\n                        [ 0.0287, -0.0052,  0.0399],\n                        [-0.0340, -0.1546, -0.0324]],\n              \n                       ...,\n              \n                       [[ 0.1087,  0.1489,  0.0797],\n                        [-0.1196, -0.0441, -0.1461],\n                        [-0.1562, -0.1738,  0.0402]],\n              \n                       [[ 0.0155, -0.0207, -0.0037],\n                        [-0.0155,  0.0236,  0.0185],\n                        [-0.0188, -0.1640, -0.1905]],\n              \n                       [[ 0.1791,  0.2706, -0.0352],\n                        [ 0.0206,  0.0680,  0.1590],\n                        [-0.2992, -0.1326, -0.0759]]],\n              \n              \n                      [[[ 0.1269, -0.2945, -0.0434],\n                        [ 0.1453, -0.0129, -0.3134],\n                        [ 0.0193, -0.0149,  0.0144]],\n              \n                       [[ 0.1531, -0.0687, -0.1398],\n                        [-0.0116, -0.0797,  0.3249],\n                        [-0.0321,  0.1436, -0.1178]],\n              \n                       [[ 0.3442, -0.1167, -0.5459],\n                        [ 0.2661, -0.0372, -0.2242],\n                        [ 0.0732,  0.0761, -0.0819]],\n              \n                       ...,\n              \n                       [[ 0.3139, -0.0176, -0.4671],\n                        [ 0.2121, -0.3507, -0.2657],\n                        [ 0.1787,  0.0720, -0.0045]],\n              \n                       [[ 0.0291,  0.0530, -0.0113],\n                        [-0.0153,  0.0320, -0.2691],\n                        [-0.0406,  0.0695, -0.0829]],\n              \n                       [[ 0.1837,  0.1134, -0.0165],\n                        [ 0.1681, -0.1264,  0.1078],\n                        [ 0.1079,  0.0347,  0.0088]]],\n              \n              \n                      [[[-0.0233, -0.0299, -0.0227],\n                        [ 0.1366, -0.0063,  0.0114],\n                        [ 0.0765, -0.0358, -0.0138]],\n              \n                       [[-0.1122, -0.0133,  0.0194],\n                        [ 0.0374,  0.2325, -0.0536],\n                        [ 0.0460,  0.1395,  0.0161]],\n              \n                       [[-0.2046, -0.4913, -0.0687],\n                        [ 0.5400, -0.0591, -0.0850],\n                        [ 0.3938,  0.1316, -0.0603]],\n              \n                       ...,\n              \n                       [[-0.2779, -0.3552, -0.0335],\n                        [ 0.5666,  0.0704, -0.0411],\n                        [ 0.5377,  0.0030, -0.0554]],\n              \n                       [[-0.0102, -0.0106,  0.0070],\n                        [ 0.0115,  0.0121,  0.0405],\n                        [-0.0445, -0.0302, -0.1256]],\n              \n                       [[-0.3732, -0.2983, -0.1807],\n                        [ 0.3340, -0.2104, -0.3842],\n                        [ 0.3637, -0.0109, -0.0672]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0693, -0.0757,  0.1491],\n                        [ 0.0669, -0.3260,  0.0149],\n                        [ 0.0922, -0.0667, -0.0130]],\n              \n                       [[-0.2538, -0.1283,  0.0207],\n                        [ 0.2859,  0.0849, -0.0079],\n                        [ 0.0891,  0.1589,  0.0609]],\n              \n                       [[-0.4085, -0.4252, -0.1250],\n                        [ 0.4578, -0.0435, -0.1264],\n                        [ 0.2850,  0.0906, -0.1731]],\n              \n                       ...,\n              \n                       [[-0.5354, -0.3256, -0.0748],\n                        [ 0.5681,  0.0256, -0.0914],\n                        [ 0.5638, -0.0538, -0.1063]],\n              \n                       [[-0.0140, -0.0075,  0.0009],\n                        [ 0.0283, -0.0429, -0.0481],\n                        [-0.0181, -0.0233, -0.0278]],\n              \n                       [[-0.4245, -0.2478, -0.0833],\n                        [ 0.3619, -0.2555, -0.1322],\n                        [ 0.3155, -0.0517, -0.1177]]],\n              \n              \n                      [[[-0.1072, -0.0546, -0.0598],\n                        [ 0.0236, -0.2433, -0.0453],\n                        [ 0.1034, -0.0550, -0.0750]],\n              \n                       [[-0.0234, -0.1367, -0.0261],\n                        [ 0.1595, -0.0739,  0.0715],\n                        [ 0.0718,  0.1458, -0.1529]],\n              \n                       [[-0.3922, -0.4321, -0.0986],\n                        [ 0.4215, -0.0531, -0.1443],\n                        [ 0.2460,  0.0759, -0.1659]],\n              \n                       ...,\n              \n                       [[-0.2970, -0.4113, -0.0507],\n                        [ 0.5262,  0.0263, -0.1188],\n                        [ 0.5641, -0.0495, -0.0534]],\n              \n                       [[ 0.0126, -0.0369, -0.0270],\n                        [ 0.0128,  0.0940, -0.0092],\n                        [-0.0036, -0.0172, -0.1022]],\n              \n                       [[-0.2450, -0.3527, -0.2200],\n                        [ 0.2615, -0.1401, -0.1424],\n                        [ 0.3664, -0.0355, -0.0574]]],\n              \n              \n                      [[[-0.0006, -0.0240,  0.0016],\n                        [ 0.0669, -0.1367, -0.3061],\n                        [ 0.0407,  0.0270, -0.0091]],\n              \n                       [[-0.3156, -0.2236, -0.0310],\n                        [ 0.1033, -0.0325,  0.0303],\n                        [ 0.0189, -0.0264,  0.1924]],\n              \n                       [[-0.2562, -0.4027, -0.1338],\n                        [ 0.2901,  0.0220,  0.0864],\n                        [ 0.1805,  0.0550,  0.0196]],\n              \n                       ...,\n              \n                       [[-0.2639, -0.4353, -0.1316],\n                        [ 0.3717,  0.0936,  0.1156],\n                        [ 0.3232,  0.0575, -0.0342]],\n              \n                       [[ 0.0043, -0.0238, -0.0719],\n                        [-0.0012, -0.0329, -0.1415],\n                        [ 0.0098, -0.0277, -0.0015]],\n              \n                       [[-0.3677, -0.6046, -0.2271],\n                        [ 0.2458,  0.0907, -0.0174],\n                        [ 0.1961, -0.0025, -0.0355]]]], device='cuda:0')),\n             ('layer3.1.bn1.weight',\n              tensor([0.8242, 0.5493, 0.9728, 0.8117, 0.7915, 0.9908, 1.0298, 0.7691, 0.9707,\n                      0.8920, 0.7693, 0.3524, 0.9730, 1.0345, 1.0399, 0.6215, 0.9072, 0.8339,\n                      0.7474, 0.8628, 0.9555, 0.9115, 0.6898, 1.0393, 1.0748, 0.4121, 0.7376,\n                      0.6046, 0.7669, 0.7635, 1.0306, 0.9377, 0.6521, 1.0199, 0.8784, 0.9751,\n                      0.9767, 0.6121, 0.8679, 0.6728, 0.9861, 0.8025, 0.8392, 0.8762, 0.7752,\n                      1.0832, 0.8798, 0.8332, 0.8458, 1.0929, 0.9665, 0.6958, 1.0562, 0.9539,\n                      0.8841, 0.6554, 0.8135, 0.9780, 0.8052, 0.7964, 0.9855, 0.8700, 0.7963,\n                      0.5476, 0.9171, 0.8452, 0.6669, 0.9784, 0.6454, 1.0512, 0.8993, 0.5171,\n                      0.9159, 0.8936, 0.9922, 0.5225, 1.0381, 0.8707, 1.0577, 0.7093, 0.8646,\n                      1.0241, 0.8670, 0.9670, 0.7909, 0.8790, 0.8563, 0.9255, 0.7335, 0.4223,\n                      0.8804, 0.7263, 0.6571, 0.8527, 0.7150, 1.0775, 0.9859, 0.6505, 0.9409,\n                      1.0353, 0.8358, 0.7620, 0.7725, 0.6959, 0.8026, 0.6929, 1.0016, 1.0010,\n                      0.9903, 0.9679, 0.8884, 0.9985, 0.9334, 1.0277, 0.9990, 1.0689, 0.5343,\n                      0.7623, 0.6774, 0.8390, 0.9495, 1.0276, 1.0193, 0.7473, 1.0320, 0.9672,\n                      1.0782, 1.1075, 0.8519, 0.8127, 0.6535, 0.8533, 1.0554, 0.8372, 0.7573,\n                      1.0057, 0.5780, 0.7762, 0.8693, 0.8426, 0.5465, 0.9997, 0.9801, 0.7930,\n                      0.7570, 0.8273, 0.5891, 1.1513, 1.0655, 0.6963, 1.0337, 0.9063, 0.4749,\n                      0.7295, 0.6238, 0.7661, 0.6709, 0.8617, 0.9748, 0.7253, 0.5860, 1.3309,\n                      0.9185, 0.7138, 0.9565, 0.7558, 0.7394, 0.2282, 1.0083, 0.9359, 0.9130,\n                      0.5721, 0.7398, 0.6440, 0.8245, 0.9503, 0.8587, 0.9770, 0.9133, 0.9672,\n                      1.0544, 0.8697, 0.7778, 1.0020, 1.0475, 0.5058, 0.9584, 0.7509, 0.8509,\n                      1.0435, 0.7607, 0.9064, 1.0964, 1.0619, 1.0145, 0.9156, 0.3763, 0.8540,\n                      0.8837, 0.9483, 0.6130, 0.8648, 0.9702, 0.8643, 0.9144, 0.7568, 0.7291,\n                      0.9327, 0.7831, 0.9802, 0.8232, 0.9089, 0.7778, 1.0816, 1.0008, 0.8064,\n                      0.8461, 0.7367, 0.8592, 0.9769, 0.8174, 0.7100, 1.0612, 0.9969, 0.9227,\n                      0.9484, 0.9067, 0.5211, 0.7442, 0.7267, 0.9709, 0.4787, 0.7122, 1.0394,\n                      0.9069, 0.7176, 0.6703, 0.8146, 0.8117, 0.8641, 1.1497, 0.8442, 0.7402,\n                      0.9033, 0.0932, 0.9453, 0.9826, 0.8723, 0.6766, 0.6877, 0.5660, 0.7209,\n                      0.5220, 1.0350, 0.8616, 0.4805], device='cuda:0')),\n             ('layer3.1.bn1.bias',\n              tensor([ 0.0124,  0.0097,  0.0256,  0.1019,  0.1248,  0.0445,  0.0091,  0.0866,\n                      -0.0133, -0.0502,  0.0342, -0.0728,  0.0327,  0.2200,  0.0857, -0.0558,\n                       0.0900, -0.0420,  0.0540,  0.1305,  0.0087,  0.1451,  0.0433,  0.0142,\n                      -0.0964, -0.0779, -0.1388, -0.0119,  0.0865,  0.1037,  0.1096, -0.0273,\n                      -0.1311, -0.0222, -0.0739, -0.0031,  0.1119, -0.0467, -0.0924, -0.0190,\n                       0.2367,  0.0276, -0.0520, -0.0189, -0.0922,  0.0864,  0.0814,  0.0560,\n                      -0.0540,  0.0823,  0.0263, -0.0361,  0.0494,  0.0027,  0.1189,  0.0026,\n                       0.1519,  0.2275,  0.1602,  0.0623,  0.0583,  0.1056,  0.0558,  0.0110,\n                       0.1573, -0.0996,  0.0313,  0.0394, -0.0333,  0.0719, -0.0049, -0.2043,\n                       0.0123, -0.0202,  0.0318,  0.0621, -0.1393,  0.1068, -0.1340,  0.0757,\n                      -0.0215, -0.0686,  0.1235, -0.0017, -0.0899,  0.1270,  0.1003,  0.1866,\n                       0.0383, -0.0366, -0.0297,  0.0949,  0.0289,  0.1481,  0.0307,  0.2182,\n                      -0.1519, -0.0350, -0.0358,  0.0201,  0.0299,  0.0679,  0.0866, -0.0470,\n                       0.0452, -0.0588,  0.0425,  0.0299,  0.0395,  0.0349, -0.1248,  0.0739,\n                       0.1722,  0.0240,  0.1136,  0.1110, -0.0958,  0.0479,  0.0401,  0.0575,\n                       0.1551,  0.0901, -0.0319, -0.0822, -0.0281,  0.0019,  0.0698,  0.0161,\n                       0.0800, -0.0700, -0.0105,  0.0084,  0.2085, -0.0278,  0.0642,  0.0327,\n                      -0.0298, -0.0601,  0.1035,  0.1087, -0.0715, -0.1191,  0.0017,  0.0813,\n                      -0.0430, -0.1072, -0.0860,  0.0366,  0.0553,  0.0246,  0.0299,  0.1507,\n                      -0.0653,  0.0499,  0.0669,  0.1284,  0.0032,  0.0204, -0.0273, -0.0026,\n                      -0.0105,  0.1421, -0.0027, -0.1274,  0.0450,  0.0542, -0.0744, -0.0590,\n                      -0.1040,  0.0198, -0.0298, -0.1246,  0.0554, -0.0182,  0.0653, -0.0211,\n                      -0.0575,  0.0143, -0.0163,  0.0081,  0.0481,  0.1182,  0.0234,  0.0287,\n                       0.0436, -0.1412,  0.0886,  0.1339,  0.1558, -0.0130,  0.0231,  0.1019,\n                       0.0649, -0.0117,  0.0293,  0.1613, -0.0959, -0.0660,  0.1193,  0.1372,\n                      -0.0339,  0.1110,  0.0237,  0.0768, -0.0373,  0.0923,  0.0714,  0.0272,\n                       0.0361,  0.0458,  0.1097, -0.0582,  0.1335, -0.0335, -0.0143,  0.0535,\n                      -0.0205,  0.0727,  0.0769,  0.0898,  0.1198,  0.0090,  0.0899,  0.0154,\n                       0.1466,  0.1773,  0.0454, -0.0726,  0.0061,  0.0452,  0.2035, -0.0401,\n                      -0.3079,  0.0211, -0.0517,  0.0339, -0.1366, -0.1931,  0.1197,  0.0675,\n                       0.0682, -0.0710, -0.0221,  0.0658, -0.0659,  0.0878, -0.0460, -0.0139,\n                       0.0080,  0.0310, -0.0034,  0.0632, -0.0818,  0.0998, -0.0134, -0.0763],\n                     device='cuda:0')),\n             ('layer3.1.bn1.running_mean',\n              tensor([-10.5601,  -7.0109,  -8.5078, -15.8763, -12.9788,  -9.8596,  -5.6416,\n                       -9.3432, -10.9132, -11.9470, -11.4099, -11.8802,  -8.3739, -17.0314,\n                       -7.9147, -12.4695, -11.4749,  -9.9012,  -7.1405, -11.5922,  -8.6660,\n                      -11.3434, -11.0895,  -6.2050,  -4.8962, -16.0144, -10.6865,  -9.6807,\n                      -17.4394, -13.5085,  -8.9492,  -9.9686,  -4.8008,  -4.9847,  -9.0954,\n                       -7.6115,  -8.9622, -14.3351,  -6.2359, -10.4197, -14.2525, -10.1990,\n                       -9.6988,  -6.7627, -11.5452,  -6.0558,  -5.9137, -16.9994,  -8.0502,\n                      -10.0304,  -8.7410, -10.8653,  -9.3049,  -9.5078, -11.8219, -13.3593,\n                      -13.9552, -10.9919, -15.1812, -15.9229,  -6.6420, -11.5410, -14.3830,\n                      -16.8308, -16.4062, -11.2297, -11.6413,  -8.7475, -11.7530, -10.6367,\n                       -8.8471, -10.2639, -12.5877,  -7.0119,  -9.7175,  -9.2422,  -6.8547,\n                      -14.5936,  -6.8332,  -9.0677, -11.2361,  -6.5937, -10.3538, -11.4375,\n                      -11.8543, -11.7040, -10.4732, -14.3528, -11.3002,  -8.3326,  -9.5758,\n                      -12.1160, -10.0870,  -8.3548, -12.6969,  -8.5292,  -8.7802, -11.8578,\n                       -8.4461,  -7.1654, -12.6965,  -8.9154, -10.9663,  -9.7896, -15.1586,\n                       -9.5987,  -9.2835,  -9.0822, -11.8964,  -9.0973,  -6.0069,  -8.3276,\n                      -18.3482,  -9.7670, -11.0989, -10.1206, -14.4385, -10.2196,  -5.5881,\n                      -12.2648, -15.6224,  -6.7531,  -7.2444,  -8.9803,  -5.9050,  -7.5534,\n                       -7.3201,  -3.4979,  -9.7278,  -7.5282, -16.1786,  -8.1737,  -7.8114,\n                       -8.1212, -10.9984,  -8.1478, -17.1994,  -9.1077, -16.3678,  -9.3638,\n                      -13.8607,  -4.3000, -11.3755,  -8.0781, -10.3418,  -6.8467,  -5.9397,\n                       -6.6560,  -7.6198, -14.5674,  -9.2438, -17.8785, -15.1742, -14.8090,\n                      -10.7358, -15.2119,  -8.0009,  -9.3803,  -9.8460, -13.4004,  -6.4226,\n                       -4.1940,  -9.5884, -11.3713, -11.4928,  -9.8495,  -6.5524, -11.2203,\n                       -6.7120, -11.4947,  -6.1569, -11.2127, -11.9505, -14.6282,  -9.2165,\n                       -6.6125,  -7.4995,  -7.8908, -11.2468,  -6.9712,  -8.2495,  -8.7358,\n                      -13.6201,  -6.9912,  -8.3711, -16.3291,  -9.1803, -14.3971, -10.9389,\n                       -5.8137,  -8.4935,  -6.0042,  -7.9659,  -6.0226,  -6.9742, -15.2231,\n                      -14.1814, -10.9156,  -8.5932, -16.8644,  -7.6931, -11.3862,  -9.7314,\n                       -5.5798,  -8.3778,  -9.7779,  -7.2222,  -6.3859, -12.7653,  -8.3291,\n                      -12.2805,  -6.3260, -16.5981,  -4.7742,  -7.9138, -15.3771, -11.5028,\n                      -15.1103, -12.5367,  -9.8967,  -6.8092, -15.4498,  -8.7289, -10.1227,\n                       -7.4894, -13.1343,  -6.2256, -11.9218,  -9.4171,  -8.7530, -10.9514,\n                       -7.9715, -12.6617,  -7.4643,  -7.0607,  -8.3812, -10.5829,  -5.8824,\n                       -7.3131, -10.2105,  -4.3760,  -7.5961, -11.8214,  -9.1812, -10.7049,\n                      -10.8431,  -6.3321,  -6.7106, -13.0767, -11.7860, -14.7044, -13.8297,\n                      -10.8679,  -8.6428, -11.1377, -16.3403], device='cuda:0')),\n             ('layer3.1.bn1.running_var',\n              tensor([445.6662, 299.6186, 263.9027, 577.9957, 458.7096, 293.1259, 225.1081,\n                      382.3066, 247.8945, 252.4245, 526.9567, 422.2894, 252.3062, 628.0569,\n                       51.9584, 515.4226, 564.6435, 271.4881, 376.8589, 365.9488, 266.8186,\n                      423.4525, 526.7545, 235.3778, 120.5705, 571.4541, 264.6248, 438.4934,\n                      607.8984, 487.1487, 305.0629, 262.5525, 151.0482, 133.2682, 284.6821,\n                      259.1898, 195.8304, 704.5754, 188.6621, 439.1529, 453.0970, 383.1427,\n                      238.1279, 171.7805, 272.3075, 247.5111, 310.1860, 675.8784, 192.9070,\n                      259.9370, 187.5883, 516.1500, 266.7220, 297.1297, 598.9008, 565.5278,\n                      494.3920, 336.2380, 509.8270, 574.9277, 289.6980, 575.2211, 547.1913,\n                      630.9990, 535.9242, 307.0989, 392.4705, 296.4895, 519.4037, 236.4650,\n                      271.7543, 356.0619, 331.3998, 214.4388, 266.3866, 690.8796,  65.8070,\n                      536.7165,  65.3802, 379.0461, 242.7229, 129.6937, 307.1915, 251.4467,\n                      278.9666, 460.9330, 510.9585, 503.1442, 414.5081, 263.0345, 248.9476,\n                      413.0089, 433.8857, 284.9927, 497.6223, 195.5539,  79.5974, 602.2808,\n                      292.5945, 234.3910, 524.8987, 213.7229, 390.8149, 527.6545, 603.9077,\n                      375.4775, 264.2623, 228.1837, 281.8430, 252.5356, 251.0435, 339.3401,\n                      637.7871, 254.2476, 359.8693, 251.6642, 617.5187, 414.9092, 253.2933,\n                      528.0016, 551.0861, 275.0750, 116.9337, 176.6951, 179.9148, 233.5880,\n                      276.6930,  76.4884, 496.8430, 213.2988, 590.0699, 419.9561, 346.3776,\n                      182.0487, 494.0987, 293.4068, 651.7192, 252.7165, 632.9572, 356.8660,\n                      667.9546, 116.1035, 293.1538, 199.6961, 266.1458, 257.2835, 211.2390,\n                      252.8203, 253.9276, 517.7567, 241.7799, 642.3853, 470.6180, 571.0681,\n                      392.3199, 535.0067, 349.2774, 395.8744, 228.4443, 460.5255, 186.3269,\n                       48.0774, 277.7119, 278.2198, 281.3751, 433.9590, 142.8506, 538.2963,\n                      113.4865, 266.2662, 183.0944, 273.7714, 448.9257, 561.7773, 490.2241,\n                      125.4883, 237.7647, 187.7536, 255.3353, 180.3679, 285.2437, 403.2964,\n                      468.7838, 186.8140, 235.1851, 725.0082, 373.5446, 530.8503, 444.8865,\n                      223.4986, 396.9058, 288.2993, 251.1415, 139.6360, 279.6871, 489.9872,\n                      618.7816, 244.1918, 221.4483, 556.4470, 406.1793, 509.8155, 213.5579,\n                      322.8581, 286.2609, 265.4634, 223.2635, 199.5774, 624.0150, 226.1506,\n                      540.3696, 250.5057, 589.5431,  73.7593, 204.2974, 532.4202, 385.1188,\n                      473.4345, 587.5845, 259.0807, 194.6725, 621.2875, 295.7844, 251.7509,\n                      264.5834, 432.3381, 311.2780, 556.3077, 413.5897, 237.1847, 294.2524,\n                      337.9655, 513.8376, 212.2357, 238.3471, 355.2431, 233.5591,  77.4871,\n                      243.0866, 422.5858,  89.6662, 261.3156, 531.3607, 265.4735, 515.4614,\n                      577.2280, 230.1608, 244.6761, 577.1924, 375.6796, 625.2059, 519.4860,\n                      420.2735, 256.9900, 270.2244, 646.5214], device='cuda:0')),\n             ('layer3.1.bn1.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('layer3.1.conv2.weight',\n              tensor([[[[-5.3524e-02, -2.6872e-02, -1.2373e-03],\n                        [-2.4420e-01,  7.0648e-02,  1.1049e-01],\n                        [ 1.0042e-01,  8.3437e-02,  2.9097e-02]],\n              \n                       [[-6.9737e-02, -1.7281e-01,  1.3573e-01],\n                        [-1.4099e-01, -2.0103e-01,  1.1179e-01],\n                        [ 1.6408e-01,  1.3312e-01,  5.2241e-02]],\n              \n                       [[-1.2623e-03, -3.5835e-01,  1.5121e-01],\n                        [ 2.5573e-02,  1.4415e-01,  3.7393e-02],\n                        [-2.2744e-03, -5.5484e-02, -1.2982e-02]],\n              \n                       ...,\n              \n                       [[-4.2713e-02, -2.6117e-01,  1.3592e-01],\n                        [-2.7882e-02,  1.1660e-01,  5.0388e-02],\n                        [ 2.0101e-02,  1.4873e-02, -1.6639e-02]],\n              \n                       [[-1.7720e-02, -3.5213e-01,  1.4191e-01],\n                        [-5.1991e-02,  1.2954e-01,  3.4245e-02],\n                        [-6.9573e-04,  8.8308e-03,  1.3833e-02]],\n              \n                       [[-1.8427e-01, -1.6854e-01,  1.3310e-01],\n                        [ 1.1001e-01,  6.0924e-02,  5.4030e-02],\n                        [ 4.2483e-03, -1.0369e-02,  1.2898e-02]]],\n              \n              \n                      [[[-1.9575e-01,  4.0810e-02, -3.1950e-03],\n                        [ 8.6148e-02,  4.7552e-01,  1.2859e-01],\n                        [-3.1640e-02, -4.1953e-01, -5.0518e-01]],\n              \n                       [[-9.5809e-03,  6.7081e-02,  1.1192e-01],\n                        [ 2.9895e-01, -2.7522e-02,  5.4977e-02],\n                        [-4.6852e-02, -4.3238e-02, -4.7162e-01]],\n              \n                       [[-7.6613e-02,  6.8831e-02,  1.2951e-01],\n                        [ 4.4535e-02, -5.2456e-02, -6.1722e-01],\n                        [-8.8063e-03,  2.7358e-02,  5.5924e-02]],\n              \n                       ...,\n              \n                       [[-3.9112e-02,  5.8386e-02,  1.0009e-01],\n                        [-1.0434e-02, -2.3806e-02, -6.1022e-01],\n                        [ 6.8223e-03, -1.5921e-02,  2.9050e-02]],\n              \n                       [[-1.3480e-02,  5.3109e-02,  1.0324e-01],\n                        [-9.7841e-02, -5.5123e-02, -5.4597e-01],\n                        [ 1.9547e-03,  4.1450e-02,  2.0667e-02]],\n              \n                       [[ 9.6381e-02,  1.5927e-01,  1.2917e-01],\n                        [-9.5474e-03, -1.5364e-01, -4.1823e-01],\n                        [ 1.9070e-02, -1.0680e-03, -1.2408e-02]]],\n              \n              \n                      [[[-7.3205e-02,  1.3646e-01,  5.5596e-02],\n                        [ 1.1818e-01,  2.0459e-01,  1.3738e-01],\n                        [ 2.6401e-02, -3.1945e-01, -5.5142e-01]],\n              \n                       [[-2.6721e-03,  5.3145e-02,  1.0355e-01],\n                        [-5.1552e-02, -1.6633e-05, -1.4875e-03],\n                        [ 2.8132e-01, -7.6731e-03, -5.6489e-01]],\n              \n                       [[ 1.3733e-02,  9.9391e-02,  1.3203e-01],\n                        [-1.7267e-02,  5.5611e-02, -5.6227e-01],\n                        [ 4.2240e-03, -4.8779e-02, -3.2395e-02]],\n              \n                       ...,\n              \n                       [[ 3.4865e-02,  9.3784e-02,  1.2253e-01],\n                        [-2.6780e-02,  5.4847e-03, -5.6903e-01],\n                        [ 1.1933e-02, -1.2470e-01, -2.6530e-02]],\n              \n                       [[-4.5443e-02,  2.4679e-02,  1.2565e-01],\n                        [-3.4160e-02,  1.5526e-02, -5.7007e-01],\n                        [ 3.6296e-02, -8.8993e-02, -2.5537e-02]],\n              \n                       [[ 4.9449e-02,  6.7428e-02,  1.0503e-01],\n                        [ 6.4982e-03, -5.2504e-02, -4.8970e-01],\n                        [-1.5865e-02, -3.3541e-02, -1.2641e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.2216e-03,  1.5591e-02,  4.2050e-02],\n                        [ 3.6146e-02,  1.6722e-01,  2.8683e-01],\n                        [-1.2704e-01, -1.1616e-01,  3.6154e-02]],\n              \n                       [[-4.9834e-03, -2.8644e-02,  2.5036e-01],\n                        [-6.2467e-02, -1.2533e-01,  2.3567e-01],\n                        [-2.8673e-02, -1.4181e-01,  4.8161e-02]],\n              \n                       [[-1.4822e-02,  1.0329e-02,  3.2755e-01],\n                        [-5.2568e-02, -1.3478e-01,  4.9647e-02],\n                        [-1.3487e-02,  5.9126e-03, -1.8984e-02]],\n              \n                       ...,\n              \n                       [[ 2.6552e-02, -3.7938e-02,  2.9298e-01],\n                        [-7.1552e-02, -1.6026e-01,  2.8865e-02],\n                        [-1.3140e-02, -1.2913e-02,  1.6727e-02]],\n              \n                       [[-5.2119e-03, -3.7781e-02,  2.9907e-01],\n                        [-1.0288e-02, -1.0932e-01,  4.6129e-02],\n                        [ 2.1992e-02,  1.5937e-03, -4.0240e-02]],\n              \n                       [[ 5.6793e-02,  4.5792e-02,  2.4184e-01],\n                        [-1.3831e-01, -1.6154e-01,  7.5233e-02],\n                        [ 1.0874e-02, -1.5091e-02,  1.0494e-02]]],\n              \n              \n                      [[[ 2.2369e-02,  3.1889e-02, -3.1737e-02],\n                        [-3.2120e-01, -9.4961e-02,  2.9320e-02],\n                        [ 6.2082e-02,  1.6877e-01,  6.5093e-02]],\n              \n                       [[ 9.9561e-03, -2.4800e-01, -9.9252e-03],\n                        [ 2.2952e-02, -3.9178e-02,  4.6323e-02],\n                        [ 6.8713e-02,  7.1892e-02,  7.6010e-02]],\n              \n                       [[ 3.4867e-02, -2.7966e-01,  2.2882e-02],\n                        [-1.4835e-02,  9.8922e-02,  7.8259e-02],\n                        [-9.6866e-03, -2.5114e-03, -9.1404e-03]],\n              \n                       ...,\n              \n                       [[ 4.6995e-04, -2.4998e-01, -1.1689e-03],\n                        [-2.0872e-03,  1.1007e-01,  7.5561e-02],\n                        [-1.4875e-02, -7.9689e-03, -1.5316e-03]],\n              \n                       [[ 4.6149e-02, -1.9377e-01,  4.2185e-02],\n                        [-1.1304e-02,  1.0632e-01,  9.8340e-02],\n                        [-1.2490e-02,  5.7342e-02,  3.8966e-02]],\n              \n                       [[-1.8683e-01, -1.8453e-01,  5.8835e-02],\n                        [ 6.9580e-02,  1.0639e-01,  8.8327e-02],\n                        [-2.0372e-02, -1.0588e-02,  7.0250e-03]]],\n              \n              \n                      [[[-1.4039e-01, -9.4401e-02,  1.1446e-01],\n                        [ 1.2184e-01,  2.8679e-02, -3.8508e-02],\n                        [ 1.0413e-01, -1.7395e-01, -3.2240e-01]],\n              \n                       [[-6.5908e-02,  1.2164e-01, -7.6017e-02],\n                        [-1.2329e-02,  2.4431e-01, -2.5646e-01],\n                        [ 3.9067e-02,  1.0965e-01, -2.9188e-01]],\n              \n                       [[-3.7989e-02,  1.1475e-01, -7.9616e-02],\n                        [-8.6802e-02,  1.0565e-01, -3.3930e-01],\n                        [-4.7154e-02,  3.0892e-02, -1.1467e-02]],\n              \n                       ...,\n              \n                       [[-1.0951e-01,  1.3765e-01, -8.2271e-02],\n                        [-1.2796e-01,  8.9351e-02, -3.5171e-01],\n                        [-2.4407e-02,  4.7164e-02,  4.7651e-02]],\n              \n                       [[ 1.1489e-02,  1.5064e-01, -6.6790e-02],\n                        [-1.2978e-02,  1.3474e-01, -3.0814e-01],\n                        [-5.0102e-02,  5.9179e-02,  7.2102e-02]],\n              \n                       [[ 1.1385e-01,  9.3442e-02, -6.9385e-02],\n                        [ 8.3800e-02, -6.7145e-02, -2.8008e-01],\n                        [ 2.3499e-02,  1.6660e-02,  3.1676e-02]]]], device='cuda:0')),\n             ('layer3.1.bn2.weight',\n              tensor([ 0.7080,  0.2728,  0.3731,  0.6981,  0.3272,  0.6756,  0.5710,  0.6823,\n                       0.5254,  0.4582,  0.4143,  0.3947,  0.4135,  0.7235,  0.6339,  0.8194,\n                       0.5940,  0.4799,  0.5078,  0.2808,  0.4809,  0.5305,  0.5517,  0.6060,\n                       0.4048,  0.3442,  0.3644,  0.4935,  0.7012,  0.4090,  0.7310,  0.6313,\n                       0.5157,  0.3405,  0.3263,  0.5671,  0.4988,  0.2460,  0.3109,  0.5158,\n                       0.2904,  0.4675,  0.2698,  0.4426,  0.2455,  0.5740,  0.7733,  0.6376,\n                       0.5912,  0.5511,  0.3485,  0.3522,  0.5036,  0.3654,  0.4121,  0.4929,\n                       0.4306,  0.7507,  0.8048,  0.0069,  0.6754,  0.4635,  0.4853,  0.4525,\n                       0.3488,  0.4274,  0.1216,  0.5556,  0.5339,  0.2859,  0.5274,  0.3954,\n                       0.4588,  0.4522,  0.5774,  0.5634,  0.3159,  0.7986,  0.4609,  0.3327,\n                       0.2998,  0.7176,  0.3692,  0.4989,  0.5181,  0.3848,  0.2538,  0.2945,\n                       0.4170,  0.4271,  0.4188,  0.2206,  0.8485,  0.3385,  0.4488,  0.6753,\n                       0.7023,  0.5935,  0.6339,  0.2966,  0.4734,  0.3519,  0.7028,  0.6929,\n                       0.5732,  0.5104,  0.3177,  0.3554,  0.5451,  0.5318,  0.3120,  0.2833,\n                       0.3131,  0.4597,  0.3156,  0.6047,  0.2443,  0.7002,  0.3734,  0.7260,\n                       0.5252,  0.8497,  0.3800,  0.6778,  0.4425,  0.4734,  0.0401,  0.4984,\n                       0.5577,  0.3541,  0.0824,  0.3370,  0.3399,  0.3282,  0.3986,  0.5604,\n                       0.4670,  0.6279, -0.0045,  0.5136,  0.4984,  0.6123,  0.4575,  0.4965,\n                       0.3751,  0.3864,  0.5082,  0.7556,  0.4674,  0.6363,  0.3771,  0.4866,\n                       0.6939,  0.4757,  0.2824,  0.7844,  0.6511,  0.4869,  0.5631,  0.6779,\n                       0.4616,  0.6123,  0.6981,  0.4761,  0.4209,  0.5336,  0.4361,  0.4759,\n                       0.7965,  0.5987,  0.3944,  0.5579,  0.5138,  0.7767,  0.3606,  0.4465,\n                       0.3717,  0.0223,  0.6089,  0.7424,  0.4640,  0.3862,  0.4658,  0.6638,\n                       0.6043,  0.5031,  0.4121,  0.6358,  0.5412,  0.3340,  0.6859,  0.4626,\n                       0.3970,  0.6688,  0.3675,  0.4858,  0.3876,  0.3604,  0.4765,  0.7381,\n                       0.2850,  0.7108,  0.5428,  0.6834,  0.3743,  0.6028,  0.1995,  0.6723,\n                       0.4913,  0.6327,  0.6486,  0.4936,  0.5130,  0.2259,  0.6698,  0.6553,\n                       0.7717,  0.5102,  0.7451,  0.7493,  0.6678,  0.6847,  0.5940,  0.4495,\n                       0.4963,  0.2405,  0.5381,  0.5156,  0.1863,  0.5091,  0.4707,  0.5717,\n                       0.1969,  0.7739,  0.4766,  0.7495,  0.5656,  0.7515,  0.3772,  0.5371,\n                       0.3788,  0.3862,  0.4374,  0.4123,  0.4485,  0.6433,  0.4824,  0.4131,\n                       0.5942,  0.6956,  0.7101,  0.7956,  0.4738,  0.3605,  0.4675,  0.4058],\n                     device='cuda:0')),\n             ('layer3.1.bn2.bias',\n              tensor([-0.9399, -0.6788, -0.4757, -1.0466, -0.8669, -0.8788, -0.6235, -0.3072,\n                      -0.4562, -0.6568, -0.4569, -0.6091, -0.9402, -0.2314, -0.7477, -0.1845,\n                      -0.7514, -0.6464, -0.6137, -0.6290, -0.9254, -0.4879, -0.2620, -0.5102,\n                      -0.7482, -0.6473, -0.5063, -0.9027, -0.8807, -0.6413, -0.8152, -0.9695,\n                      -1.0328, -0.7959, -0.6053, -0.5699, -0.5814, -0.6613, -0.5533, -0.4571,\n                      -0.6414, -0.4437, -0.9464, -0.5578, -0.6625, -0.5641, -0.1815, -0.8528,\n                      -0.3962, -0.7610, -0.5107, -0.5491, -0.7519, -0.4795, -0.5474, -0.4918,\n                      -0.5479, -0.2529, -0.1843, -0.7821, -0.2769, -0.6946, -0.6162, -0.8675,\n                      -0.6815, -1.0320, -0.8694, -1.0792, -0.9300, -0.5283, -1.1069, -0.7268,\n                      -0.5179, -0.8373, -0.7875, -0.5610, -0.6851, -0.1713, -0.5949, -0.7071,\n                      -0.8654, -0.2702, -0.4629, -0.9272, -0.7347, -0.5971, -0.6196, -0.6168,\n                      -0.8707, -0.5245, -0.6514, -0.5781, -0.1158, -0.9028, -0.4908, -0.3114,\n                      -0.2872, -0.3463, -1.2542, -0.6579, -0.4405, -0.5649, -0.2580, -0.5372,\n                      -0.7004, -0.4684, -1.0601, -0.7255, -0.9695, -0.6776, -0.3946, -0.7766,\n                      -0.7380, -0.5677, -0.7616, -0.3203, -0.6098, -0.2602, -0.8219, -0.7534,\n                      -0.4099, -0.1189, -0.5993, -0.2826, -0.5816, -0.5190, -0.8414, -0.8972,\n                      -0.3475, -0.6520, -0.6592, -0.5275, -0.8231, -0.6367, -0.6365, -0.8381,\n                      -0.8214, -0.2761, -0.5742, -0.8411, -0.6616, -1.1303, -0.4491, -0.3964,\n                      -0.7069, -0.9573, -0.5414, -0.2204, -0.5598, -0.3131, -0.4631, -0.7155,\n                      -1.0736, -0.5843, -0.9422, -0.2537, -1.0540, -0.6878, -0.6528, -1.2819,\n                      -0.5461, -0.6875, -0.2727, -0.5032, -1.0963, -0.9589, -0.4328, -0.4794,\n                      -0.1679, -0.3466, -0.7533, -1.0325, -0.5612, -0.2218, -0.6021, -1.0023,\n                      -0.7194, -0.6620, -0.3263, -0.2379, -0.7525, -0.4703, -0.8983, -0.2951,\n                      -0.3324, -0.8434, -0.3057, -0.7110, -0.3694, -0.6191, -0.9294, -0.7881,\n                      -0.8181, -0.3136, -0.6859, -0.6790, -0.4253, -0.6385, -0.7585, -0.2518,\n                      -0.8868, -0.2766, -0.5786, -0.3157, -0.5859, -0.8896, -0.6089, -0.3087,\n                      -0.4852, -0.6682, -0.7545, -0.7600, -0.6218, -0.6873, -0.3062, -0.5577,\n                      -0.7965, -0.6720, -0.2377, -0.7548, -0.9876, -0.9978, -0.5789, -0.7429,\n                      -0.4623, -0.4899, -0.9295, -0.4562, -0.8657, -0.6398, -0.7191, -1.0338,\n                      -0.5380, -0.1963, -0.9586, -1.1864, -0.8939, -0.2261, -0.8788, -0.7050,\n                      -0.6414, -0.7875, -0.5318, -0.4935, -0.5889, -0.3460, -0.5928, -0.6508,\n                      -1.0353, -0.2903, -0.1985, -1.1758, -0.8396, -1.0850, -0.5740, -0.4677],\n                     device='cuda:0')),\n             ('layer3.1.bn2.running_mean',\n              tensor([  4.0004, -32.0752, -23.2018,  -4.8005,   3.1608,   0.1602,  -3.1041,\n                        7.1359,  -6.5095,  -6.8668, -27.7032,  -9.4481,   4.1104,  -3.8596,\n                        1.9803,   1.5071,  -1.6838, -18.9876, -18.3672,  -0.8936,  -2.7125,\n                       -9.8721,   5.1894, -11.0260,  -0.6642,  -4.5553, -70.6099,  -7.8808,\n                       -1.5900,  -0.4787,   1.4577, -13.5154,   1.2762,  -3.4432,  -4.7351,\n                       -0.5669, -42.5364,  -1.5000,  -7.4825, -22.5002, -47.9323, -50.1440,\n                        3.9965,   0.6754, -12.6804, -14.5145, -10.9191,  11.6052,  21.3080,\n                        6.0896,  -7.0010,   0.8510,   2.0353, -40.6417, -39.0359,  -5.6092,\n                       -8.8212,  20.2528, -10.1190,   0.8084, -15.1722,  -5.3601, -26.9667,\n                        0.6346,  -1.0722,   5.8806, -12.8864,   3.7797, -21.5409, -11.8091,\n                       11.6334,   1.1573, -18.5802,  -8.2590,   7.0655, -12.5323, -11.1625,\n                       -9.8854,   1.2206,  37.6867,  -7.6509,   4.8622, -38.7450,  -8.8081,\n                       -6.1700, -18.3737,   0.4813,   2.5833,   1.1676, -13.0495, -18.2344,\n                      -11.3101, -17.2783,   4.4981, -13.8765,  14.9087,   9.5133,   1.6467,\n                      -10.0304, -17.6082, -38.3643,   4.4722,   2.4186, -24.1769,  22.7820,\n                      -26.2720, -17.2565,   1.9940,  15.4012,   4.3231, -38.8045, -10.0201,\n                      -49.7666,   4.6570,   2.0436,   8.7061, -10.7462,  15.1746,  -7.4801,\n                      -51.4768, -12.7555,  -9.4221, -35.0342,  14.1552, -16.4363,   7.3711,\n                        4.6075,  11.5716,   0.2514,  -8.0190,   8.7311, -44.2078,  -5.3947,\n                        9.2080,  -9.6739,  12.2447,  -0.4807,  22.6503,  -1.7112,  13.0625,\n                      -10.9260,  15.6328, -40.4654, -63.0291,   0.2091,  -5.7347,  -6.9358,\n                      -22.3354, -12.3349,  12.2224,  20.6843,  -2.3709,  -7.4958, -16.8097,\n                        5.9036,  30.8428,   3.7913,  -1.2736,  -9.1994,  -0.6767, -12.7080,\n                       -4.1321,  18.2708,  -0.4551,  -9.3096, -13.2393,  -9.8707, -21.4956,\n                       -6.4534,  -3.2953,  -1.4030,  -9.3706,  -2.5206,  33.3701, -23.9970,\n                       -6.3242,   3.0160, -37.3467, -16.7282,  10.2107, -17.2761, -71.7922,\n                       13.5460,   8.0297,  16.5460,  -2.9510,  15.7330, -13.3572,  37.0372,\n                      -21.0803, -11.8181, -30.0136,  10.5424,  20.1764, -17.9820,  -5.1494,\n                      -75.7296,  -2.7675,   8.7563,  27.2198,  -6.6311,   5.6115,  -8.5620,\n                        8.0370,  -8.8150,  -5.6738,  -9.2450,   6.1619, -59.4493,  -3.4276,\n                        7.3964,  -9.0057,  -9.2786,   2.4387, -19.7477, -21.5873, -15.7001,\n                       -2.9769,   9.9469, -13.3825,   3.1862,   2.4179,  -8.8640,   6.5393,\n                        7.4253,   8.9840,  22.5226, -19.8825,   8.4674,  -1.0071,   9.0896,\n                        1.8947,   4.7797,  38.1378,  13.5247,   3.2148, -12.7597, -28.0478,\n                       -8.3781,   0.3573, -16.9210,   9.5231,  -2.4908, -30.8455,  -8.7673,\n                       16.0938,  -4.7959,  -6.6983,  -9.9512,  12.0018, -12.1049,  11.5352,\n                       -2.0843,  11.4872,  -1.8432, -19.5434], device='cuda:0')),\n             ('layer3.1.bn2.running_var',\n              tensor([ 2652.6294,  8130.0005,  7525.5088,  1480.9249,   717.5845,  1912.1235,\n                       5840.2988,  3264.7371,  3290.2356,  5026.3271,  6205.5562,  2652.8474,\n                       4753.0957,  2878.0203,  1944.1062,  3563.0879,  2023.4872,  3498.3262,\n                       2421.8135,  4618.6455,  1814.4358,   973.3505,  3944.0947,  2314.9846,\n                       1771.7876,  3793.5715, 19969.4180,  2591.7524,  2651.9912,  1526.7836,\n                       1573.7260,  4837.7334,  2413.8994,  2368.0725,  4789.5649,  3670.1118,\n                      10801.4238,  3853.1326,  3795.3232,  4434.1172,  6427.7910, 13235.9502,\n                        434.9283,  2869.7583,  3637.6475,  4016.3992,  3291.0083,  4562.7939,\n                       3713.8206,  3680.0012,  7540.4312,   432.5041,  3123.9480, 12151.5107,\n                       7522.9526,  1050.6594,   910.4191,  4737.3291,  3135.4312,  3031.7505,\n                       4057.2695,  4851.8286,  6705.7305,   346.4185,   307.6547,  2727.2107,\n                       4754.9429,  2336.4631,  4500.3169,  3210.3694,  1778.0067,  1952.1006,\n                       4160.4956,  3333.8352,  2720.3267,  4844.5576,  5720.4487,  3305.0071,\n                       4269.8911,  5361.9883,  2957.1301,  3162.5220,  8549.7012,  2054.5779,\n                       5612.1646,  3339.0173,   159.9606,  2320.5154,  1946.9496,  4628.7124,\n                       6344.6377,  3428.3760,  3500.6274,  2583.4512,  5638.6655,  2761.2473,\n                       3544.7288,  6999.5317,   853.8159,  1965.3137, 13018.8975,  3825.5486,\n                       3401.9250,  7477.4263,  2572.5933,  4042.6748,  3238.6719,  1806.0701,\n                       2428.0417,  3642.2219,  6836.6270,  3690.6184, 15075.6826,  4447.7949,\n                       2105.3203,  4967.6768,  1907.0549,  2669.5129,  2386.4067, 11443.7764,\n                       5928.9829,  2995.5825,  6240.7490,  3488.2671,  4188.6899,  1975.4258,\n                       2160.8208,  3891.0654,  4117.1489,  3069.0286,   911.5954, 10699.8887,\n                       2989.4795,  1066.3451,  1090.8386,  1165.3328,  1601.7202,  4746.4941,\n                       2284.6021,  3575.0964,  1945.2604,  4812.7236, 11584.3936, 17930.2422,\n                        982.9963,  2342.8416,  4501.6587,  3874.9534,  5563.1294,  4168.6255,\n                       6975.1597,  3756.4653,  3416.7917,  6545.3438,  1672.6866,  4375.1313,\n                        913.3228,  1181.9104,  1952.2458,   217.9956,  6323.9438,  4973.8667,\n                       3758.6077,  3843.8911,  4073.0396,  1458.2075,  2970.5991,  4341.4238,\n                       2958.0461,  6941.4756,  1544.1454,   240.6510,  3376.5623,  2629.0579,\n                       4788.3398,  3207.8984,  3956.9858, 10244.5059,  5254.3394,  3716.6528,\n                       3406.8997, 18679.2168,  3070.5164,  3420.6313,  4238.5703,  2212.6802,\n                       5959.2998,  2286.8779,  6175.7461,  2417.9834,  4303.9297,  1967.5215,\n                       1613.8450,  2861.5208,  3542.9492,  3460.8389, 19582.6797,  1992.4159,\n                       3858.9194,  4453.8887,  2516.7441,  3167.5989,  4013.5154,  2751.7720,\n                       3299.8188,  2108.1506,  3415.7820,  3004.4607, 15622.4775,  2405.3584,\n                       3035.5027,  1852.5088,  5610.1763,  1346.1301,  3751.4951,  5538.2334,\n                       3654.7307,   820.8381,  2997.8628,  3299.2454,  3751.9351,  1641.1561,\n                       4000.6277,  2202.0352,  4037.6697,  5495.0396,  4068.2778,  4134.0718,\n                       2107.4614,  3275.9358,  5132.4165,  1631.1859,  1753.0117,  6442.5454,\n                       2263.5940,  3817.7261,  6275.6113,  4413.0483,  1393.8865,   600.1643,\n                       4999.2090,  3201.3176,  6585.6040,  7335.9751,  4002.1980,  2677.1511,\n                       1826.6615,  2152.2498,  5324.0098,  3145.7009,  2953.2234,  2111.1741,\n                       2353.9502,  3252.5598,  2050.8323,  4757.7803], device='cuda:0')),\n             ('layer3.1.bn2.num_batches_tracked',\n              tensor(46695, device='cuda:0')),\n             ('fc.weight',\n              tensor([[ 4.4501e-02,  2.1664e-01, -1.5743e-01,  2.0691e-02,  7.5609e-03,\n                        2.0250e-03, -2.5927e-02, -9.0458e-02, -4.6027e-02,  2.2368e-01,\n                       -2.5589e-02, -2.1430e-02, -1.7535e-01,  8.3554e-02,  3.9013e-02,\n                        1.7890e-01,  3.1588e-02,  9.0039e-02,  2.6075e-02, -8.0030e-02,\n                        2.8926e-02,  1.6605e-03, -7.4999e-02, -1.5609e-02,  4.6577e-02,\n                       -4.0311e-02,  2.9843e-01,  4.2360e-02,  7.9470e-02,  6.0766e-02,\n                        1.6065e-02,  6.2104e-02, -1.4182e-01, -6.4115e-02, -1.2229e-01,\n                        2.5841e-02,  2.3550e-01, -1.6100e-03, -1.7652e-02,  1.5788e-02,\n                        5.1487e-02,  1.3203e-01,  7.9141e-03,  1.7043e-02, -8.3175e-02,\n                        4.2414e-03,  1.2124e-01,  2.0418e-01, -8.8399e-02,  9.8340e-02,\n                       -1.1272e-01,  1.4899e-02, -3.4388e-02,  2.0947e-01,  4.5845e-02,\n                       -1.5581e-02,  1.1204e-02,  9.7110e-02,  1.4453e-01, -4.6131e-01,\n                        1.0035e-01,  1.4323e-01,  9.1031e-02,  1.9816e-02, -8.0357e-03,\n                       -2.0664e-02,  2.4574e-01,  3.6293e-02,  5.6205e-02,  9.1119e-02,\n                        1.1513e-01,  1.6438e-01, -1.9248e-02,  1.5515e-01,  3.9667e-02,\n                        3.4828e-03, -2.9661e-01,  9.4270e-02,  2.6567e-02, -2.2276e-01,\n                       -5.4651e-02, -1.0469e-01, -1.1633e-01, -3.4286e-02, -1.8465e-01,\n                       -6.2096e-03, -1.5053e-02, -5.8341e-02, -5.2837e-03, -7.9008e-02,\n                        1.6978e-01, -1.0169e-01,  1.6194e-01, -1.2439e-03,  7.2474e-03,\n                       -1.0189e-01, -9.4636e-02, -4.5148e-02, -7.9065e-03, -1.4649e-02,\n                       -7.2083e-02,  1.1252e-02,  6.2024e-02, -2.1296e-02, -1.0366e-01,\n                       -4.6121e-02, -7.1965e-02, -9.6299e-02, -4.3793e-02, -4.5610e-02,\n                        2.0836e-01, -1.9400e-01,  3.7158e-01,  9.6402e-02,  3.0094e-02,\n                       -6.6278e-02,  4.9391e-02, -9.4924e-02,  1.3027e-02, -2.1222e-01,\n                        2.9833e-02,  1.6318e-01, -1.5381e-01, -1.0430e-01,  1.2237e-01,\n                       -1.5533e-04,  5.8360e-02,  7.3445e-02,  6.7453e-02,  3.1405e-03,\n                       -6.6972e-02, -9.3320e-02,  7.3068e-02,  8.8654e-02, -1.2437e-02,\n                        1.7532e-02,  2.6596e-02, -1.2603e-01,  1.8279e-01, -7.4396e-02,\n                        1.4867e-02,  7.9463e-02, -3.1132e-02,  1.6365e-01, -5.5462e-03,\n                        9.3653e-02,  1.7883e-02,  1.0645e-01,  9.2478e-02, -8.9137e-02,\n                        2.5652e-01,  3.0961e-02,  4.6948e-02, -7.3957e-02,  2.8154e-02,\n                        9.7243e-02,  1.4539e-02,  1.1193e-01, -6.7889e-03, -4.5329e-02,\n                        5.7087e-03,  6.0822e-03,  1.0950e-01, -1.4511e-02,  5.7499e-02,\n                       -4.2285e-04, -1.3602e-02, -3.3773e-02,  1.2060e-01, -6.0369e-02,\n                        2.1042e-02,  5.1829e-02,  5.0451e-02, -1.9770e-01,  4.1335e-02,\n                       -7.6827e-02, -4.2756e-02, -1.8114e-01,  7.8157e-02,  1.5677e-01,\n                       -1.0071e-02,  2.1580e-01,  2.2900e-02, -7.4162e-02,  5.6636e-02,\n                       -5.6119e-02, -6.4179e-02, -1.4044e-02, -9.1487e-02,  1.2878e-01,\n                       -1.2755e-01, -4.3480e-02, -2.5209e-01, -7.7606e-02, -4.5787e-02,\n                       -5.8841e-04,  1.8376e-01, -4.3920e-02, -7.0686e-02,  9.9915e-02,\n                       -5.2067e-02, -9.9690e-02,  3.8661e-02, -9.0996e-02, -1.3192e-02,\n                       -1.9895e-02, -1.4796e-01, -7.8251e-02, -1.3684e-01,  2.6744e-02,\n                       -3.8008e-02,  1.8736e-02,  5.4697e-02,  2.2051e-02,  7.1794e-02,\n                       -1.2693e-02, -2.4514e-02, -1.3518e-02, -1.2661e-01, -2.0151e-02,\n                        1.3693e-02,  1.6260e-02,  4.6825e-02,  1.8547e-02,  7.2193e-03,\n                        2.2634e-01,  5.4383e-02,  1.1579e-02, -2.9680e-02,  1.4900e-02,\n                        9.3999e-02, -1.1896e-04,  2.2701e-02, -1.8453e-01,  7.6297e-03,\n                       -2.4427e-02,  3.5322e-01,  9.7918e-02,  2.3294e-02,  2.6221e-02,\n                       -1.8808e-01,  3.4085e-02, -1.5792e-01,  1.8171e-01, -6.1199e-02,\n                       -7.7542e-02, -3.2381e-03,  2.2185e-02,  3.1276e-01, -8.8649e-02,\n                        1.1518e-01, -5.4766e-02,  3.3844e-02,  4.1040e-02,  2.2538e-02,\n                        9.3154e-02]], device='cuda:0')),\n             ('fc.bias', tensor([0.0161], device='cuda:0'))])"},"metadata":{}}]}]}